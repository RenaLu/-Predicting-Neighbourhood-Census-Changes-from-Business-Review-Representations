{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CensusPrediction_MotherTongue.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5QEoXb0vhch",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmm2_PLSXXms",
        "colab_type": "code",
        "outputId": "e673c176-7430-419b-ad11-516d38163893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n",
            "\r\u001b[K     |███▏                            | 10kB 22.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 2.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51kB 1.9MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 2.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71kB 2.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81kB 2.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 92kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102kB 2.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.4.0)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-UhiP9qfJOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from scipy import sparse\n",
        "\n",
        "from sklearn.linear_model import RidgeCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xsyi3AX56i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ignite.handlers import ModelCheckpoint, EarlyStopping, TerminateOnNan\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
        "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
        "from ignite.contrib.handlers import ProgressBar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwPy5Lh3CH-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import string\n",
        "import random\n",
        "import torch.utils.data as Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itB2xTKXZm3o",
        "colab_type": "code",
        "outputId": "25409f27-2b1c-485c-ed9c-cd31bcb888f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import OrderedDict\n",
        "import locale\n",
        "from locale import atof, atoi\n",
        "locale.setlocale(locale.LC_NUMERIC, '')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en_US.UTF-8'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBzBYsqc1OAw",
        "colab_type": "code",
        "outputId": "4d986c7f-1035-4087-85ac-96fb603d7f66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmD0YDfwfafe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_idf = True\n",
        "max_features = 2**10\n",
        "pca_components = 2**4\n",
        "DRIVE_PATH = Path('/content/drive/My Drive/Thesis2019/')\n",
        "NEIGHBOURHOOD_PROFILES_PATH = Path('neighbourhood-profiles-2016-csv.csv')\n",
        "NEIGHBOURHOOD_PROFILES_PATH_OLD = Path('neighbourhood-data-2001-2011.xlsx')\n",
        "DATA_DIR = DRIVE_PATH.joinpath('data_stripped')\n",
        "use_cuda = True\n",
        "years = [2011, 2016]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KxXDNKRxRpT",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHv4QW7DYBKP",
        "colab_type": "text"
      },
      "source": [
        "Pytorch Dataset wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDgKDOkoxTQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReviewsVector(Data.Dataset):\n",
        "    \"\"\"Reviews Vector dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "          data\n",
        "\n",
        "        \"\"\"\n",
        "        self.shape = data.shape\n",
        "        self.data = torch.tensor(data).type(torch.FloatTensor)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          self.data = self.data.cuda()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class CensusVector(Data.Dataset):\n",
        "  def __init__(self, data, reviews_embedding):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "          data\n",
        "\n",
        "        \"\"\"\n",
        "        self.shape = data.shape\n",
        "        self.data = torch.tensor(data).type(torch.FloatTensor)\n",
        "        self.reviews_embedding = torch.tensor(reviews_embedding).type(torch.FloatTensor)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            self.data = self.data.cuda()\n",
        "            self.reviews_embedding = self.reviews_embedding.cuda()\n",
        "            \n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return {\"data\": self.data[idx],\n",
        "              \"reviews_embedding\": self.reviews_embedding[idx]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0yvBkQovmDE",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdiWF8NSwfqx",
        "colab_type": "text"
      },
      "source": [
        "## Selecting and Matching Attributes for 2011 and 2016 data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiwEZ3YDYGGC",
        "colab_type": "text"
      },
      "source": [
        "Match census categories from different years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV574nlTv42q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbh_profiles = pd.read_csv(DRIVE_PATH.joinpath(NEIGHBOURHOOD_PROFILES_PATH))\n",
        "nbh_profiles_2011 = pd.read_excel(DRIVE_PATH.joinpath(NEIGHBOURHOOD_PROFILES_PATH_OLD), sheet_name='2011')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7N_jBiOZonh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lang_2011 = nbh_profiles_2011[nbh_profiles_2011['Topic']=='Mother tongue']['Attribute'].to_numpy()\n",
        "l_2011 = []\n",
        "for i in lang_2011:\n",
        "  l_2011.append(i.replace(' ', ''))\n",
        "language_at_home_2011 = nbh_profiles_2011[(nbh_profiles_2011['Topic']=='Mother tongue')].drop(columns=['Category', 'Topic', 'City of Toronto'])\n",
        "language_at_home_2011['Attribute'] = l_2011"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcItAlkVnLkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lang_2016 = nbh_profiles[nbh_profiles['Topic']=='Mother tongue']['Characteristic'].to_numpy()\n",
        "l_2016 = []\n",
        "for i in lang_2016:\n",
        "  l_2016.append(i.replace(' ', ''))\n",
        "language_at_home_2016 = nbh_profiles[(nbh_profiles['Topic']=='Mother tongue')].drop(columns=['_id', 'Category', 'Topic', 'Data Source', 'City of Toronto'])\n",
        "language_at_home_2016['Characteristic'] = l_2016"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HOILGeIcQp_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "list(language_at_home_2016['Characteristic'].unique())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadRLgh2wKM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_2011_idx = language_at_home_2011[language_at_home_2011['Attribute'] == 'Singleresponses'].index\n",
        "multi_2011_idx = language_at_home_2011[language_at_home_2011['Attribute'] == 'Multipleresponses'].index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QE2nDlv9wQ1d",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lath_2011_single = language_at_home_2011.loc[single_2011_idx[0]+1:multi_2011_idx[0]-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bm2ulN1n9Gp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "language_at_home_2011 = lath_2011_single"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SHoy7TD7n_r7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "single_2016_idx = language_at_home_2016[language_at_home_2016['Characteristic'] == 'Singleresponses'].index\n",
        "multi_2016_idx = language_at_home_2016[language_at_home_2016['Characteristic'] == 'Multipleresponses'].index\n",
        "lath_2016_single = language_at_home_2016.loc[single_2016_idx[0]+1:multi_2016_idx[0]-1]\n",
        "\n",
        "language_at_home_2016 = lath_2016_single"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hMffQO9GoCcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "language_at_home_2011.sort_values(by='Attribute', inplace=True)\n",
        "language_at_home_2016.sort_values(by='Characteristic', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4U9h-V4oEHY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "language_at_home_2011.reset_index(inplace=True, drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4vphaU5HoFRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "language_at_home_2016.set_index('Characteristic', inplace=True)\n",
        "language_at_home_2016 = language_at_home_2016.applymap(atoi)\n",
        "language_at_home_2016.reset_index(inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H-G_5JqIoGwQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lath_2011 = pd.DataFrame()\n",
        "lath_2016 = pd.DataFrame()\n",
        "languages = []\n",
        "languages_2011 = []\n",
        "\n",
        "for index_2016, row_2016 in language_at_home_2016.iterrows():\n",
        "  for index_2011, row_2011 in language_at_home_2011.iterrows():\n",
        "    if row_2011['Attribute'] == row_2016['Characteristic']:\n",
        "      languages.append(row_2016['Characteristic'])\n",
        "      languages_2011.append(row_2011['Attribute'])\n",
        "      lath_2011 = lath_2011.append(row_2011.iloc[1:], ignore_index=True)\n",
        "      lath_2016 = lath_2016.append(row_2016.iloc[1:], ignore_index=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmSvGTznxLVP",
        "colab_type": "text"
      },
      "source": [
        "## Build Census datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBWZFdE4oK9m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lath_2011.to_csv(DRIVE_PATH.joinpath('mother_tongue_2011.csv'))\n",
        "lath_2016.to_csv(DRIVE_PATH.joinpath('mother_tongue_2016.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_q_9zDIxom",
        "colab_type": "text"
      },
      "source": [
        "## Load Built Census datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAwtOk-zIxCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lath = {}\n",
        "\n",
        "for year in years:\n",
        "  lath[year] = pd.read_csv(DRIVE_PATH.joinpath('mother_tongue_{}.csv'.format(year)))\n",
        "  lath[year].set_index('Unnamed: 0', inplace=True)\n",
        "  lath[year] = lath[year].div(lath[year].sum(axis=1), axis=0)\n",
        "  lath[year] = lath[year].replace(0,0.0000000001)\n",
        "  lath[year].sort_index(inplace=True)\n",
        "\n",
        "  # After plotting heatmaps, found this column to be an outlier, which has significant negative delta for all neighbourhoods\n",
        "  # The reason is that in 2011, many different Chinese dialects are grouped into one 'Chinese,n.o.s.' group, while in 2016\n",
        "  # A few different kind is separated out.\n",
        "  lath[year].drop(columns='Chinese,n.o.s.', inplace=True) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtT3yMRfxfY5",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Census Change\n",
        "\n",
        "Census Change prediction is evaluated with Mean Total Absolute Error:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbBZZj91qKMy",
        "colab_type": "text"
      },
      "source": [
        "$Mean Total Absolute Error = $\n",
        "\n",
        "$\\frac{1}{\\#Neighbourhoods}\\sum_{Neighbourhoods} \\sum_{categories} |Actual Proportion - Predicted Proportion|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34dueeBjxz6o",
        "colab_type": "text"
      },
      "source": [
        "### Train-test split\n",
        "\n",
        "Split by neighbourhoods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTuAGFwx4EU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_val_neighbourhoods, test_neighbourhoods = train_test_split(range(0, 140), test_size=0.15)\n",
        "\n",
        "all_trains, all_vals = [], [] #Do random splits to cross validate\n",
        "folds = 5\n",
        "\n",
        "for i in range(folds):\n",
        "  train_neighbourhoods, val_neighbourhoods = train_test_split(train_val_neighbourhoods, test_size=0.30)\n",
        "  all_trains.append(train_neighbourhoods)\n",
        "  all_vals.append(val_neighbourhoods)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY9kM4UO-8pA",
        "colab_type": "text"
      },
      "source": [
        "### Baseline-Predicting no change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLrDG6hs-_Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_data = lath[2011].values\n",
        "actual_data = lath[2016].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuyEnG-Q8yGn",
        "colab_type": "code",
        "outputId": "afc47847-1255-469b-af07-f6b9e192960b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs(dummy_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1198190504037125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4a7r_zD82P2",
        "colab_type": "code",
        "outputId": "93f10536-a230-4e33-8369-e9454393deb1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs(dummy_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11770054428156798"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5675IoiJxhit",
        "colab_type": "text"
      },
      "source": [
        "## Raw TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB2B8NFXbVKy",
        "colab_type": "text"
      },
      "source": [
        "### Load built csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmEZ_P7n_-3J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_all = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews_all[year] = pd.read_csv(DRIVE_PATH.joinpath('neighbourhood_reviews_{}.csv'.format(year)))\n",
        "  reviews_all[year].set_index('Unnamed: 0', inplace=True)\n",
        "\n",
        "reviews = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews[year] = ReviewsVector(reviews_all[year].T.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1-Ww885bO0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "delta_census = lath[2016].values - lath[2011].values\n",
        "delta_reviews = (reviews_all[2016].values - reviews_all[2011].values).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9945yCARSghw",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwRxg-p1SYBA",
        "colab_type": "code",
        "outputId": "580663f8-a2c8-47cf-f4a6-60415654b2f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 605
        }
      },
      "source": [
        "lr = RidgeCV(cv=5)\n",
        "lr.fit(delta_reviews[train_val_neighbourhoods], delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/base.py:434: FutureWarning: The default value of multioutput (not exposed in score method) will change from 'variance_weighted' to 'uniform_average' in 0.23 to keep consistent with 'metrics.r2_score'. To specify the default value manually and avoid the warning, please either call 'metrics.r2_score' directly or make a custom scorer with 'metrics.make_scorer' (the built-in scorer 'r2' uses multioutput='uniform_average').\n",
            "  \"multioutput='uniform_average').\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RidgeCV(alphas=array([ 0.1,  1. , 10. ]), cv=5, fit_intercept=True,\n",
              "        gcv_mode=None, normalize=False, scoring=None, store_cv_values=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzoFFgPSSezq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_change = lr.predict(delta_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X68hopx1TU7c",
        "colab_type": "code",
        "outputId": "b50efec6-353b-4c02-ac03-2b54229183fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change[train_val_neighbourhoods]-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12369619872283547"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emFk-RbVTSLh",
        "colab_type": "code",
        "outputId": "ee577006-dc0b-4d0d-9a27-31978552fe33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((predicted_change[test_neighbourhoods]-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1202751343426184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkbhD73ATaS5",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target  Non-Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSq-AIdQTfVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder_C(nn.Module):\n",
        "    def __init__(self, sizes, softmax=False):\n",
        "        super(Decoder_C, self).__init__()\n",
        "        sizes = sizes[::-1]\n",
        "        layers_de = OrderedDict()\n",
        "\n",
        "        for i in range(len(sizes)-2):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_de[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            layers_de[act_name] = nn.Tanh()\n",
        "\n",
        "        layers_de['linear{}'.format(len(sizes)-1)] = nn.Linear(sizes[-2], sizes[-1])\n",
        "        # layers_de['softmax'] = nn.Softmax(dim=1) # row sum to 1\n",
        "        layers_de['tanh'] = nn.Tanh()\n",
        "        if softmax:\n",
        "          layers_de['softmax'] = nn.Softmax(dim=1)\n",
        "        self.decoder = nn.Sequential(layers_de)\n",
        "\n",
        "    def forward(self, encoded):\n",
        "        return self.decoder(encoded) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YOFZyePTvJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def criterion_c(data, decoded):\n",
        "    mse_loss = nn.MSELoss()\n",
        "    loss = mse_loss(data, decoded)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEp0bW7CTyEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_decoder(decoder, dataset, train_indices, test_indices, name='edu_auto'):\n",
        "  optimizer_de = optim.Adam(decoder.parameters(), lr=0.001)\n",
        "  scheduler_de = optim.lr_scheduler.ReduceLROnPlateau(optimizer_de, 'min', patience=20, min_lr=min_lr, factor=0.1)\n",
        "\n",
        "  def process_function(engine, batch):\n",
        "    decoder.train()\n",
        "    optimizer_de.zero_grad()\n",
        "    decoded = decoder(batch['reviews_embedding'])\n",
        "    loss = criterion_c(decoded, batch['data'])\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer_de.step()\n",
        "    return loss.item()\n",
        "  \n",
        "\n",
        "  def eval_function(engine, batch):\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        decoded = decoder(batch['reviews_embedding'])\n",
        "        return decoded, batch['data']\n",
        "  \n",
        "  trainer = Engine(process_function)\n",
        "  train_evaluator = Engine(eval_function)\n",
        "  validation_evaluator = Engine(eval_function)\n",
        "\n",
        "  metric = Loss(criterion_c)\n",
        "  metric.attach(train_evaluator, 'loss')\n",
        "  metric.attach(validation_evaluator, 'loss')\n",
        "\n",
        "  training_losses = []\n",
        "  validation_losses = []\n",
        "\n",
        "  @trainer.on(Events.EPOCH_COMPLETED)\n",
        "  def log_training_results(engine):\n",
        "      train_evaluator.run(train_iterator)\n",
        "      metrics = train_evaluator.state.metrics\n",
        "      avg_loss = metrics['loss']    \n",
        "      training_losses.append(avg_loss)\n",
        "      print(\"Training Results - Epoch: {}  Avg loss: {:.10f}\"\n",
        "           .format(engine.state.epoch, avg_loss))\n",
        "      \n",
        "  def log_validation_results(engine):\n",
        "      validation_evaluator.run(valid_iterator)\n",
        "      metrics = validation_evaluator.state.metrics\n",
        "      avg_loss = metrics['loss']\n",
        "      validation_losses.append(avg_loss)\n",
        "\n",
        "      print(\"Validation Results - Epoch: {}  Avg loss: {:.10f}\"\n",
        "            .format(engine.state.epoch, avg_loss))\n",
        "\n",
        "  trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)\n",
        "\n",
        "  # Reduce on Plateau\n",
        "  def average_loss(engine):\n",
        "    print(\"Current lr: {}\".format(optimizer_de.param_groups[0]['lr']))\n",
        "    average_loss = engine.state.metrics['loss']\n",
        "    scheduler_de.step(average_loss)\n",
        "\n",
        "  validation_evaluator.add_event_handler(Events.COMPLETED, average_loss)\n",
        "  \n",
        "  # Early Stopping\n",
        "  def score_function(engine):\n",
        "      val_loss = engine.state.metrics['loss']\n",
        "      return -val_loss\n",
        "\n",
        "  handler = EarlyStopping(patience=30, score_function=score_function, trainer=trainer)\n",
        "  validation_evaluator.add_event_handler(Events.COMPLETED, handler)\n",
        "\n",
        "  # Model Checkpoint\n",
        "  checkpointer = ModelCheckpoint(str(DRIVE_PATH.joinpath('models')), name, n_saved=1, create_dir=False, save_as_state_dict=True, require_empty=False)\n",
        "  trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'decoder_c': decoder})\n",
        "\n",
        "  train_iterator = Data.DataLoader(dataset, batch_size=1, drop_last=False, sampler=Data.SubsetRandomSampler(train_indices))\n",
        "  valid_iterator = Data.DataLoader(dataset, batch_size=1, drop_last=False, sampler=Data.SubsetRandomSampler(test_indices))\n",
        "\n",
        "  trainer.run(train_iterator, max_epochs=1000)\n",
        "  return training_losses, validation_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSxYLT6SURpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pvNScaFT514",
        "colab_type": "text"
      },
      "source": [
        "#### With one layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv0Bx5ryT2nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "census_data = CensusVector(delta_census, delta_reviews)\n",
        "\n",
        "sizes_c = [lath[2011].shape[1], delta_reviews.shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceWeX7xvUTwm",
        "colab_type": "code",
        "outputId": "51262858-3675-4f5d-9c4d-424aea0cf820",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=78, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9js1V34UXd9",
        "colab_type": "code",
        "outputId": "cb269cd8-79a3-4da8-cb04-ab32e7acdecd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='mt_raw')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000299603\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000365727\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000254318\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000354790\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000228996\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000356740\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000213215\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000352305\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000200729\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000356980\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000190719\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000368367\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000178601\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000361702\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000173854\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000366101\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000159097\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000372322\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000150221\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000366738\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000146557\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000370612\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000139455\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000376333\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000126597\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000381633\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000124344\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000395894\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000116421\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000370512\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000109915\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000379733\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000106333\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000384527\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000108374\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000395049\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000105330\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000386832\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000103127\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000387422\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000100288\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000401046\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000089216\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000386920\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000087360\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000396412\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000082781\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000394778\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000078032\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000408693\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000067885\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000398776\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000065586\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000396897\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000064304\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000397105\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000062910\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000396891\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000062093\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000396382\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000061506\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000396891\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000060996\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0000397311\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000060458\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0000397027\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000060078\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0000397254\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000168559\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000145938\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000164507\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000147065\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000161113\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000162860\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000129326\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000143141\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000121274\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000135324\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000112951\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000137545\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000110711\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000154872\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000099756\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000143951\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000112047\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000183812\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000094695\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000159747\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000088141\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000168036\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000098220\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000168380\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000094984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000165696\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000088994\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000162385\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000089961\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000174077\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000083810\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000176660\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000083666\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000170030\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000077723\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000179884\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000072834\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000182935\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000079325\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000210683\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000062019\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000185439\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000058095\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000189098\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000055956\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000194595\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000055307\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000194333\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000053973\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000201535\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000053432\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000203090\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000047078\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000202280\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000041741\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000201303\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000039217\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000201251\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000037856\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000200988\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000036364\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000202095\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000035582\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0000202071\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000035045\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0000202813\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000034438\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0000203050\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000034032\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0000203122\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000102407\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000101594\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000108269\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000124699\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000113265\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000142299\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000109146\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000137288\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000064371\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000106124\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000060159\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000108329\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000060214\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000122623\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000058531\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000126414\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000055574\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000131353\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000055142\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000130521\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000058530\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000140441\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000062740\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000150876\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000062368\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000151825\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000075100\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000175345\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000062636\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000164779\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000056675\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000166894\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000046295\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000168138\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000041833\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000177168\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000037079\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000177915\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000036384\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000187894\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000035181\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000188030\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000037341\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000191992\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000032270\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000191232\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000030251\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000191902\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000028621\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000192637\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000027714\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000193388\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000027107\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000193251\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000026548\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000195155\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000026183\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000194810\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000025896\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000195588\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000025704\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000196104\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000062114\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000157078\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000079945\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000180488\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000084230\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000197532\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000050764\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000169832\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000032937\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000158635\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000036272\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000167814\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000030110\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000162754\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000033501\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000168616\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000031504\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000165226\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000034271\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000164616\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000046408\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000176939\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000043099\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000165693\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000035635\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000169349\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000029670\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000176154\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000025241\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000173677\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000022150\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000176066\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000021812\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000176178\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000020744\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000175817\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000028054\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000183620\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000028746\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000192043\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000023517\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000182050\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000022123\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000187490\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000014716\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000183325\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000011741\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000183225\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000009608\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000183067\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000008634\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000183281\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000007603\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000183409\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000007144\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000183330\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000006645\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000183610\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000006368\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000183870\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000006146\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000184118\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000032530\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000151116\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000043654\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000161493\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000033286\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000158994\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000027016\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000160862\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000029218\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000178329\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000026654\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000176695\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000023141\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000178889\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000022354\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000174748\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000023740\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000179215\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000024099\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000179582\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000022514\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000187742\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000021257\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000188742\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000019693\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000193895\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000023252\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000196759\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000022632\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000204525\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000027748\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000211026\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000022172\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000206916\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000022908\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000212708\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000021836\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000221513\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000018597\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000221708\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000021574\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000228829\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000019266\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000223398\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000014007\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000220301\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000011948\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000219406\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000010495\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000220026\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000009019\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000219445\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000008317\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000219254\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000007882\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000219494\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000007528\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000220065\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000007307\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000220558\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000007121\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000220861\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28Nf-Dz1VbMw",
        "colab_type": "code",
        "outputId": "9df64d25-9372-4f72-c724-1dda4409e972",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(range(len(validation_losses)), validation_losses, range(len(training_losses)), training_losses)\n",
        "plt.legend(['Validation Losses', 'Training Losses'])\n",
        "plt.title('Loss curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEWCAYAAABrDZDcAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__TazGaLVoNH",
        "colab_type": "code",
        "outputId": "12b081bd-1cc8-4f1b-d3f6-1c1f46186977",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test Loss\n",
        "criterion_c(census_data.data[test_neighbourhoods], decoder_c(census_data.reviews_embedding[test_neighbourhoods]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.8127e-05, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tndj2D0hVvg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(census_data.reviews_embedding).cpu().detach().numpy()\n",
        "actual_data = lath[2016].values\n",
        "predicted_data = lath[2011].values+ decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV6qN9zQWBAk",
        "colab_type": "code",
        "outputId": "62017772-e396-4ecf-d94d-25a30bd6d8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.02913268008830192"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQRnaAFMWE-4",
        "colab_type": "code",
        "outputId": "7c69f4cd-f1f5-4faf-9644-afb23121debb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((predicted_data[test_neighbourhoods]-actual_data[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2109503225502908"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE0Nz2mQWhHs",
        "colab_type": "text"
      },
      "source": [
        "#### With Additional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GY37oDxWjpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [lath[2011].shape[1], delta_reviews.shape[1]//2, delta_reviews.shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee2cnHFRXIL6",
        "colab_type": "code",
        "outputId": "d4128039-7ba6-427f-e70e-4d45f30b5173",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=512, out_features=78, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVXdg94uXKDN",
        "colab_type": "code",
        "outputId": "e0bd65f7-cb31-49ec-9120-9625cd7e9bc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='edu_raw_2')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000409032\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000459455\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000260803\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000382916\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000421449\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000649226\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000258233\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000487629\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000232668\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000409320\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000208890\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000389595\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000186613\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000380415\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000179980\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000411974\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000136254\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000399936\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000124946\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000397886\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000112695\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000459740\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000086645\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000384755\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000075379\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000419777\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000088608\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000409319\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000098162\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000426369\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000093123\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000411257\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000066384\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000427952\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000069227\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000429296\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000067639\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000457621\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000081791\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000412992\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000062507\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000421240\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000059554\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000440570\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000063487\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000421145\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000070225\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000420223\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000064176\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000462335\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000069804\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000413517\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000062084\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000464949\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000054431\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000434722\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000030476\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000435905\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000024625\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000442256\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000020560\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000439669\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000017501\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0000437869\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000015189\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0000444724\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000013903\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0000442481\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000013001\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0000446755\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000013341\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0000449943\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000011673\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0000448283\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000172230\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000129415\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000143089\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000134699\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000153421\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000138173\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000136426\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000159486\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000167058\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000176297\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000148009\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000153111\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000160010\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000192799\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000132119\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000168842\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000110324\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000174880\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000178392\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000192473\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000201978\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000263178\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000188868\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000215834\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000120464\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000210812\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000079069\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000169404\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000070599\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000167592\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000062245\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000200197\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000040339\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000199444\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000048043\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000208123\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000045936\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000207804\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000105550\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000266812\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000068143\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000221615\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000041868\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000231026\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000023343\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000214286\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000017215\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000211061\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000014422\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000213413\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000012885\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000214228\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000011222\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000217555\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000010629\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000218491\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000010304\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000220517\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000008858\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000222218\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000008504\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000228956\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000122405\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000198956\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000083338\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000173117\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000128893\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000261897\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000192895\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000288035\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000084457\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000185779\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000087654\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000188312\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000099598\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000188326\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000079285\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000204654\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000069104\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000182312\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000074367\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000199697\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000041517\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000217420\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000026008\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000194042\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000040168\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000213076\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000029760\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000211302\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000031203\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000202464\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000034386\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000214774\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000024934\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000217449\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000039119\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000215464\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000030449\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000192233\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000049357\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000206140\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000048089\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000152300\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000053030\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000208002\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000065310\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000244091\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000082292\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000281111\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000110420\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000259633\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000051781\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000251368\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000067713\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000260041\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000056041\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000237104\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000177746\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000209332\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000768856\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000376691\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000702730\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000347043\n",
            "Training Results - Epoch: 32  Avg loss: 0.0001633857\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0000566659\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001338915\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0000480859\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000661418\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0000377359\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000289546\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0000262190\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000095362\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0000248105\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000019082\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0000216663\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000012195\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0000223774\n",
            "Training Results - Epoch: 39  Avg loss: 0.0000008383\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0000221586\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000007504\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0000220946\n",
            "Training Results - Epoch: 41  Avg loss: 0.0000005829\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0000220840\n",
            "Training Results - Epoch: 42  Avg loss: 0.0000005557\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0000224190\n",
            "Training Results - Epoch: 43  Avg loss: 0.0000004207\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0000222160\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000004003\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0000221672\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000003855\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0000221368\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000003750\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0000221625\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000003653\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0000221558\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000003574\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0000221322\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000003473\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0000221348\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000003401\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0000221515\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000003348\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0000221514\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000158571\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000148674\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000237974\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000198722\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000121355\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000193574\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000145285\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000205570\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000046767\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000160559\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000035155\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000138989\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000027396\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000135471\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000037960\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000125347\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000025345\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000126430\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000033622\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000131136\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000038792\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000162812\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000045693\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000168771\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000046342\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000171962\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000046676\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000144990\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000046975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000142564\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000044882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000156411\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000044261\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000171775\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000040641\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000179600\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000031528\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000162088\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000046966\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000190979\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000034243\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000192774\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000046782\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000165655\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000041010\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000180838\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000049920\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000172990\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000057333\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000169131\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000112917\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000204395\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000591852\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000740998\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000288506\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000341734\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001613426\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000655731\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000381198\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000389225\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000172408\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000263895\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000118829\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0000250165\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000086823\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0000233538\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000068504\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0000224269\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000055431\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0000219548\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000045469\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0000212978\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000038908\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0000211690\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000033579\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0000207960\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000096816\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000220741\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000069168\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000279169\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000044100\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000208825\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000058685\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000252246\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000053062\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000268779\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000028487\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000222040\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000035439\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000208325\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000031879\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000235970\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000048308\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000284425\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000034794\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000237761\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000040627\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000237048\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000061913\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000335121\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000044168\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000268085\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000042494\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000231827\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000039130\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000252758\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000047861\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000255340\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000033823\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000293005\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000037694\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000289897\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000033304\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000274269\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000029346\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000224003\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000023328\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000271298\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000023266\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000245055\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000048908\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000266933\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000039447\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000275042\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000051968\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000282012\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000075972\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000254913\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000064995\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000294737\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000063939\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000299569\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000029423\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000261863\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000018586\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000249694\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000013694\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000247060\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000010550\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0000242341\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000007968\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0000242779\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000006537\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0000245274\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000005375\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0000245110\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000004437\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0000245159\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000003938\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0000245479\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu5kttTMXhJ7",
        "colab_type": "code",
        "outputId": "75419adf-af6a-43a7-d9d7-850b64f800c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(range(len(validation_losses)), validation_losses, range(len(training_losses)), training_losses)\n",
        "plt.legend(['Validation Losses', 'Training Losses'])\n",
        "plt.title('Loss curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAEWCAYAAAAgpUMxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNBvQWM8Xlmv",
        "colab_type": "code",
        "outputId": "beae97cf-19f0-4067-f63d-48e56dad3013",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test Loss\n",
        "criterion_c(census_data.data[test_neighbourhoods], decoder_c(census_data.reviews_embedding[test_neighbourhoods]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(8.3936e-05, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqyiM_kxXnNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(census_data.reviews_embedding).cpu().detach().numpy()\n",
        "actual_data = lath[2016].values\n",
        "predicted_data = lath[2011].values+ decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUx6T9e3Xqvu",
        "colab_type": "code",
        "outputId": "110f38d4-7ffa-47ca-fb5e-50175c862dbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.06355199197769561"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R27qAN4XvBj",
        "colab_type": "code",
        "outputId": "56826986-536c-4ae5-fc51-db1b4fa4c74f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((predicted_data[test_neighbourhoods]-actual_data[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18067353250028173"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZoVjqAibPFT",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qqk26vojbJYP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(Encoder, self).__init__()\n",
        "                \n",
        "        layers_en = OrderedDict()       \n",
        "        for i in range(len(sizes)-1):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_en[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            if i==0:\n",
        "                nn.init.xavier_uniform_(layers_en[layer_name].weight)\n",
        "            layers_en[act_name] = nn.Tanh() #-1 to 1\n",
        "        \n",
        "        self.encoder = nn.Sequential(layers_en)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x) \n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        sizes = sizes[::-1]\n",
        "        \n",
        "        layers_de = OrderedDict()\n",
        "        for i in range(len(sizes)-2):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_de[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            layers_de[act_name] = nn.Tanh()\n",
        "\n",
        "        layers_de['linear{}'.format(len(sizes)-1)] = nn.Linear(sizes[-2], sizes[-1])\n",
        "        layers_de['sigmoid'] = nn.Sigmoid() #0 to 1\n",
        "        self.decoder = nn.Sequential(layers_de)\n",
        "\n",
        "    def forward(self, encoded):\n",
        "        return self.decoder(encoded) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQQbaL7TxjKb",
        "colab_type": "text"
      },
      "source": [
        "### Load trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdhvmqz3xrNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X44OFzttxs10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_r = [max_features, 2**10, 2**8, pca_components]\n",
        "encoder = Encoder(sizes_r)\n",
        "decoder = Decoder(sizes_r)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGf84g_Oh34S",
        "colab_type": "code",
        "outputId": "75387363-22a8-435e-c273-b11b3338d16b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "encoder.load_state_dict(torch.load('/content/drive/My Drive/Thesis2019/models/review_encoder2_500.pth'))\n",
        "encoder.eval()\n",
        "encoder.to(torch.device(\"cuda\"))\n",
        "\n",
        "decoder.load_state_dict(torch.load('/content/drive/My Drive/Thesis2019/models/review_decoder2_500.pth'))\n",
        "decoder.eval()\n",
        "decoder.to(torch.device(\"cuda\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=16, out_features=256, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=256, out_features=1024, bias=True)\n",
              "    (activation2): Tanh()\n",
              "    (linear3): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71OoLpU43hWF",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH7z_0fK3vee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = RidgeCV(cv=5)\n",
        "delta_census = lath[2016].values - lath[2011].values\n",
        "delta_embedding = (encoder(reviews[2016].data) - encoder(reviews[2011].data)).detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ApOZxNV3xtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr.fit(delta_embedding[train_val_neighbourhoods], delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6IgNOM03y5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_change = lr.predict(delta_embedding[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GC53L79OzAuS",
        "colab_type": "code",
        "outputId": "8ce53794-0dc3-42c2-b83f-3aaf445b5b59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12207533693150446"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZM4QZ_tazKTy",
        "colab_type": "code",
        "outputId": "7a05eaff-e638-41cc-ea9e-0458675b21dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((lr.predict(delta_embedding[test_neighbourhoods])-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.12103099693519383"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG5ldoCryItc",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Non-Linear Regression (Census Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK0G6Xm--bxQ",
        "colab_type": "code",
        "outputId": "5eaded01-8721-40c1-f807-3b633123e0f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "delta_embedding = encoder(reviews[2016].data) - encoder(reviews[2011].data)\n",
        "delta_census = lath[2016].values - lath[2011].values\n",
        "census_data = CensusVector(delta_census, delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N13NFMvWJZRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [lath[2011].shape[1], pca_components]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvBlIbgHOMbH",
        "colab_type": "code",
        "outputId": "2007636f-7270-467e-d4a3-82280a28ce11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i])\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0100058220\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0112684306\n",
            "Training Results - Epoch: 2  Avg loss: 0.0043881810\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0057412855\n",
            "Training Results - Epoch: 3  Avg loss: 0.0023859152\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0036368536\n",
            "Training Results - Epoch: 4  Avg loss: 0.0015832255\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0027059970\n",
            "Training Results - Epoch: 5  Avg loss: 0.0012104704\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0022078647\n",
            "Training Results - Epoch: 6  Avg loss: 0.0009680197\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0018602075\n",
            "Training Results - Epoch: 7  Avg loss: 0.0007934476\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0015808542\n",
            "Training Results - Epoch: 8  Avg loss: 0.0006692975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0013835546\n",
            "Training Results - Epoch: 9  Avg loss: 0.0005694671\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0012228279\n",
            "Training Results - Epoch: 10  Avg loss: 0.0004902904\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0010688335\n",
            "Training Results - Epoch: 11  Avg loss: 0.0004260333\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0009526027\n",
            "Training Results - Epoch: 12  Avg loss: 0.0003718672\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0008494087\n",
            "Training Results - Epoch: 13  Avg loss: 0.0003261025\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0007675138\n",
            "Training Results - Epoch: 14  Avg loss: 0.0002890689\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0006876875\n",
            "Training Results - Epoch: 15  Avg loss: 0.0002581004\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0006186548\n",
            "Training Results - Epoch: 16  Avg loss: 0.0002317960\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0005700646\n",
            "Training Results - Epoch: 17  Avg loss: 0.0002088431\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0005091747\n",
            "Training Results - Epoch: 18  Avg loss: 0.0001885978\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004636987\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001705496\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0004151093\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001569667\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0003815604\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001428230\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0003449879\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001309707\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0003160686\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001206136\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002891861\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001097379\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002637128\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001028065\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002451025\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000942642\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002214166\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000878394\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002009063\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000818931\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0001878267\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000758293\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0001708370\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000712815\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0001624939\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000670981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0001486223\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000623816\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0001351630\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000592315\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0001276592\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000553526\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0001164102\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000528283\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0001113595\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000501188\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0001013952\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000487966\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0000958172\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000460237\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0000915665\n",
            "Training Results - Epoch: 39  Avg loss: 0.0000435657\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0000867354\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000413939\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0000806034\n",
            "Training Results - Epoch: 41  Avg loss: 0.0000392445\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0000768544\n",
            "Training Results - Epoch: 42  Avg loss: 0.0000382837\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0000734108\n",
            "Training Results - Epoch: 43  Avg loss: 0.0000369567\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0000684376\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000374416\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0000696404\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000339192\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0000631438\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000332469\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0000617784\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000326161\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0000581081\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000327374\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0000582795\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000312247\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0000554607\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000307470\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0000563384\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000297585\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0000545386\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000292627\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0000514605\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000289989\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0000524275\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000281138\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0000517194\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000275149\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0000484997\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000272971\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0000487179\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000267751\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0000475357\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000264026\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0000471909\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000264039\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0000464635\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000273153\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0000496874\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000264774\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0000477199\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000261004\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0000469596\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000255421\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0000454214\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000259782\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0000465262\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000253442\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0000444308\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000247097\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0000444367\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000260544\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0000452427\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000255992\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0000434765\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000260678\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0000451422\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000258620\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0000454924\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000263995\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0000449570\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000253222\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0000429857\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000259431\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0000454530\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000263524\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0000451690\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000258059\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0000429159\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000280810\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0000453008\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000255267\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0000451408\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000250390\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0000441252\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000255099\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0000457820\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000258935\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0000436516\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000274886\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0000473187\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000270329\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0000460562\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000256096\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0000441958\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000264775\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0000461069\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000243167\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0000438485\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000264655\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0000452158\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000259409\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0000438984\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000249967\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0000435505\n",
            "Training Results - Epoch: 89  Avg loss: 0.0000283492\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0000473719\n",
            "Training Results - Epoch: 90  Avg loss: 0.0000269209\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0000454534\n",
            "Training Results - Epoch: 91  Avg loss: 0.0000256787\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0000438470\n",
            "Training Results - Epoch: 92  Avg loss: 0.0000278602\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0000450976\n",
            "Training Results - Epoch: 93  Avg loss: 0.0000262975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 93  Avg loss: 0.0000451119\n",
            "Training Results - Epoch: 94  Avg loss: 0.0000277328\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 94  Avg loss: 0.0000481979\n",
            "Training Results - Epoch: 95  Avg loss: 0.0000285527\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 95  Avg loss: 0.0000457416\n",
            "Training Results - Epoch: 96  Avg loss: 0.0000265826\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 96  Avg loss: 0.0000451702\n",
            "Training Results - Epoch: 97  Avg loss: 0.0000240257\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 97  Avg loss: 0.0000424702\n",
            "Training Results - Epoch: 98  Avg loss: 0.0000234615\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 98  Avg loss: 0.0000421499\n",
            "Training Results - Epoch: 99  Avg loss: 0.0000232439\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 99  Avg loss: 0.0000420593\n",
            "Training Results - Epoch: 100  Avg loss: 0.0000231240\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 100  Avg loss: 0.0000419999\n",
            "Training Results - Epoch: 101  Avg loss: 0.0000230719\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 101  Avg loss: 0.0000420242\n",
            "Training Results - Epoch: 102  Avg loss: 0.0000230424\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 102  Avg loss: 0.0000420018\n",
            "Training Results - Epoch: 103  Avg loss: 0.0000230120\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 103  Avg loss: 0.0000417514\n",
            "Training Results - Epoch: 104  Avg loss: 0.0000229944\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 104  Avg loss: 0.0000419642\n",
            "Training Results - Epoch: 105  Avg loss: 0.0000229934\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 105  Avg loss: 0.0000418751\n",
            "Training Results - Epoch: 106  Avg loss: 0.0000229745\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 106  Avg loss: 0.0000418374\n",
            "Training Results - Epoch: 107  Avg loss: 0.0000229691\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 107  Avg loss: 0.0000419262\n",
            "Training Results - Epoch: 108  Avg loss: 0.0000229600\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 108  Avg loss: 0.0000419095\n",
            "Training Results - Epoch: 109  Avg loss: 0.0000229570\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 109  Avg loss: 0.0000420831\n",
            "Training Results - Epoch: 110  Avg loss: 0.0000229654\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 110  Avg loss: 0.0000419349\n",
            "Training Results - Epoch: 111  Avg loss: 0.0000229609\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 111  Avg loss: 0.0000419978\n",
            "Training Results - Epoch: 112  Avg loss: 0.0000229511\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 112  Avg loss: 0.0000419046\n",
            "Training Results - Epoch: 113  Avg loss: 0.0000229515\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 113  Avg loss: 0.0000419774\n",
            "Training Results - Epoch: 114  Avg loss: 0.0000229411\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 114  Avg loss: 0.0000417977\n",
            "Training Results - Epoch: 115  Avg loss: 0.0000229461\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 115  Avg loss: 0.0000417507\n",
            "Training Results - Epoch: 116  Avg loss: 0.0000229462\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 116  Avg loss: 0.0000417296\n",
            "Training Results - Epoch: 117  Avg loss: 0.0000229467\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 117  Avg loss: 0.0000419762\n",
            "Training Results - Epoch: 118  Avg loss: 0.0000229539\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 118  Avg loss: 0.0000419267\n",
            "Training Results - Epoch: 119  Avg loss: 0.0000229461\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 119  Avg loss: 0.0000419417\n",
            "Training Results - Epoch: 120  Avg loss: 0.0000229563\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 120  Avg loss: 0.0000417754\n",
            "Training Results - Epoch: 121  Avg loss: 0.0000229451\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 121  Avg loss: 0.0000419619\n",
            "Training Results - Epoch: 122  Avg loss: 0.0000229366\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 122  Avg loss: 0.0000419735\n",
            "Training Results - Epoch: 123  Avg loss: 0.0000229437\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 123  Avg loss: 0.0000419464\n",
            "Training Results - Epoch: 124  Avg loss: 0.0000229419\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 124  Avg loss: 0.0000418980\n",
            "Training Results - Epoch: 125  Avg loss: 0.0000229576\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 125  Avg loss: 0.0000418345\n",
            "Training Results - Epoch: 126  Avg loss: 0.0000229493\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 126  Avg loss: 0.0000418336\n",
            "Training Results - Epoch: 127  Avg loss: 0.0000229554\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 127  Avg loss: 0.0000419318\n",
            "Training Results - Epoch: 128  Avg loss: 0.0000229533\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 128  Avg loss: 0.0000418840\n",
            "Training Results - Epoch: 129  Avg loss: 0.0000229447\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 129  Avg loss: 0.0000418538\n",
            "Training Results - Epoch: 130  Avg loss: 0.0000229522\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 130  Avg loss: 0.0000419170\n",
            "Training Results - Epoch: 131  Avg loss: 0.0000229478\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 131  Avg loss: 0.0000419145\n",
            "Training Results - Epoch: 132  Avg loss: 0.0000229450\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 132  Avg loss: 0.0000419233\n",
            "Training Results - Epoch: 133  Avg loss: 0.0000229793\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 133  Avg loss: 0.0000419613\n",
            "Training Results - Epoch: 134  Avg loss: 0.0000229518\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 134  Avg loss: 0.0000419088\n",
            "Training Results - Epoch: 135  Avg loss: 0.0000229579\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 135  Avg loss: 0.0000419671\n",
            "Training Results - Epoch: 136  Avg loss: 0.0000229419\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 136  Avg loss: 0.0000418346\n",
            "Training Results - Epoch: 137  Avg loss: 0.0000229646\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 137  Avg loss: 0.0000419097\n",
            "Training Results - Epoch: 138  Avg loss: 0.0000229234\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 138  Avg loss: 0.0000418844\n",
            "Training Results - Epoch: 139  Avg loss: 0.0000229126\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 139  Avg loss: 0.0000418606\n",
            "Training Results - Epoch: 140  Avg loss: 0.0000229075\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 140  Avg loss: 0.0000418571\n",
            "Training Results - Epoch: 141  Avg loss: 0.0000229055\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 141  Avg loss: 0.0000418543\n",
            "Training Results - Epoch: 142  Avg loss: 0.0000229040\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 142  Avg loss: 0.0000418724\n",
            "Training Results - Epoch: 143  Avg loss: 0.0000229030\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 143  Avg loss: 0.0000418594\n",
            "Training Results - Epoch: 144  Avg loss: 0.0000229024\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 144  Avg loss: 0.0000418727\n",
            "Training Results - Epoch: 145  Avg loss: 0.0000229019\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 145  Avg loss: 0.0000418719\n",
            "Training Results - Epoch: 146  Avg loss: 0.0000229019\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 146  Avg loss: 0.0000418719\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000342749\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000296888\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000314860\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000287595\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000330071\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000305383\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000317830\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000301812\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000300284\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000294328\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000335053\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000307466\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000409050\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000344527\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000328955\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000316283\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000317108\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000334601\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000304362\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000310243\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000297015\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000311084\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000310674\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000306378\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000296745\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000307302\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000303225\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000304606\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000308124\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000310370\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000309336\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000321224\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000309655\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000317058\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000301210\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000317341\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000298676\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000321322\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000300428\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000319068\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000380595\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000384562\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000345483\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000353065\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000304823\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000319056\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000280600\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000296735\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000274908\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000294523\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000272975\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000293393\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000271637\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000292867\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000270924\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000292944\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000270471\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000292154\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000270274\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000292622\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000270115\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000291917\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000269862\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0000292324\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000349285\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000295152\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000331770\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000336864\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000324987\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000343655\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000321632\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000356178\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000305778\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000358839\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000310777\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000368600\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000321948\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000396896\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000329255\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000407846\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000337806\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000441956\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000321021\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000444968\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000300274\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000434204\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000301807\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000438315\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000315744\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000456229\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000304295\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000447568\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000307603\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000471518\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000297686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000464164\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000325157\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000492914\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000358299\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000560694\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000343794\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000552949\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000307268\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000519490\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000311163\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000492946\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000311629\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000506532\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000283569\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000480447\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000277149\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000471661\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000274184\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000467495\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000272798\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000463559\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000272301\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000463753\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000271762\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000463315\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000271496\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000462527\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000271366\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000459707\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000271360\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000460316\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000306388\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000465674\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000292421\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000458766\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000260037\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000437185\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000258275\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000440123\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000258089\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000432603\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000264920\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000448129\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000258032\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000459508\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000297197\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000483019\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000273667\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000478237\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000279243\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000493818\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000257809\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000465972\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000250149\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000466308\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000256217\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000467435\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000282142\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000513722\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000275269\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000479024\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000266933\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000470071\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000245324\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000461099\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000250148\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000466915\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000268576\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000489744\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000260523\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000478245\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000260861\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000462340\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000263788\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000500077\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000271646\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000489992\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000255715\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000476175\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000248615\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000468112\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000242071\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000479807\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000221687\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000457529\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000218897\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000452461\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000217620\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000451146\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000217031\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000449297\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000216743\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000447456\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000216542\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0000447594\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000216377\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0000448074\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000216279\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0000447400\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000216296\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0000445559\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000330156\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000279430\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000339073\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000292171\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000321522\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000298817\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000330673\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000382827\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000325481\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000350696\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000327203\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000385457\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000316769\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000390732\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000324548\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000418944\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000311403\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000435435\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000315109\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000411836\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000322918\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000456062\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000314048\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000437487\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000326524\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000492735\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000327215\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000464985\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000328281\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000520307\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000321724\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000471537\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000327406\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000435413\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000319297\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000469647\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000308810\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000479764\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000316653\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000496721\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000311838\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000457580\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000315162\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000503181\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000292360\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000474192\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000287995\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000467297\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000285986\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000462297\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000284967\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000460627\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000284426\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000459099\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000284031\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000456668\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000283728\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000456353\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000283501\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000453424\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000283379\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000453550\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVsY58vBYt1u",
        "colab_type": "code",
        "outputId": "ac4cff3a-35da-44fb-de46-73b6d032dfe9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        }
      },
      "source": [
        "plt.plot(range(len(validation_losses)), validation_losses, range(len(training_losses)), training_losses)\n",
        "plt.savefig(DRIVE_PATH.joinpath('loss.png'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8VlJwKMyxKzY",
        "colab_type": "code",
        "outputId": "47700065-4fe7-4c02-c93c-99479a1c8eed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test Loss\n",
        "criterion_c(census_data.data[test_neighbourhoods], decoder_c(census_data.reviews_embedding[test_neighbourhoods]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(4.1247e-05, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cMq3lkm-zGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "actual_data = lath[2016].values\n",
        "predicted_data = lath[2011].values+ decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7Pnj6AV8jXh",
        "colab_type": "code",
        "outputId": "10eafc9c-55da-4c3c-f05a-fda85ae15e47",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14819325991617496"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0Sp-VUP0dDe",
        "colab_type": "code",
        "outputId": "d56359ea-f958-4a51-9783-a6700fd98f32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training error\n",
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.17444296065207543"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc7Acl_o_q8Y",
        "colab_type": "text"
      },
      "source": [
        "## ELMO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBVwfdBT_xAg",
        "colab_type": "text"
      },
      "source": [
        "### Load trained model and built csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYNGkUzM_9Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_elmo_all = {}\n",
        "reviews_elmo = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews_elmo_all[year] = pd.read_csv(DRIVE_PATH.joinpath('elmo_reviews_{}.csv'.format(year)))\n",
        "  reviews_elmo_all[year].set_index('Unnamed: 0', inplace=True)\n",
        "  reviews_elmo[year] = ReviewsVector(reviews_elmo_all[year].T.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWkrab33AUoX",
        "colab_type": "code",
        "outputId": "289c0415-4cca-4c57-fcd4-30c734517730",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "delta_embedding = reviews_elmo[2016].data.T - reviews_elmo[2011].data.T\n",
        "delta_census = lath[2016].values - lath[2011].values\n",
        "census_data = CensusVector(delta_census, delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtFEv6GZAMBR",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK-0krFfAPq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lingreg = RidgeCV(cv=5)\n",
        "lingreg.fit(delta_embedding[train_val_neighbourhoods].detach().cpu().numpy(), delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24j2v2XYqqQZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_change = lingreg.predict(delta_embedding.detach().cpu().numpy())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDvok7RlO9Te",
        "colab_type": "code",
        "outputId": "c879b0e6-7543-469b-ea37-c84d2600c3a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change[train_val_neighbourhoods]-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11931084794328486"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjkWFcjRO-Lv",
        "colab_type": "code",
        "outputId": "d66efd0e-5f74-49dd-c2f3-d13d8fdfd4d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs((predicted_change[test_neighbourhoods]-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.121899702279359"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-NPDm4AooY",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Non-Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUFcqugAwjZ",
        "colab_type": "text"
      },
      "source": [
        "#### With one layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wksmfZieA1ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3OvtNXeA2mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [lath[2011].shape[1], reviews_elmo_all[2016].shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NssE1sABNrK3",
        "colab_type": "code",
        "outputId": "3ce779e2-2d8b-4be8-bf1d-b25b3caedbec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='edu_elmo')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0020757126\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0003870178\n",
            "Training Results - Epoch: 2  Avg loss: 0.0057896024\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0021175532\n",
            "Training Results - Epoch: 3  Avg loss: 0.0021644640\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0003045417\n",
            "Training Results - Epoch: 4  Avg loss: 0.0026804343\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0006112208\n",
            "Training Results - Epoch: 5  Avg loss: 0.0038239381\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0008091827\n",
            "Training Results - Epoch: 6  Avg loss: 0.0031671803\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0006739741\n",
            "Training Results - Epoch: 7  Avg loss: 0.0042581016\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0012319483\n",
            "Training Results - Epoch: 8  Avg loss: 0.0138879325\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0147209736\n",
            "Training Results - Epoch: 9  Avg loss: 0.0050477278\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0020586740\n",
            "Training Results - Epoch: 10  Avg loss: 0.0036137581\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0009632641\n",
            "Training Results - Epoch: 11  Avg loss: 0.0029932780\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0007705606\n",
            "Training Results - Epoch: 12  Avg loss: 0.0042657087\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0022896590\n",
            "Training Results - Epoch: 13  Avg loss: 0.0040799767\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0007773582\n",
            "Training Results - Epoch: 14  Avg loss: 0.0125940758\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0133993678\n",
            "Training Results - Epoch: 15  Avg loss: 0.0025212848\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0003015838\n",
            "Training Results - Epoch: 16  Avg loss: 0.0038111668\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0017116009\n",
            "Training Results - Epoch: 17  Avg loss: 0.0057787792\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0021283591\n",
            "Training Results - Epoch: 18  Avg loss: 0.0031393291\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0012467022\n",
            "Training Results - Epoch: 19  Avg loss: 0.0089002014\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0091728859\n",
            "Training Results - Epoch: 20  Avg loss: 0.0051188509\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0017883173\n",
            "Training Results - Epoch: 21  Avg loss: 0.0032590798\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0012603397\n",
            "Training Results - Epoch: 22  Avg loss: 0.0012642552\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0021983356\n",
            "Training Results - Epoch: 23  Avg loss: 0.0029312922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0004346161\n",
            "Training Results - Epoch: 24  Avg loss: 0.0115461664\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0122244954\n",
            "Training Results - Epoch: 25  Avg loss: 0.0018208448\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0022050503\n",
            "Training Results - Epoch: 26  Avg loss: 0.0029454138\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0008269694\n",
            "Training Results - Epoch: 27  Avg loss: 0.0045572310\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0037008181\n",
            "Training Results - Epoch: 28  Avg loss: 0.0029029425\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0004497680\n",
            "Training Results - Epoch: 29  Avg loss: 0.0029895241\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0007094911\n",
            "Training Results - Epoch: 30  Avg loss: 0.0053830098\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0052920152\n",
            "Training Results - Epoch: 31  Avg loss: 0.0061084492\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0037910828\n",
            "Training Results - Epoch: 32  Avg loss: 0.0024840407\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0009560773\n",
            "Training Results - Epoch: 33  Avg loss: 0.0033277125\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0015439250\n",
            "Training Results - Epoch: 34  Avg loss: 0.0032632187\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0005215857\n",
            "Training Results - Epoch: 35  Avg loss: 0.0103008446\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0108710815\n",
            "Training Results - Epoch: 36  Avg loss: 0.0033134583\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0004156689\n",
            "Training Results - Epoch: 37  Avg loss: 0.0020784777\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0002481427\n",
            "Training Results - Epoch: 38  Avg loss: 0.0012099026\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0002083966\n",
            "Training Results - Epoch: 39  Avg loss: 0.0006866778\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0002642868\n",
            "Training Results - Epoch: 40  Avg loss: 0.0003947239\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0002415757\n",
            "Training Results - Epoch: 41  Avg loss: 0.0002503819\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0002525186\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001948616\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0002011827\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001170645\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0002243316\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000938625\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002102775\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000787050\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0002091987\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000688259\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0002016260\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000600984\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0002034372\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000522921\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0001936028\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000470274\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0001860228\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000432341\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0001810930\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000390387\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0001774118\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000360237\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0001718401\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000336305\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0001696194\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000314355\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0001652548\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000297063\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0001630941\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000279067\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0001594888\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000266517\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0001562219\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000252783\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0001545593\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000245355\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0001542211\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000238424\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0001553041\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000228362\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0001499143\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000221732\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0001485457\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000217274\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001467402\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000215708\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0001477649\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000207637\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0001465924\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000202946\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0001432954\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000201731\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0001453327\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000199974\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0001408172\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000193953\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0001433551\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000195037\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0001451118\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000192709\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0001440303\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000191042\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0001374626\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000189299\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0001459701\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000185394\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0001446539\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000185178\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0001369975\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000180229\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0001385485\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000179911\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0001398826\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000178146\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0001397385\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000175889\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0001402740\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000175739\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0001416383\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000182215\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0001362680\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000177617\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0001468906\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000176580\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0001376513\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000171779\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0001419668\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000169268\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0001400491\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000166281\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0001371726\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000170975\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0001364036\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000162634\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0001396951\n",
            "Training Results - Epoch: 89  Avg loss: 0.0000184157\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0001344529\n",
            "Training Results - Epoch: 90  Avg loss: 0.0000230696\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0001549697\n",
            "Training Results - Epoch: 91  Avg loss: 0.0000242806\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0001348888\n",
            "Training Results - Epoch: 92  Avg loss: 0.0000350638\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0001494758\n",
            "Training Results - Epoch: 93  Avg loss: 0.0000474421\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 93  Avg loss: 0.0001569203\n",
            "Training Results - Epoch: 94  Avg loss: 0.0000416950\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 94  Avg loss: 0.0001464144\n",
            "Training Results - Epoch: 95  Avg loss: 0.0000653845\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 95  Avg loss: 0.0001525117\n",
            "Training Results - Epoch: 96  Avg loss: 0.0000959076\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 96  Avg loss: 0.0001628842\n",
            "Training Results - Epoch: 97  Avg loss: 0.0000928667\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 97  Avg loss: 0.0001461615\n",
            "Training Results - Epoch: 98  Avg loss: 0.0003632135\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 98  Avg loss: 0.0004079418\n",
            "Training Results - Epoch: 99  Avg loss: 0.0003911163\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 99  Avg loss: 0.0005104422\n",
            "Training Results - Epoch: 100  Avg loss: 0.0001257294\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 100  Avg loss: 0.0001658400\n",
            "Training Results - Epoch: 101  Avg loss: 0.0000954922\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 101  Avg loss: 0.0001441184\n",
            "Training Results - Epoch: 102  Avg loss: 0.0000373291\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 102  Avg loss: 0.0001443127\n",
            "Training Results - Epoch: 103  Avg loss: 0.0001687339\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 103  Avg loss: 0.0001742022\n",
            "Training Results - Epoch: 104  Avg loss: 0.0002487450\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 104  Avg loss: 0.0003050421\n",
            "Training Results - Epoch: 105  Avg loss: 0.0000980640\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 105  Avg loss: 0.0002212015\n",
            "Training Results - Epoch: 106  Avg loss: 0.0000767089\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 106  Avg loss: 0.0001501536\n",
            "Training Results - Epoch: 107  Avg loss: 0.0001091179\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 107  Avg loss: 0.0001873233\n",
            "Training Results - Epoch: 108  Avg loss: 0.0000417997\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 108  Avg loss: 0.0001872766\n",
            "Training Results - Epoch: 109  Avg loss: 0.0000558526\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 109  Avg loss: 0.0001441009\n",
            "Training Results - Epoch: 110  Avg loss: 0.0000342664\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 110  Avg loss: 0.0001401206\n",
            "Training Results - Epoch: 111  Avg loss: 0.0000273656\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 111  Avg loss: 0.0001395223\n",
            "Training Results - Epoch: 112  Avg loss: 0.0000233664\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 112  Avg loss: 0.0001390459\n",
            "Training Results - Epoch: 113  Avg loss: 0.0000194358\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 113  Avg loss: 0.0001383028\n",
            "Training Results - Epoch: 114  Avg loss: 0.0000176177\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 114  Avg loss: 0.0001381920\n",
            "Training Results - Epoch: 115  Avg loss: 0.0000164670\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 115  Avg loss: 0.0001374038\n",
            "Training Results - Epoch: 116  Avg loss: 0.0000154911\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 116  Avg loss: 0.0001377024\n",
            "Training Results - Epoch: 117  Avg loss: 0.0000149623\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 117  Avg loss: 0.0001376739\n",
            "Training Results - Epoch: 118  Avg loss: 0.0000146875\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 118  Avg loss: 0.0001374979\n",
            "Training Results - Epoch: 119  Avg loss: 0.0000144577\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 119  Avg loss: 0.0001377140\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0005562951\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0004705266\n",
            "Training Results - Epoch: 2  Avg loss: 0.0011150095\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0020819083\n",
            "Training Results - Epoch: 3  Avg loss: 0.0015482515\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0023970798\n",
            "Training Results - Epoch: 4  Avg loss: 0.0012075104\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0012901970\n",
            "Training Results - Epoch: 5  Avg loss: 0.0004697329\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0003994932\n",
            "Training Results - Epoch: 6  Avg loss: 0.0009077866\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0007846520\n",
            "Training Results - Epoch: 7  Avg loss: 0.0023425006\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0064329635\n",
            "Training Results - Epoch: 8  Avg loss: 0.0009676242\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002750042\n",
            "Training Results - Epoch: 9  Avg loss: 0.0018813369\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0040482578\n",
            "Training Results - Epoch: 10  Avg loss: 0.0006229910\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0003620261\n",
            "Training Results - Epoch: 11  Avg loss: 0.0003682218\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0004819556\n",
            "Training Results - Epoch: 12  Avg loss: 0.0007341918\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0011089327\n",
            "Training Results - Epoch: 13  Avg loss: 0.0003479643\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0007134458\n",
            "Training Results - Epoch: 14  Avg loss: 0.0010551130\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0009227782\n",
            "Training Results - Epoch: 15  Avg loss: 0.0013921240\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0021249573\n",
            "Training Results - Epoch: 16  Avg loss: 0.0023584886\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0024035276\n",
            "Training Results - Epoch: 17  Avg loss: 0.0003762695\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0002317785\n",
            "Training Results - Epoch: 18  Avg loss: 0.0005195114\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004707439\n",
            "Training Results - Epoch: 19  Avg loss: 0.0003508475\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0005469470\n",
            "Training Results - Epoch: 20  Avg loss: 0.0014976636\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0040013752\n",
            "Training Results - Epoch: 21  Avg loss: 0.0005677671\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0008453330\n",
            "Training Results - Epoch: 22  Avg loss: 0.0019692977\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0030412681\n",
            "Training Results - Epoch: 23  Avg loss: 0.0005150240\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0005317329\n",
            "Training Results - Epoch: 24  Avg loss: 0.0003752295\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0007306054\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002929238\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002607215\n",
            "Training Results - Epoch: 26  Avg loss: 0.0007779952\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0007763130\n",
            "Training Results - Epoch: 27  Avg loss: 0.0011511568\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0010389504\n",
            "Training Results - Epoch: 28  Avg loss: 0.0012619726\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0010066195\n",
            "Training Results - Epoch: 29  Avg loss: 0.0007800102\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002417912\n",
            "Training Results - Epoch: 30  Avg loss: 0.0014819951\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0016083643\n",
            "Training Results - Epoch: 31  Avg loss: 0.0008338342\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0006343552\n",
            "Training Results - Epoch: 32  Avg loss: 0.0004411792\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0004829924\n",
            "Training Results - Epoch: 33  Avg loss: 0.0007042491\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0006376936\n",
            "Training Results - Epoch: 34  Avg loss: 0.0003219196\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0005684508\n",
            "Training Results - Epoch: 35  Avg loss: 0.0007970674\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0011639577\n",
            "Training Results - Epoch: 36  Avg loss: 0.0005797013\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0006280579\n",
            "Training Results - Epoch: 37  Avg loss: 0.0003096617\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0006710045\n",
            "Training Results - Epoch: 38  Avg loss: 0.0004123015\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0006563034\n",
            "Training Results - Epoch: 39  Avg loss: 0.0002316501\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0002407517\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001628540\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003386888\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001073567\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0002614694\n",
            "Training Results - Epoch: 42  Avg loss: 0.0000823477\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0002063514\n",
            "Training Results - Epoch: 43  Avg loss: 0.0000619059\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0002475045\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000469909\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0001717026\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000397694\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0001656314\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000299137\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0001688197\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000263274\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0001750686\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000235518\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0001602950\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000209286\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0001521537\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000221681\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0001600081\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000180618\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0001534776\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000168059\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0001528163\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000159017\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0001446988\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000159296\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0001504033\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000148827\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0001435530\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000143423\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0001381341\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000139875\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0001452712\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000137059\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0001453100\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000132305\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0001449711\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000130249\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0001433476\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000127875\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0001399789\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000135466\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0001457313\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000125704\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001470118\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000125035\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0001470908\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000121335\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0001463772\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000121682\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0001491260\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000122462\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0001371507\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000118893\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0001483697\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000116094\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0001495939\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000115960\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0001531559\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000112706\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0001485446\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000112823\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0001523939\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000112240\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0001495560\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000110434\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0001470577\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000125057\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0001564082\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000111163\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0001488718\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000108504\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0001526573\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000110102\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0001461047\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000107230\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0001534535\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000104811\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0001555926\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000105957\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0001631339\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000104346\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0001541438\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000102405\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0001609223\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000108281\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0001661485\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000112039\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0001599157\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000099873\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0001643413\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000104347\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0001680688\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000102629\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0001719359\n",
            "Training Results - Epoch: 89  Avg loss: 0.0000095889\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0001637157\n",
            "Training Results - Epoch: 90  Avg loss: 0.0000095282\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0001630342\n",
            "Training Results - Epoch: 91  Avg loss: 0.0000094941\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0001624022\n",
            "Training Results - Epoch: 92  Avg loss: 0.0000094671\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0001627205\n",
            "Training Results - Epoch: 93  Avg loss: 0.0000094508\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 93  Avg loss: 0.0001638076\n",
            "Training Results - Epoch: 94  Avg loss: 0.0000094374\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 94  Avg loss: 0.0001638686\n",
            "Training Results - Epoch: 95  Avg loss: 0.0000094231\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 95  Avg loss: 0.0001639839\n",
            "Training Results - Epoch: 96  Avg loss: 0.0000094096\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 96  Avg loss: 0.0001629045\n",
            "Training Results - Epoch: 97  Avg loss: 0.0000093869\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 97  Avg loss: 0.0001626143\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0012214758\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0019569654\n",
            "Training Results - Epoch: 2  Avg loss: 0.0036370280\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0075524060\n",
            "Training Results - Epoch: 3  Avg loss: 0.0009417800\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0011158248\n",
            "Training Results - Epoch: 4  Avg loss: 0.0004322246\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0005145525\n",
            "Training Results - Epoch: 5  Avg loss: 0.0006600738\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0010145565\n",
            "Training Results - Epoch: 6  Avg loss: 0.0005204944\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0007234891\n",
            "Training Results - Epoch: 7  Avg loss: 0.0008125587\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0020651280\n",
            "Training Results - Epoch: 8  Avg loss: 0.0006026306\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0009706401\n",
            "Training Results - Epoch: 9  Avg loss: 0.0004833905\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0005422422\n",
            "Training Results - Epoch: 10  Avg loss: 0.0006128955\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0005695291\n",
            "Training Results - Epoch: 11  Avg loss: 0.0006316432\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0003779251\n",
            "Training Results - Epoch: 12  Avg loss: 0.0011181623\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0020321302\n",
            "Training Results - Epoch: 13  Avg loss: 0.0006763409\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0002614641\n",
            "Training Results - Epoch: 14  Avg loss: 0.0012060211\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0009228394\n",
            "Training Results - Epoch: 15  Avg loss: 0.0007613444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0003827672\n",
            "Training Results - Epoch: 16  Avg loss: 0.0023355418\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0035625444\n",
            "Training Results - Epoch: 17  Avg loss: 0.0009832668\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0014643208\n",
            "Training Results - Epoch: 18  Avg loss: 0.0009155456\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0005107196\n",
            "Training Results - Epoch: 19  Avg loss: 0.0006262294\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0003348385\n",
            "Training Results - Epoch: 20  Avg loss: 0.0009953491\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0016984961\n",
            "Training Results - Epoch: 21  Avg loss: 0.0014320058\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0033749985\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001362103\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0002149119\n",
            "Training Results - Epoch: 23  Avg loss: 0.0004273688\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0005787278\n",
            "Training Results - Epoch: 24  Avg loss: 0.0006666599\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0009395896\n",
            "Training Results - Epoch: 25  Avg loss: 0.0004654152\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002596333\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002346164\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0004073896\n",
            "Training Results - Epoch: 27  Avg loss: 0.0006678002\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0001973822\n",
            "Training Results - Epoch: 28  Avg loss: 0.0008430003\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0006176290\n",
            "Training Results - Epoch: 29  Avg loss: 0.0011861548\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0008052783\n",
            "Training Results - Epoch: 30  Avg loss: 0.0010600897\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0022653289\n",
            "Training Results - Epoch: 31  Avg loss: 0.0009754350\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0006155682\n",
            "Training Results - Epoch: 32  Avg loss: 0.0010326301\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0020836906\n",
            "Training Results - Epoch: 33  Avg loss: 0.0041696258\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0053674978\n",
            "Training Results - Epoch: 34  Avg loss: 0.0007150445\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0031067068\n",
            "Training Results - Epoch: 35  Avg loss: 0.0003241921\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0004267512\n",
            "Training Results - Epoch: 36  Avg loss: 0.0002792514\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0012021690\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002917303\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0009704531\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002307997\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0005650761\n",
            "Training Results - Epoch: 39  Avg loss: 0.0003768692\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0005309695\n",
            "Training Results - Epoch: 40  Avg loss: 0.0005563606\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003450625\n",
            "Training Results - Epoch: 41  Avg loss: 0.0005677006\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0002424496\n",
            "Training Results - Epoch: 42  Avg loss: 0.0004643987\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003305848\n",
            "Training Results - Epoch: 43  Avg loss: 0.0018412336\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0046910758\n",
            "Training Results - Epoch: 44  Avg loss: 0.0013806487\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0023733157\n",
            "Training Results - Epoch: 45  Avg loss: 0.0017863590\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0027419774\n",
            "Training Results - Epoch: 46  Avg loss: 0.0017964538\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0016044363\n",
            "Training Results - Epoch: 47  Avg loss: 0.0018004948\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0041053160\n",
            "Training Results - Epoch: 48  Avg loss: 0.0013418747\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0009302065\n",
            "Training Results - Epoch: 49  Avg loss: 0.0006904152\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0003345754\n",
            "Training Results - Epoch: 50  Avg loss: 0.0004124176\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0002136929\n",
            "Training Results - Epoch: 51  Avg loss: 0.0002739300\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0002204106\n",
            "Training Results - Epoch: 52  Avg loss: 0.0002152967\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0002346045\n",
            "Training Results - Epoch: 53  Avg loss: 0.0001488212\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0002222289\n",
            "Training Results - Epoch: 54  Avg loss: 0.0001033566\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0002537959\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000833530\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0002443067\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000737037\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0002126060\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000565959\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0002566524\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0006911676\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0006259161\n",
            "Training Results - Epoch: 2  Avg loss: 0.0014048846\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0010657486\n",
            "Training Results - Epoch: 3  Avg loss: 0.0007101586\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0006493303\n",
            "Training Results - Epoch: 4  Avg loss: 0.0002426475\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0003558275\n",
            "Training Results - Epoch: 5  Avg loss: 0.0002260593\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002763843\n",
            "Training Results - Epoch: 6  Avg loss: 0.0002093978\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002713545\n",
            "Training Results - Epoch: 7  Avg loss: 0.0005792062\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0007855363\n",
            "Training Results - Epoch: 8  Avg loss: 0.0019281789\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0042743682\n",
            "Training Results - Epoch: 9  Avg loss: 0.0022641517\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0049395664\n",
            "Training Results - Epoch: 10  Avg loss: 0.0011206848\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0008824222\n",
            "Training Results - Epoch: 11  Avg loss: 0.0015602043\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0019085112\n",
            "Training Results - Epoch: 12  Avg loss: 0.0014254217\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0016803897\n",
            "Training Results - Epoch: 13  Avg loss: 0.0065162738\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0108674687\n",
            "Training Results - Epoch: 14  Avg loss: 0.0028101886\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0019276271\n",
            "Training Results - Epoch: 15  Avg loss: 0.0016326319\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0026733456\n",
            "Training Results - Epoch: 16  Avg loss: 0.0005001629\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0006370027\n",
            "Training Results - Epoch: 17  Avg loss: 0.0014775413\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0066008458\n",
            "Training Results - Epoch: 18  Avg loss: 0.0006129785\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0011556113\n",
            "Training Results - Epoch: 19  Avg loss: 0.0002756591\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002565240\n",
            "Training Results - Epoch: 20  Avg loss: 0.0026801152\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0053920617\n",
            "Training Results - Epoch: 21  Avg loss: 0.0003340213\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0001949032\n",
            "Training Results - Epoch: 22  Avg loss: 0.0002634847\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0001784571\n",
            "Training Results - Epoch: 23  Avg loss: 0.0004166388\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0004884926\n",
            "Training Results - Epoch: 24  Avg loss: 0.0003868040\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002671126\n",
            "Training Results - Epoch: 25  Avg loss: 0.0009193391\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0013473227\n",
            "Training Results - Epoch: 26  Avg loss: 0.0009896868\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0009191124\n",
            "Training Results - Epoch: 27  Avg loss: 0.0003056297\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0005002941\n",
            "Training Results - Epoch: 28  Avg loss: 0.0003927379\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0010795706\n",
            "Training Results - Epoch: 29  Avg loss: 0.0003038949\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003848213\n",
            "Training Results - Epoch: 30  Avg loss: 0.0004733350\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0010747670\n",
            "Training Results - Epoch: 31  Avg loss: 0.0006747180\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0009228122\n",
            "Training Results - Epoch: 32  Avg loss: 0.0008268094\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003771235\n",
            "Training Results - Epoch: 33  Avg loss: 0.0012608168\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0010256824\n",
            "Training Results - Epoch: 34  Avg loss: 0.0008291778\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0004417314\n",
            "Training Results - Epoch: 35  Avg loss: 0.0007133702\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0008471763\n",
            "Training Results - Epoch: 36  Avg loss: 0.0035896789\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0112039608\n",
            "Training Results - Epoch: 37  Avg loss: 0.0014798193\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0034617909\n",
            "Training Results - Epoch: 38  Avg loss: 0.0010981097\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0052372518\n",
            "Training Results - Epoch: 39  Avg loss: 0.0011950884\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0002972492\n",
            "Training Results - Epoch: 40  Avg loss: 0.0005336359\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0002524045\n",
            "Training Results - Epoch: 41  Avg loss: 0.0003871398\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003242286\n",
            "Training Results - Epoch: 42  Avg loss: 0.0004427509\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0005159362\n",
            "Training Results - Epoch: 43  Avg loss: 0.0004667474\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003165845\n",
            "Training Results - Epoch: 44  Avg loss: 0.0003013034\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002182055\n",
            "Training Results - Epoch: 45  Avg loss: 0.0001661086\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0001807650\n",
            "Training Results - Epoch: 46  Avg loss: 0.0001083916\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0001926763\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000715899\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0001857172\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000513036\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0001876523\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000394947\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0001877600\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000349395\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0001803360\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000275435\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0001786986\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000250604\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0001780474\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000218325\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0001780124\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000204353\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0001752354\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000190962\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0001722186\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000182912\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0001716783\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000176337\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0001698985\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000162734\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0001690188\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000178306\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0001797150\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000154601\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0001737752\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000155333\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0001720689\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000133487\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0001676011\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000131516\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001699619\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000127599\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0001670177\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000119165\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0001676310\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000117978\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0001657453\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000111125\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0001688824\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000109562\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0001665546\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000105265\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0001702074\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000105868\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0001650478\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000101228\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0001696458\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000097776\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0001700707\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000095198\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0001692683\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000100257\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0001711986\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000093784\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0001648510\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000087405\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0001704610\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000095083\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0001670117\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000084668\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0001674972\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000086234\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0001713596\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000083662\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0001691203\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000082058\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0001748197\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000078733\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0001709635\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000080976\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0001729604\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000077116\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0001737714\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000077959\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0001705984\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000077305\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0001753015\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000074663\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0001729658\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000077302\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0001730313\n",
            "Training Results - Epoch: 89  Avg loss: 0.0000075529\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0001783736\n",
            "Training Results - Epoch: 90  Avg loss: 0.0000073281\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0001765059\n",
            "Training Results - Epoch: 91  Avg loss: 0.0000071889\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0001755995\n",
            "Training Results - Epoch: 92  Avg loss: 0.0000074463\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0001770470\n",
            "Training Results - Epoch: 93  Avg loss: 0.0000072024\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 93  Avg loss: 0.0001732694\n",
            "Training Results - Epoch: 94  Avg loss: 0.0000079361\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 94  Avg loss: 0.0001872617\n",
            "Training Results - Epoch: 95  Avg loss: 0.0000072533\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 95  Avg loss: 0.0001823660\n",
            "Training Results - Epoch: 96  Avg loss: 0.0000080386\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 96  Avg loss: 0.0001821389\n",
            "Training Results - Epoch: 97  Avg loss: 0.0000069804\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 97  Avg loss: 0.0001788663\n",
            "Training Results - Epoch: 98  Avg loss: 0.0000068450\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 98  Avg loss: 0.0001779558\n",
            "Training Results - Epoch: 99  Avg loss: 0.0000067599\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 99  Avg loss: 0.0001783623\n",
            "Training Results - Epoch: 100  Avg loss: 0.0000067169\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 100  Avg loss: 0.0001786038\n",
            "Training Results - Epoch: 101  Avg loss: 0.0000066806\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 101  Avg loss: 0.0001793072\n",
            "Training Results - Epoch: 102  Avg loss: 0.0000066575\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 102  Avg loss: 0.0001791462\n",
            "Training Results - Epoch: 103  Avg loss: 0.0000066414\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 103  Avg loss: 0.0001794739\n",
            "Training Results - Epoch: 104  Avg loss: 0.0000066226\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 104  Avg loss: 0.0001790171\n",
            "Training Results - Epoch: 105  Avg loss: 0.0000066096\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 105  Avg loss: 0.0001791673\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0001873444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0003540650\n",
            "Training Results - Epoch: 2  Avg loss: 0.0006714986\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0007100318\n",
            "Training Results - Epoch: 3  Avg loss: 0.0008049676\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0014484989\n",
            "Training Results - Epoch: 4  Avg loss: 0.0012826891\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0013121052\n",
            "Training Results - Epoch: 5  Avg loss: 0.0014708047\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0020335443\n",
            "Training Results - Epoch: 6  Avg loss: 0.0006477625\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0007713839\n",
            "Training Results - Epoch: 7  Avg loss: 0.0008130710\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0011196371\n",
            "Training Results - Epoch: 8  Avg loss: 0.0001892830\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0005428165\n",
            "Training Results - Epoch: 9  Avg loss: 0.0001666457\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0004925609\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000828897\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0003944118\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001421954\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0003679673\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001093996\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0003339208\n",
            "Training Results - Epoch: 13  Avg loss: 0.0002547641\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0004426324\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000455002\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0002200957\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000645846\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002935316\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001537997\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0004358888\n",
            "Training Results - Epoch: 17  Avg loss: 0.0003252159\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0006731253\n",
            "Training Results - Epoch: 18  Avg loss: 0.0063278731\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0087097089\n",
            "Training Results - Epoch: 19  Avg loss: 0.0023331098\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0032632608\n",
            "Training Results - Epoch: 20  Avg loss: 0.0010925831\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0014043561\n",
            "Training Results - Epoch: 21  Avg loss: 0.0007085215\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0014145711\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003198482\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0005652272\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003049779\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0007726359\n",
            "Training Results - Epoch: 24  Avg loss: 0.0002755394\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0010265735\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001019049\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002925850\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002591797\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0004545026\n",
            "Training Results - Epoch: 27  Avg loss: 0.0002646581\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0012058146\n",
            "Training Results - Epoch: 28  Avg loss: 0.0004008750\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0007332026\n",
            "Training Results - Epoch: 29  Avg loss: 0.0002962314\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0006823281\n",
            "Training Results - Epoch: 30  Avg loss: 0.0009583858\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0014239750\n",
            "Training Results - Epoch: 31  Avg loss: 0.0013259212\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0026119314\n",
            "Training Results - Epoch: 32  Avg loss: 0.0019297451\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0029841329\n",
            "Training Results - Epoch: 33  Avg loss: 0.0013677721\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0018983987\n",
            "Training Results - Epoch: 34  Avg loss: 0.0018840454\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0056230399\n",
            "Training Results - Epoch: 35  Avg loss: 0.0010557535\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0025340510\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001142115\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003477620\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000922729\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003946278\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000800602\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003329815\n",
            "Training Results - Epoch: 39  Avg loss: 0.0000587154\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003259538\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000519154\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003061431\n",
            "Training Results - Epoch: 41  Avg loss: 0.0000411551\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003106687\n",
            "Training Results - Epoch: 42  Avg loss: 0.0000345166\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0002809874\n",
            "Training Results - Epoch: 43  Avg loss: 0.0000361709\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003072768\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000271028\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002646797\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbmY8XxTBIuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "predicted_data = lath[2011].values + decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p60UxdAkPsj4",
        "colab_type": "code",
        "outputId": "40b15485-cfed-4b86-db79-d0b4faa69073",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.2959962495566842"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 99
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JIFWfuGxPt15",
        "colab_type": "code",
        "outputId": "e58f191d-e7fa-4fed-ef45-c38a60225127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4453066074260302"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJXQIgaBZav",
        "colab_type": "text"
      },
      "source": [
        "#### With Additional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPwMX2LsBuoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [lath[2011].shape[1], reviews_elmo_all[2016].shape[1]//2, reviews_elmo_all[2016].shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOBwpUnlPL7T",
        "colab_type": "code",
        "outputId": "7e2ed513-2c4d-418f-af85-3dc45614d3ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=512, out_features=78, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-Wt9cjDBzxJ",
        "colab_type": "code",
        "outputId": "2b2d5ec3-488d-42cc-dea1-935a3953add5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='edu_elmo')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0012632927\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0005507670\n",
            "Training Results - Epoch: 2  Avg loss: 0.0142277758\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0053524103\n",
            "Training Results - Epoch: 3  Avg loss: 0.0038584909\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0008690110\n",
            "Training Results - Epoch: 4  Avg loss: 0.0028182838\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0014757848\n",
            "Training Results - Epoch: 5  Avg loss: 0.0034881001\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0020770685\n",
            "Training Results - Epoch: 6  Avg loss: 0.0015096117\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0004022144\n",
            "Training Results - Epoch: 7  Avg loss: 0.0041949020\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0044009521\n",
            "Training Results - Epoch: 8  Avg loss: 0.0096808845\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0066825170\n",
            "Training Results - Epoch: 9  Avg loss: 0.0130787069\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0113365246\n",
            "Training Results - Epoch: 10  Avg loss: 0.0025989602\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0015453069\n",
            "Training Results - Epoch: 11  Avg loss: 0.0026290989\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0010825743\n",
            "Training Results - Epoch: 12  Avg loss: 0.0091829751\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0034547164\n",
            "Training Results - Epoch: 13  Avg loss: 0.0106270374\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0066919841\n",
            "Training Results - Epoch: 14  Avg loss: 0.0078979910\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0019401530\n",
            "Training Results - Epoch: 15  Avg loss: 0.0100120688\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0075519653\n",
            "Training Results - Epoch: 16  Avg loss: 0.0082908387\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0030487723\n",
            "Training Results - Epoch: 17  Avg loss: 0.0159875980\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0125485304\n",
            "Training Results - Epoch: 18  Avg loss: 0.0075821733\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0043318406\n",
            "Training Results - Epoch: 19  Avg loss: 0.0120232556\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0110667055\n",
            "Training Results - Epoch: 20  Avg loss: 0.0080262409\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0018314021\n",
            "Training Results - Epoch: 21  Avg loss: 0.0033747265\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0035110706\n",
            "Training Results - Epoch: 22  Avg loss: 0.0017783981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0015518720\n",
            "Training Results - Epoch: 23  Avg loss: 0.0038107350\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0005697096\n",
            "Training Results - Epoch: 24  Avg loss: 0.0062566005\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0046539954\n",
            "Training Results - Epoch: 25  Avg loss: 0.0025471535\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0034388572\n",
            "Training Results - Epoch: 26  Avg loss: 0.0027029829\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0009343849\n",
            "Training Results - Epoch: 27  Avg loss: 0.0091038899\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0068008821\n",
            "Training Results - Epoch: 28  Avg loss: 0.0022222706\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0011925304\n",
            "Training Results - Epoch: 29  Avg loss: 0.0006588890\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002902163\n",
            "Training Results - Epoch: 30  Avg loss: 0.0004212653\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0001810321\n",
            "Training Results - Epoch: 31  Avg loss: 0.0002945890\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0001514226\n",
            "Training Results - Epoch: 32  Avg loss: 0.0002243593\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0001198484\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001691099\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0001095319\n",
            "Training Results - Epoch: 34  Avg loss: 0.0001297844\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0000972995\n",
            "Training Results - Epoch: 35  Avg loss: 0.0001060162\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0000944088\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000962502\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0000925199\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000713895\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0000912567\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000610436\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0000859269\n",
            "Training Results - Epoch: 39  Avg loss: 0.0000511771\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0000866415\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000460321\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0000823175\n",
            "Training Results - Epoch: 41  Avg loss: 0.0000402274\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0000833400\n",
            "Training Results - Epoch: 42  Avg loss: 0.0000354016\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0000831571\n",
            "Training Results - Epoch: 43  Avg loss: 0.0000337085\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0000830460\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000306007\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0000836950\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000286381\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0000824373\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000273401\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0000812837\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000276112\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0000801311\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000263431\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0000847525\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000248698\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0000822503\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000239975\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0000824969\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000240773\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0000811172\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000233380\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0000808159\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000237982\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0000863150\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000238482\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0000767279\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000234612\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0000849819\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000257250\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0000805690\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000275557\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0000860462\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000261756\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0000804694\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000313740\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0000843105\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000379007\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0000938128\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000459496\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0000915804\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000938182\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0000980772\n",
            "Training Results - Epoch: 63  Avg loss: 0.0001470127\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001012087\n",
            "Training Results - Epoch: 64  Avg loss: 0.0001444607\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0000971710\n",
            "Training Results - Epoch: 65  Avg loss: 0.0001398133\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0001400076\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000677091\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0000839376\n",
            "Training Results - Epoch: 67  Avg loss: 0.0002304842\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0002527164\n",
            "Training Results - Epoch: 68  Avg loss: 0.0001561330\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0001393886\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000763560\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0001501324\n",
            "Training Results - Epoch: 70  Avg loss: 0.0002370029\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0001632660\n",
            "Training Results - Epoch: 71  Avg loss: 0.0003500426\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0002721106\n",
            "Training Results - Epoch: 72  Avg loss: 0.0006889239\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0004396852\n",
            "Training Results - Epoch: 73  Avg loss: 0.0005151246\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0003676693\n",
            "Training Results - Epoch: 74  Avg loss: 0.0002510870\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0001619667\n",
            "Training Results - Epoch: 75  Avg loss: 0.0001646394\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0001323075\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000914759\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0001071481\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000667170\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0000982743\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000407806\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0000914997\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000348244\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0000880801\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000276063\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0000868153\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000235183\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0000847479\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000222241\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0000837990\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000205756\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0000839481\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000197903\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0000829766\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0012922760\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0012228244\n",
            "Training Results - Epoch: 2  Avg loss: 0.0034229516\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0055752397\n",
            "Training Results - Epoch: 3  Avg loss: 0.0024659208\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0025935349\n",
            "Training Results - Epoch: 4  Avg loss: 0.0012691164\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0010319693\n",
            "Training Results - Epoch: 5  Avg loss: 0.0015685311\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0009769470\n",
            "Training Results - Epoch: 6  Avg loss: 0.0014388002\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0014043781\n",
            "Training Results - Epoch: 7  Avg loss: 0.0008268638\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0008849333\n",
            "Training Results - Epoch: 8  Avg loss: 0.0006850915\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0008800663\n",
            "Training Results - Epoch: 9  Avg loss: 0.0009549182\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0010434113\n",
            "Training Results - Epoch: 10  Avg loss: 0.0004954048\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0003713807\n",
            "Training Results - Epoch: 11  Avg loss: 0.0002238449\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002784286\n",
            "Training Results - Epoch: 12  Avg loss: 0.0003678875\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0005047127\n",
            "Training Results - Epoch: 13  Avg loss: 0.0002891951\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0003989402\n",
            "Training Results - Epoch: 14  Avg loss: 0.0006183177\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0005446722\n",
            "Training Results - Epoch: 15  Avg loss: 0.0009903599\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0005026325\n",
            "Training Results - Epoch: 16  Avg loss: 0.0057970429\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0084598942\n",
            "Training Results - Epoch: 17  Avg loss: 0.0126227138\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0240537605\n",
            "Training Results - Epoch: 18  Avg loss: 0.0069089493\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0111086011\n",
            "Training Results - Epoch: 19  Avg loss: 0.0041598209\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0057848675\n",
            "Training Results - Epoch: 20  Avg loss: 0.0053401056\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0052656578\n",
            "Training Results - Epoch: 21  Avg loss: 0.0009210816\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0009663883\n",
            "Training Results - Epoch: 22  Avg loss: 0.0002512629\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0004548077\n",
            "Training Results - Epoch: 23  Avg loss: 0.0002852395\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003864609\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001296004\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003681829\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002285638\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002316692\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000983006\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002436240\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000752790\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002577297\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000567810\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002024134\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001036070\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002412711\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000733258\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002182918\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000926805\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002509600\n",
            "Training Results - Epoch: 32  Avg loss: 0.0001024904\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0002600969\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000632073\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0002358732\n",
            "Training Results - Epoch: 34  Avg loss: 0.0001414702\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0003497569\n",
            "Training Results - Epoch: 35  Avg loss: 0.0001361557\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003236233\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001077284\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0002877139\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002123020\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0005186718\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002084429\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0007159511\n",
            "Training Results - Epoch: 39  Avg loss: 0.0003797323\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0010728304\n",
            "Training Results - Epoch: 40  Avg loss: 0.0002030735\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003467090\n",
            "Training Results - Epoch: 41  Avg loss: 0.0002645344\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0009579487\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001335363\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0002984059\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001655894\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0002813137\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001056176\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0003418874\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000897649\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0003256079\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000761872\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0002438221\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000744978\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0002275411\n",
            "Training Results - Epoch: 48  Avg loss: 0.0001404367\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0003874878\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000752640\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0003385167\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000403688\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0002241583\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000303763\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0001969819\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000251256\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0001847525\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000225717\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0001751449\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000203108\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0001758101\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000191373\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0001716645\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000182169\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0001697740\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000175778\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0001703992\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000170131\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0001693678\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000166831\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0001705718\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000162248\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0001701420\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000159305\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0001715177\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000157257\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0001729057\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000154669\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001779199\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000153645\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0001764073\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000151114\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0001776493\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000149357\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0001775077\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000149204\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0001729967\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000148479\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0001852085\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000145689\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0001814066\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000145887\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0001914987\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000142576\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0001853672\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000140668\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0001885142\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000139131\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0001915138\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000138926\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0001855093\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000139675\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0001974321\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000137419\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0001942889\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000135405\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0001966051\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000135455\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0001940552\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000133894\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0002046029\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000131226\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0002044872\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000130789\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0002050319\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000130543\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0002052709\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000130422\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0002046647\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000130244\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0002052603\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000130172\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0002050755\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000129900\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0002069953\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000129790\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0002058795\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000129563\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0002076560\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0012159439\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0026026303\n",
            "Training Results - Epoch: 2  Avg loss: 0.0035356851\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0033249739\n",
            "Training Results - Epoch: 3  Avg loss: 0.0021123292\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0032507974\n",
            "Training Results - Epoch: 4  Avg loss: 0.0191143740\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0287537527\n",
            "Training Results - Epoch: 5  Avg loss: 0.0043140290\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0056556856\n",
            "Training Results - Epoch: 6  Avg loss: 0.0024146889\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0016236828\n",
            "Training Results - Epoch: 7  Avg loss: 0.0021514546\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0013235452\n",
            "Training Results - Epoch: 8  Avg loss: 0.0023186024\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0017383293\n",
            "Training Results - Epoch: 9  Avg loss: 0.0006756001\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0006143332\n",
            "Training Results - Epoch: 10  Avg loss: 0.0004480846\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0003968890\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000583305\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002149790\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000320265\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0001751959\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000275843\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0001657331\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000294376\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0001737950\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000320683\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0001776687\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000315444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0001742888\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000300694\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0001620368\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000320037\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0001728176\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000304613\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0001674415\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000344947\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0001637272\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000405170\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0001963808\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000479293\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0001925861\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000585533\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002395365\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000750615\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0001988226\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000444606\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0001767299\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000574728\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0001673413\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000630362\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0001960102\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000412456\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002090426\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000555752\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0001813533\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000576490\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0001992107\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000468319\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0001942135\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000367199\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0001575027\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000419531\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0001747282\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000365957\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0001504642\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000391882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0001898833\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000377657\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0001810285\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000418452\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0001895822\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000541857\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0001882944\n",
            "Training Results - Epoch: 39  Avg loss: 0.0000468418\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0001641536\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000521352\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0001803376\n",
            "Training Results - Epoch: 41  Avg loss: 0.0000358403\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0001636430\n",
            "Training Results - Epoch: 42  Avg loss: 0.0000319179\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0001684238\n",
            "Training Results - Epoch: 43  Avg loss: 0.0000330790\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0001779106\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000376476\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0001252761\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000318288\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0001679849\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000320398\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0001773715\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000412228\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0001701648\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000335331\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0001491015\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000346394\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0001773538\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000340375\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0001539826\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000396147\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0001868882\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000336786\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0001461464\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000393120\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0001219981\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000426052\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0001724037\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000303090\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0001611810\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000255373\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0001455080\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000394234\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0001945213\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000415221\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0001294196\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000427988\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0001860023\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000301951\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0001863547\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000371654\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0001332219\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000538556\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0001691755\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000366829\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001115194\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000487042\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0001925504\n",
            "Training Results - Epoch: 65  Avg loss: 0.0014533480\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0009112886\n",
            "Training Results - Epoch: 66  Avg loss: 0.0541173970\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0810572062\n",
            "Training Results - Epoch: 67  Avg loss: 0.0273367139\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0284421079\n",
            "Training Results - Epoch: 68  Avg loss: 0.0103507890\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0054187147\n",
            "Training Results - Epoch: 69  Avg loss: 0.0089914022\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0054044466\n",
            "Training Results - Epoch: 70  Avg loss: 0.0025829846\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0015548310\n",
            "Training Results - Epoch: 71  Avg loss: 0.0017860079\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0009727365\n",
            "Training Results - Epoch: 72  Avg loss: 0.0005292802\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0003879014\n",
            "Training Results - Epoch: 73  Avg loss: 0.0003464395\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0004130862\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000710720\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0002266365\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000417920\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0002072115\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000304029\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0001933139\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000261521\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0001773991\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000243735\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0001685811\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000231510\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0001721930\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000228873\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0001698476\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000235837\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0001770073\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000218629\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0001863621\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000202831\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0001805338\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000199339\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0001748084\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000185556\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0001750570\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000181529\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0001762057\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000179525\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0001780778\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000177871\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0001774496\n",
            "Training Results - Epoch: 89  Avg loss: 0.0000176749\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0001774772\n",
            "Training Results - Epoch: 90  Avg loss: 0.0000176496\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0001779364\n",
            "Training Results - Epoch: 91  Avg loss: 0.0000174528\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0001792977\n",
            "Training Results - Epoch: 92  Avg loss: 0.0000173847\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0001795119\n",
            "Training Results - Epoch: 93  Avg loss: 0.0000173237\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 93  Avg loss: 0.0001805711\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0035518117\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0031578869\n",
            "Training Results - Epoch: 2  Avg loss: 0.0023182813\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0029956563\n",
            "Training Results - Epoch: 3  Avg loss: 0.0016031529\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0021574001\n",
            "Training Results - Epoch: 4  Avg loss: 0.0023998419\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0027128955\n",
            "Training Results - Epoch: 5  Avg loss: 0.0023055689\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0023717195\n",
            "Training Results - Epoch: 6  Avg loss: 0.0037862669\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0023810306\n",
            "Training Results - Epoch: 7  Avg loss: 0.0053665938\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0029507662\n",
            "Training Results - Epoch: 8  Avg loss: 0.0062607431\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0040090980\n",
            "Training Results - Epoch: 9  Avg loss: 0.0022783166\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0013334392\n",
            "Training Results - Epoch: 10  Avg loss: 0.0007232999\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0008607912\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001884425\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002935993\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001065850\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002104160\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000679698\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0001628224\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000612696\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0001442636\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000271095\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0001355175\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000467344\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0001441775\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000416206\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0001303252\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000403463\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0001360157\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000300973\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0001132917\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000291084\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0001359431\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000317150\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0001127579\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000294418\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0001214074\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000348544\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0001098672\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000370460\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0001160736\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000505580\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0001351653\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000657871\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0001172126\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000656299\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0001323055\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001349421\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002633022\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001520797\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0001796176\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000869726\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002145617\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001055616\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002009598\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000929070\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0001787414\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000539780\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0001615409\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000309281\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0001035384\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000252580\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0000978636\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000291623\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0001046024\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000310250\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0001066495\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000409134\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0001431575\n",
            "Training Results - Epoch: 39  Avg loss: 0.0000415922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0000981024\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000244066\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0001011308\n",
            "Training Results - Epoch: 41  Avg loss: 0.0000341970\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0001059379\n",
            "Training Results - Epoch: 42  Avg loss: 0.0000377641\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0001152046\n",
            "Training Results - Epoch: 43  Avg loss: 0.0000443224\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0001432726\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000455985\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0001433013\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000283678\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0001021517\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000248371\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0001150584\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000365036\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0001253753\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000380237\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0001603677\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000226581\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0001273216\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000347899\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0001290262\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000322646\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0001319175\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000425720\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0001279625\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000333754\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0001194982\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000389386\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0001200847\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000302151\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0001197719\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000294386\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0001171621\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000202536\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0001029545\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000172896\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0001007884\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000157595\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0000996119\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000148660\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0001011071\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000142694\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0001002581\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000137805\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0001013743\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000134631\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001015262\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000131720\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0001028531\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000129547\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0001027564\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0003815403\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0005993490\n",
            "Training Results - Epoch: 2  Avg loss: 0.0071129211\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0143695224\n",
            "Training Results - Epoch: 3  Avg loss: 0.0008582608\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0014402218\n",
            "Training Results - Epoch: 4  Avg loss: 0.0067207021\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0099506438\n",
            "Training Results - Epoch: 5  Avg loss: 0.0005799834\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0011288792\n",
            "Training Results - Epoch: 6  Avg loss: 0.0002134367\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002442457\n",
            "Training Results - Epoch: 7  Avg loss: 0.0001461956\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002360840\n",
            "Training Results - Epoch: 8  Avg loss: 0.0001286842\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0003822940\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000446049\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0001377947\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000307567\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0001280756\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000240345\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000993218\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000263429\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0001309728\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000230319\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0001058547\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000184142\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0001316062\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000209966\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0001175007\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000284859\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0001477683\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000355349\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0001891397\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000389262\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0001749255\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000470601\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0001374322\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000418506\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0001257775\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000543280\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0001552269\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000338629\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0001376047\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000289223\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0001569214\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000297487\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0001739851\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000353922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0001161312\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000299519\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0001463055\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000280972\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0001577436\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000392794\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0001391783\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000238282\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0001376279\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000277771\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0001738588\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000233455\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0001295968\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000243255\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0001292053\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000198055\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0001271502\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000168303\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0001323463\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000155002\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0001382575\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000146353\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0001409309\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000141519\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0001432488\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000137534\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0001476408\n",
            "Training Results - Epoch: 39  Avg loss: 0.0000134260\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0001528528\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000133405\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0001520877\n",
            "Training Results - Epoch: 41  Avg loss: 0.0000135435\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0001711572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5RM5xCQBeJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "predicted_data = lath[2011].values + decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "1bdd7d7e-b72b-4129-b421-00ff4f4adfd0",
        "id": "PJO-Vs__BhP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14882985725767778"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKVsosH0Ok-U",
        "colab_type": "code",
        "outputId": "1e2b6815-5a8a-45e9-db0b-eb1958daa6e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.20874490086198058"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w8bf-trrJvmj",
        "colab_type": "text"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ30mPWTJxmL",
        "colab_type": "text"
      },
      "source": [
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th rowspan=2>Model</th>\n",
        "            <th rowspan=2> \"No change\" </th>\n",
        "            <th colspan=2>TF-IDF Autoencoder</th>\n",
        "            <th colspan=2>ELMO</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th>LR</th>\n",
        "            <th>NLR1</th>\n",
        "            <th>LR</th>\n",
        "            <th>NLR1</th>\n",
        "            <th>NLR2</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td>Training MAE</td>\n",
        "            <td>11.92%</td>\n",
        "            <td>7.94%</td>\n",
        "            <td>8.22%</td>\n",
        "            <td>8.01%</td>\n",
        "            <td>5.62%</td>\n",
        "            <td>5.62%</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>Testing MAE</td>\n",
        "            <td>12.11%</td>\n",
        "            <td>10.07%</td>\n",
        "            <td>9.33%</td>\n",
        "            <td>8.01%</td>\n",
        "            <td>12.55%</td>\n",
        "            <td>9.25%</td>\n",
        "        </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    }
  ]
}