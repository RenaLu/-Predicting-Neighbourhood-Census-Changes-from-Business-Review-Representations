{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CensusPrediction_Income.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "kkbhD73ATaS5"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5QEoXb0vhch",
        "colab_type": "text"
      },
      "source": [
        "# Imports and Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tmm2_PLSXXms",
        "colab_type": "code",
        "outputId": "abcf88f0-e2b9-480d-f7c0-30cc96dce2c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "!pip install pytorch-ignite"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-ignite\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/35/55/41e8a995876fd2ade29bdba0c3efefa38e7d605cb353c70f3173c04928b5/pytorch_ignite-0.3.0-py2.py3-none-any.whl (103kB)\n",
            "\r\u001b[K     |███▏                            | 10kB 25.1MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 20kB 1.6MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 30kB 2.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 61kB 2.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 71kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 81kB 2.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 92kB 2.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 112kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.4.0)\n",
            "Installing collected packages: pytorch-ignite\n",
            "Successfully installed pytorch-ignite-0.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8-UhiP9qfJOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "from ast import literal_eval\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from scipy import sparse\n",
        "\n",
        "from sklearn.linear_model import RidgeCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7xsyi3AX56i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ignite.handlers import ModelCheckpoint, EarlyStopping, TerminateOnNan\n",
        "from ignite.engine import Engine, Events\n",
        "from ignite.metrics import Accuracy, Loss, RunningAverage\n",
        "from ignite.handlers import ModelCheckpoint, EarlyStopping\n",
        "from ignite.contrib.handlers import ProgressBar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwPy5Lh3CH-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import string\n",
        "import random\n",
        "import torch.utils.data as Data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "itB2xTKXZm3o",
        "colab_type": "code",
        "outputId": "04b1ad24-a249-4ede-96d1-343d2f369681",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import OrderedDict\n",
        "import locale\n",
        "from locale import atof, atoi\n",
        "locale.setlocale(locale.LC_NUMERIC, '')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'en_US.UTF-8'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBzBYsqc1OAw",
        "colab_type": "code",
        "outputId": "fc2601fb-148b-43d2-a328-e913ce7bfdc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmD0YDfwfafe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_idf = True\n",
        "max_features = 2**10\n",
        "pca_components = 2**4\n",
        "DRIVE_PATH = Path('/content/drive/My Drive/Thesis2019/')\n",
        "NEIGHBOURHOOD_PROFILES_PATH = Path('neighbourhood-profiles-2016-csv.csv')\n",
        "NEIGHBOURHOOD_PROFILES_PATH_OLD = Path('neighbourhood-data-2001-2011.xlsx')\n",
        "DATA_DIR = DRIVE_PATH.joinpath('data_stripped')\n",
        "use_cuda = True\n",
        "years = [2011, 2016]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6KxXDNKRxRpT",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Wrapper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kAxwuIYYbw3",
        "colab_type": "text"
      },
      "source": [
        "Pytorch Dataset wrappers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDgKDOkoxTQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReviewsVector(Data.Dataset):\n",
        "    \"\"\"Reviews Vector dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "          data\n",
        "\n",
        "        \"\"\"\n",
        "        self.shape = data.shape\n",
        "        self.data = torch.tensor(data).type(torch.FloatTensor)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "          self.data = self.data.cuda()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "class CensusVector(Data.Dataset):\n",
        "  def __init__(self, data, reviews_embedding):\n",
        "        \"\"\"\n",
        "        Args: \n",
        "          data\n",
        "\n",
        "        \"\"\"\n",
        "        self.shape = data.shape\n",
        "        self.data = torch.tensor(data).type(torch.FloatTensor)\n",
        "        self.reviews_embedding = torch.tensor(reviews_embedding).type(torch.FloatTensor)\n",
        "        if use_cuda and torch.cuda.is_available():\n",
        "            self.data = self.data.cuda()\n",
        "            self.reviews_embedding = self.reviews_embedding.cuda()\n",
        "            \n",
        "  def __len__(self):\n",
        "      return len(self.data)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "      return {\"data\": self.data[idx],\n",
        "              \"reviews_embedding\": self.reviews_embedding[idx]}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0yvBkQovmDE",
        "colab_type": "text"
      },
      "source": [
        "# Data Cleaning and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdiWF8NSwfqx",
        "colab_type": "text"
      },
      "source": [
        "## Selecting and Matching Attributes for 2011 and 2016 data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rd5y8dtFYfIe",
        "colab_type": "text"
      },
      "source": [
        "Match census categories from different years."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV574nlTv42q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nbh_profiles = pd.read_csv(DRIVE_PATH.joinpath(NEIGHBOURHOOD_PROFILES_PATH))\n",
        "nbh_profiles_2011 = pd.read_excel(DRIVE_PATH.joinpath(NEIGHBOURHOOD_PROFILES_PATH_OLD), sheet_name='2011')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hadRLgh2wKM7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2011 = nbh_profiles_2011[(nbh_profiles_2011['Topic']=='Income of households')].drop(columns=['Category', 'Topic', 'City of Toronto'])\n",
        "income_2016 = nbh_profiles[(nbh_profiles['Topic']=='Income of households in 2015')].drop(columns=['_id', 'Category', 'Topic', 'Data Source', 'City of Toronto'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z-UmUuLhF5mt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_2011 = income_2011[income_2011['Attribute'] == 'Household total income in 2010 of private households'].index\n",
        "idx_2011_2 = income_2011[income_2011['Attribute'] == 'After-tax income of households in 2010 of private households'].index\n",
        "income_2011 = income_2011.loc[idx_2011[0]+1:idx_2011_2[0]-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqkekXKyGH_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "idx_2016 = income_2016[income_2016['Characteristic'] == 'Total - Household total income groups in 2015 for private households - 100% data'].index\n",
        "idx_2016_2 = income_2016[income_2016['Characteristic'] == 'Total - Household after-tax income groups in 2015 for private households - 100% data'].index\n",
        "idx_2016_3 = income_2016[income_2016['Characteristic'] == '  $15,000 to $19,999'].index"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Krc_wiGbGNZ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2016_1 = income_2016.loc[idx_2016[0]+1:idx_2016_2[0]-1]\n",
        "income_2016_1.loc[1] = income_2016.loc[idx_2016_3[0]]\n",
        "\n",
        "income_2016_1 = income_2016.loc[idx_2016[0]+1:idx_2016_2[0]-1]\n",
        "income_2016_1.loc[1] = income_2016.loc[idx_2016_3[0]]\n",
        "\n",
        "income_2016 = income_2016_1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF_Acd4qD24j",
        "colab_type": "code",
        "outputId": "f157a21f-728d-46d4-fad1-b8ee95764098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "list(income_2011['Attribue'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  Under $10,000',\n",
              " '  $10,000 to $19,999',\n",
              " '  $20,000 to $29,999',\n",
              " '  $30,000 to $39,999',\n",
              " '  $40,000 to $49,999',\n",
              " '  $50,000 to $59,999',\n",
              " '  $60,000 to $79,999',\n",
              " '  $80,000 to $99,999',\n",
              " '  $100,000 and over']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DI9h2O2_LZ4U",
        "colab_type": "code",
        "outputId": "d48b2ac1-fbb8-4b2b-a097-da4f683881aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "list(income_2016['Characteristic'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['  Under $5,000',\n",
              " '  $5,000 to $9,999',\n",
              " '  $10,000 to $14,999',\n",
              " '  $20,000 to $24,999',\n",
              " '  $25,000 to $29,999',\n",
              " '  $30,000 to $34,999',\n",
              " '  $35,000 to $39,999',\n",
              " '  $40,000 to $44,999',\n",
              " '  $45,000 to $49,999',\n",
              " '  $50,000 to $59,999',\n",
              " '  $60,000 to $69,999',\n",
              " '  $70,000 to $79,999',\n",
              " '  $80,000 to $89,999',\n",
              " '  $90,000 to $99,999',\n",
              " '  $100,000 and over',\n",
              " '    $200,000 and over',\n",
              " '  $15,000 to $19,999']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXGhkeOHF_bu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2011.set_index('Attribute', inplace=True)\n",
        "\n",
        "income_2011.loc['  Under $10,000'] = income_2011.loc['  Under $5,000'] + income_2011.loc['  $5,000 to $9,999']\n",
        "income_2011.loc['  $10,000 to $19,999'] = income_2011.loc['  $10,000 to $14,999'] + income_2011.loc['  $15,000 to $19,999']\n",
        "income_2011.loc['  $100,000 and over'] = income_2011.loc['  $100,000 to $124,999'] + income_2011.loc['  $125,000 to $149,999'] + income_2011.loc['  $150,000 and over']\n",
        "\n",
        "income_2011 = income_2011.reindex(['  Under $10,000', '  $10,000 to $19,999', '  $20,000 to $29,999', '  $30,000 to $39,999', \n",
        "                     '  $40,000 to $49,999', '  $50,000 to $59,999', '  $60,000 to $79,999', '  $80,000 to $99,999', \n",
        "                     '  $100,000 and over'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te0_22JDGJ0I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2016.set_index('Characteristic', inplace=True)\n",
        "income_2016 = income_2016.applymap(atoi)\n",
        "\n",
        "income_2016.loc['  Under $10,000'] = income_2016.loc['  Under $5,000'] + income_2016.loc['  $5,000 to $9,999']\n",
        "income_2016.loc['  $10,000 to $19,999'] = income_2016.loc['  $10,000 to $14,999'] + income_2016.loc['  $15,000 to $19,999']\n",
        "income_2016.loc['  $20,000 to $29,999'] = income_2016.loc['  $20,000 to $24,999'] + income_2016.loc['  $25,000 to $29,999']\n",
        "income_2016.loc['  $30,000 to $39,999'] = income_2016.loc['  $30,000 to $34,999'] + income_2016.loc['  $35,000 to $39,999']\n",
        "income_2016.loc['  $40,000 to $49,999'] = income_2016.loc['  $40,000 to $44,999'] + income_2016.loc['  $45,000 to $49,999']\n",
        "income_2016.loc['  $60,000 to $79,999'] = income_2016.loc['  $60,000 to $69,999'] + income_2016.loc['  $70,000 to $79,999']\n",
        "income_2016.loc['  $80,000 to $99,999'] = income_2016.loc['  $80,000 to $89,999'] + income_2016.loc['  $90,000 to $99,999']\n",
        "\n",
        "income_2016 = income_2016.reindex(['  Under $10,000', '  $10,000 to $19,999', '  $20,000 to $29,999', '  $30,000 to $39,999', \n",
        "                     '  $40,000 to $49,999', '  $50,000 to $59,999', '  $60,000 to $79,999', '  $80,000 to $99,999', \n",
        "                     '  $100,000 and over'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxVKNobHGVmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "income_2011 = income_2011.T.sort_index()\n",
        "income_2016 = income_2016.T.sort_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZYuOztQw5Gl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned = {'2011': income_2011, '2016': income_2016}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmSvGTznxLVP",
        "colab_type": "text"
      },
      "source": [
        "## Build Census datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eco7ClI4GZFv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incomes = {}\n",
        "for year in cleaned:\n",
        "  income = cleaned[year]\n",
        "  incomes[year] = income.div(income.sum(axis=1), axis=0)\n",
        "  incomes[year].to_csv(DRIVE_PATH.joinpath('income_'+str(year)+'.csv'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cb_q_9zDIxom",
        "colab_type": "text"
      },
      "source": [
        "## Load Built Census datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jAwtOk-zIxCn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "incomes = {}\n",
        "\n",
        "for year in years:\n",
        "  incomes[year] = pd.read_csv(DRIVE_PATH.joinpath('income_{}.csv'.format(year)))\n",
        "  incomes[year].set_index('Unnamed: 0', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtT3yMRfxfY5",
        "colab_type": "text"
      },
      "source": [
        "# Predicting Census Change"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLjbonnLYieg",
        "colab_type": "text"
      },
      "source": [
        "Census Change prediction is evaluated with Mean Total Absolute Error:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbBZZj91qKMy",
        "colab_type": "text"
      },
      "source": [
        "$Mean Total Absolute Error = $\n",
        "\n",
        "$\\frac{1}{\\#Neighbourhoods}\\sum_{Neighbourhoods} \\sum_{categories} |Actual Proportion - Predicted Proportion|$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34dueeBjxz6o",
        "colab_type": "text"
      },
      "source": [
        "### Train-test split\n",
        "\n",
        "Split by neighbourhoods"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTuAGFwx4EU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_val_neighbourhoods, test_neighbourhoods = train_test_split(range(0, 140), test_size=0.15)\n",
        "\n",
        "all_trains, all_vals = [], [] #Do random splits to cross validate\n",
        "folds = 5\n",
        "\n",
        "for i in range(folds):\n",
        "  train_neighbourhoods, val_neighbourhoods = train_test_split(train_val_neighbourhoods, test_size=0.30)\n",
        "  all_trains.append(train_neighbourhoods)\n",
        "  all_vals.append(val_neighbourhoods)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dY9kM4UO-8pA",
        "colab_type": "text"
      },
      "source": [
        "### Baseline-Predicting no change"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VLrDG6hs-_Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dummy_data = incomes[2011].values\n",
        "actual_data = incomes[2016].values"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YKgu-xE_q-S",
        "colab_type": "code",
        "outputId": "cb212838-e39e-48d1-fa6b-efbd6ba59880",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs(dummy_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14757192238340344"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9wpLJrKG_veR",
        "colab_type": "code",
        "outputId": "a397303c-3373-4519-d36e-32af00e87866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs(dummy_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.14101081314013955"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7rgQVQISPsc",
        "colab_type": "text"
      },
      "source": [
        "## Raw TF-IDF"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJJd6jUHSwFC",
        "colab_type": "text"
      },
      "source": [
        "### Load built csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2vSX9n3Svnx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_all = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews_all[year] = pd.read_csv(DRIVE_PATH.joinpath('neighbourhood_reviews_{}.csv'.format(year)))\n",
        "  reviews_all[year].set_index('Unnamed: 0', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHklKQjXUF6T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews[year] = ReviewsVector(reviews_all[year].T.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_z8gEkBXSSUc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "delta_census = incomes[2016].values - incomes[2011].values\n",
        "delta_reviews = (reviews_all[2016].values - reviews_all[2011].values).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-BZhUCScUrR",
        "colab_type": "code",
        "outputId": "b11f16d6-d4aa-4e4f-df22-316b34302ebb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        }
      },
      "source": [
        "delta_census"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.01653441, -0.00457142,  0.00906722, ...,  0.01221596,\n",
              "        -0.0123801 ,  0.03325817],\n",
              "       [ 0.01299708, -0.00096479,  0.01146939, ..., -0.00044584,\n",
              "         0.0040148 ,  0.01733887],\n",
              "       [-0.00935728, -0.00380887, -0.03343908, ..., -0.00790805,\n",
              "        -0.00924347,  0.08670751],\n",
              "       ...,\n",
              "       [-0.01326804,  0.00185007,  0.0023348 , ..., -0.00180959,\n",
              "         0.00833662,  0.04166838],\n",
              "       [-0.04663484, -0.02984758, -0.00828498, ..., -0.00548261,\n",
              "         0.03018392,  0.05330347],\n",
              "       [-0.00920636, -0.031809  , -0.02567321, ..., -0.00369094,\n",
              "         0.01995962,  0.06479981]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9945yCARSghw",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwRxg-p1SYBA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = RidgeCV(cv=5)\n",
        "lr.fit(delta_reviews[train_val_neighbourhoods], delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KzoFFgPSSezq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_change = lr.predict(delta_reviews)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emFk-RbVTSLh",
        "colab_type": "code",
        "outputId": "11b7546c-f5f4-4ff9-b74a-774b276f28ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change[train_val_neighbourhoods]-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10231567325907694"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X68hopx1TU7c",
        "colab_type": "code",
        "outputId": "e16c5674-487a-42a9-edff-97d9867e5da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((predicted_change[test_neighbourhoods]-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10529921362758532"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kkbhD73ATaS5",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target  Non-Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SSq-AIdQTfVK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder_C(nn.Module):\n",
        "    def __init__(self, sizes, softmax=False):\n",
        "        super(Decoder_C, self).__init__()\n",
        "        sizes = sizes[::-1]\n",
        "        layers_de = OrderedDict()\n",
        "\n",
        "        for i in range(len(sizes)-2):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_de[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            layers_de[act_name] = nn.Tanh()\n",
        "\n",
        "        layers_de['linear{}'.format(len(sizes)-1)] = nn.Linear(sizes[-2], sizes[-1])\n",
        "        # layers_de['softmax'] = nn.Softmax(dim=1) # row sum to 1\n",
        "        layers_de['tanh'] = nn.Tanh()\n",
        "        if softmax:\n",
        "          layers_de['softmax'] = nn.Softmax(dim=1)\n",
        "        self.decoder = nn.Sequential(layers_de)\n",
        "\n",
        "    def forward(self, encoded):\n",
        "        return self.decoder(encoded) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-YOFZyePTvJ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def criterion_c(data, decoded):\n",
        "    mse_loss = nn.MSELoss()\n",
        "    loss = mse_loss(data, decoded)\n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEp0bW7CTyEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_decoder(decoder, dataset, train_indices, test_indices, name='inc_auto'):\n",
        "  optimizer_de = optim.Adam(decoder.parameters(), lr=0.001)\n",
        "  scheduler_de = optim.lr_scheduler.ReduceLROnPlateau(optimizer_de, 'min', patience=20, min_lr=min_lr, factor=0.1)\n",
        "\n",
        "  def process_function(engine, batch):\n",
        "    decoder.train()\n",
        "    optimizer_de.zero_grad()\n",
        "    decoded = decoder(batch['reviews_embedding'])\n",
        "    loss = criterion_c(decoded, batch['data'])\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer_de.step()\n",
        "    return loss.item()\n",
        "  \n",
        "\n",
        "  def eval_function(engine, batch):\n",
        "    decoder.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        decoded = decoder(batch['reviews_embedding'])\n",
        "        return decoded, batch['data']\n",
        "  \n",
        "  trainer = Engine(process_function)\n",
        "  train_evaluator = Engine(eval_function)\n",
        "  validation_evaluator = Engine(eval_function)\n",
        "\n",
        "  metric = Loss(criterion_c)\n",
        "  metric.attach(train_evaluator, 'loss')\n",
        "  metric.attach(validation_evaluator, 'loss')\n",
        "\n",
        "  training_losses = []\n",
        "  validation_losses = []\n",
        "\n",
        "  @trainer.on(Events.EPOCH_COMPLETED)\n",
        "  def log_training_results(engine):\n",
        "      train_evaluator.run(train_iterator)\n",
        "      metrics = train_evaluator.state.metrics\n",
        "      avg_loss = metrics['loss']    \n",
        "      training_losses.append(avg_loss)\n",
        "      print(\"Training Results - Epoch: {}  Avg loss: {:.10f}\"\n",
        "           .format(engine.state.epoch, avg_loss))\n",
        "      \n",
        "  def log_validation_results(engine):\n",
        "      validation_evaluator.run(valid_iterator)\n",
        "      metrics = validation_evaluator.state.metrics\n",
        "      avg_loss = metrics['loss']\n",
        "      validation_losses.append(avg_loss)\n",
        "\n",
        "      print(\"Validation Results - Epoch: {}  Avg loss: {:.10f}\"\n",
        "            .format(engine.state.epoch, avg_loss))\n",
        "\n",
        "  trainer.add_event_handler(Events.EPOCH_COMPLETED, log_validation_results)\n",
        "\n",
        "  # Reduce on Plateau\n",
        "  def average_loss(engine):\n",
        "    print(\"Current lr: {}\".format(optimizer_de.param_groups[0]['lr']))\n",
        "    average_loss = engine.state.metrics['loss']\n",
        "    scheduler_de.step(average_loss)\n",
        "\n",
        "  validation_evaluator.add_event_handler(Events.COMPLETED, average_loss)\n",
        "  \n",
        "  # Early Stopping\n",
        "  def score_function(engine):\n",
        "      val_loss = engine.state.metrics['loss']\n",
        "      return -val_loss\n",
        "\n",
        "  handler = EarlyStopping(patience=30, score_function=score_function, trainer=trainer)\n",
        "  validation_evaluator.add_event_handler(Events.COMPLETED, handler)\n",
        "\n",
        "  # Model Checkpoint\n",
        "  checkpointer = ModelCheckpoint(str(DRIVE_PATH.joinpath('models')), name, n_saved=1, create_dir=False, save_as_state_dict=True, require_empty=False)\n",
        "  trainer.add_event_handler(Events.EPOCH_COMPLETED, checkpointer, {'decoder_c': decoder})\n",
        "\n",
        "  train_iterator = Data.DataLoader(dataset, batch_size=1, drop_last=False, sampler=Data.SubsetRandomSampler(train_indices))\n",
        "  valid_iterator = Data.DataLoader(dataset, batch_size=1, drop_last=False, sampler=Data.SubsetRandomSampler(test_indices))\n",
        "\n",
        "  trainer.run(train_iterator, max_epochs=1000)\n",
        "  return training_losses, validation_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSxYLT6SURpv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pvNScaFT514",
        "colab_type": "text"
      },
      "source": [
        "#### With one layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yv0Bx5ryT2nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "census_data = CensusVector(delta_census, delta_reviews)\n",
        "\n",
        "sizes_c = [incomes[2011].shape[1], delta_reviews.shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceWeX7xvUTwm",
        "colab_type": "code",
        "outputId": "6ac67eea-1974-4155-eee9-5fad2c1a70e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=9, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9js1V34UXd9",
        "colab_type": "code",
        "outputId": "f2ceaf44-bdfc-4e3f-baf9-65456b524ec8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='inc_raw')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0001873186\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002576750\n",
            "Training Results - Epoch: 2  Avg loss: 0.0001618280\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002622719\n",
            "Training Results - Epoch: 3  Avg loss: 0.0001449597\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002740233\n",
            "Training Results - Epoch: 4  Avg loss: 0.0001257183\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002536580\n",
            "Training Results - Epoch: 5  Avg loss: 0.0001147006\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002607399\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001009805\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002572081\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000911819\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002667394\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000848954\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002677511\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000775310\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0002608747\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000734191\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002754607\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000675933\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002805464\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000617596\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002664478\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000599357\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0002693137\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000539251\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0002747491\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000532094\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002902844\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000473797\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0002711871\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000431681\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0002762941\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000409915\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002734530\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000396515\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002753061\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000354107\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0002814429\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000329436\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002806030\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000314861\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0002873483\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000298547\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002846230\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000280954\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002852795\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000255127\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002917276\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000246091\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002876860\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000243547\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002868869\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000241524\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002866245\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000239581\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002870019\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000237947\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002883327\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000235819\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002879962\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000233862\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0002875892\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000232084\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0002876303\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000230279\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0002874851\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000902260\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000893846\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000726888\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000882663\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000627924\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000927420\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000544912\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000910128\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000483927\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000931390\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000456371\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0001004266\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000412490\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000975648\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000387107\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0001032921\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000337574\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0001012702\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000314987\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0001028904\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000300034\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0001039858\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000290749\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0001106600\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000279653\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0001040873\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000284403\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0001086529\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000238872\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0001140520\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000231882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0001107065\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000214462\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0001126993\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000210889\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0001164260\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000189685\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0001157279\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000172605\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0001157755\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000171725\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0001171864\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000156458\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0001186990\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000142457\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0001203893\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000135566\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0001209952\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000132624\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0001212826\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000129821\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0001212441\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000127876\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0001215112\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000126307\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0001214194\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000125143\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0001217624\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000123748\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0001220270\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000122355\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0001222387\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000121308\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0001219682\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000272943\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000817032\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000225300\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000862918\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000185443\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000838067\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000169999\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000885611\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000155882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000887651\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000154078\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000918101\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000138489\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000908200\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000141613\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000935564\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000120952\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000914731\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000133001\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000972982\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000130252\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000984585\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000097091\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000961277\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000091880\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000951987\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000095026\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0001001796\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000073922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000977598\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000078481\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0001009481\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000076145\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0001009191\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000068647\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0001014903\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000068294\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0001017193\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000065819\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0001054155\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000063188\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0001020081\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000059571\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0001051943\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000048838\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0001038390\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000045004\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0001042028\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000042446\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0001040313\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000040802\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0001038706\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000039571\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0001045209\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000038397\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0001047984\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000037631\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0001045836\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000036762\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0001047467\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000036344\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0001046762\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000323325\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000164479\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000242625\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000186204\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000186698\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000185773\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000166229\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000228487\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000115724\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000198045\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000114830\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000237619\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000095486\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000212698\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000097002\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000229672\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000089675\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000221312\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000077129\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000236051\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000076644\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000244275\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000070745\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000240899\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000065720\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000255195\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000074445\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000290203\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000073229\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000292885\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000066164\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000278906\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000074374\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000309813\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000071529\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000282154\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000052080\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000299526\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000051089\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000293534\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000047596\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000309934\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000045234\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000322958\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000036911\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000315970\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000033855\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000315585\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000032561\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000317803\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000030916\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000318185\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000029854\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000314451\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000029205\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000318883\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000028554\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000317614\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000028124\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000317116\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000027775\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000316835\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000088973\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000131622\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000071106\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000146819\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000072222\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000174985\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000072356\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000163677\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000064674\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000150103\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000059140\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000165294\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000069964\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000196277\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000057936\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000174389\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000051467\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000178006\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000051821\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000181135\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000056912\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000200575\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000041174\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000191712\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000045466\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000207022\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000041198\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000208112\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000041250\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000213688\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000040710\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000220910\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000038352\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000224024\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000039278\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000223664\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000030042\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000233000\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000033016\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000241778\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000029363\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000238815\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000034304\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000245392\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000023473\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000241004\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000020081\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000243916\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000017972\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000246209\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000016652\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000246523\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000015561\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000246029\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000014990\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000246238\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000014758\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000245203\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000014164\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000248863\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000014010\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000250970\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28Nf-Dz1VbMw",
        "colab_type": "code",
        "outputId": "aa7732d6-8a2c-4c0d-f37b-e04813c2537b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(range(len(validation_losses)), validation_losses, range(len(training_losses)), training_losses)\n",
        "plt.legend(['Validation Losses', 'Training Losses'])\n",
        "plt.title('Loss curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__TazGaLVoNH",
        "colab_type": "code",
        "outputId": "ba7f76fd-4911-49f7-f955-c43bbdf27f9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test Loss\n",
        "criterion_c(census_data.data[test_neighbourhoods], decoder_c(census_data.reviews_embedding[test_neighbourhoods]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tndj2D0hVvg6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(census_data.reviews_embedding).cpu().detach().numpy()\n",
        "actual_data = incomes[2016].values\n",
        "predicted_data = incomes[2011].values+ decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BV6qN9zQWBAk",
        "colab_type": "code",
        "outputId": "ad29d03d-a556-4d5f-a512-415baa1b643d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.011501145466415195"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQRnaAFMWE-4",
        "colab_type": "code",
        "outputId": "49afe711-0e6c-4e44-89c6-24fb45c1ae43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((predicted_data[test_neighbourhoods]-actual_data[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11859679838385852"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NE0Nz2mQWhHs",
        "colab_type": "text"
      },
      "source": [
        "#### With Additional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7GY37oDxWjpM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [incomes[2011].shape[1], delta_reviews.shape[1]//2, delta_reviews.shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ee2cnHFRXIL6",
        "colab_type": "code",
        "outputId": "a745f812-d6c3-4356-e81a-0c7edfef139b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=512, out_features=9, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RVXdg94uXKDN",
        "colab_type": "code",
        "outputId": "a32b636f-16d4-4c07-d5e1-1f5b03b781fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='inc_raw_2')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0001807683\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002924373\n",
            "Training Results - Epoch: 2  Avg loss: 0.0001470540\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002812501\n",
            "Training Results - Epoch: 3  Avg loss: 0.0001251920\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0003252818\n",
            "Training Results - Epoch: 4  Avg loss: 0.0001105063\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002982442\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000815447\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002716199\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001014858\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002684427\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000907430\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002917017\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000661014\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002724714\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000723769\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0002902443\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000684850\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0003205835\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000716354\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0003280852\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000503449\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002853706\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000518505\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0003157132\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000499043\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0003122825\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000470151\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002998814\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000357375\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0002837245\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000327836\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0002888957\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000343231\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002963993\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000539727\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0003062347\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000323160\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0003010816\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000293660\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002848934\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000385290\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0002964235\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000323338\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003119445\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000274749\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003025946\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000503676\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0003104042\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000322368\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002945114\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000391534\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002830701\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000236997\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002906644\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000186941\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002908837\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000152465\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002944538\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000130357\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002965349\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000108062\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0002943897\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000092667\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0002966873\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000082762\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0002968227\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000076062\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003003487\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000067635\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0002997552\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000920051\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0001132804\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000771292\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0001072895\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000634050\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0001079461\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000645897\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0001079226\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000601722\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0001227012\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000531573\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0001165884\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000574092\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0001382415\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000422389\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0001121616\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000439293\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0001143430\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000479750\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0001366215\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000433631\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0001255575\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000434940\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0001223461\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000437035\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0001165531\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000434145\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0001288313\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000328635\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0001254756\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000380317\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0001370718\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000327427\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0001343265\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000317537\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0001272561\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000563841\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0001400606\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000359057\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0001405111\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000468163\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0001399339\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000419556\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0001454357\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000527435\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0001692733\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000206083\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0001372262\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000155590\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0001359348\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000121492\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0001362675\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000098812\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0001362826\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000087522\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0001354992\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000074557\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0001385659\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000065820\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0001378411\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000060037\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0001391222\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000056998\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0001382834\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000565242\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0001057619\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000440732\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0001045041\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000622943\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0001208578\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000442163\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0001087721\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000376252\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0001091461\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000373526\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000958874\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000391583\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0001147180\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000419661\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0001148630\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000337811\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0001172475\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000299607\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0001118028\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000476308\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0001283162\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000312147\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0001098790\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000220820\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0001047978\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000218201\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0001070282\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000272660\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0001211673\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000287184\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0001220921\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000217223\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0001052160\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000189252\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0001156458\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000240951\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0001229657\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000424581\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0001329146\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000313370\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0001152399\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000245408\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0001220941\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000255835\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0001149755\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000261089\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0001202806\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000314644\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0001233867\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000241227\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0001137587\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000308850\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0001383442\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000120881\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0001161707\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000084724\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0001128707\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000062465\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0001137325\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000049939\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0001133557\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000043139\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0001134188\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000037301\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0001134001\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000032031\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0001133322\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000029014\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0001139273\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000026274\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0001135769\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000357373\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000285561\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000372313\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000360085\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000264984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000286209\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000366235\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000455492\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000272309\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000352888\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000294737\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000400884\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000359989\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000415483\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000333431\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000419878\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000389170\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000340254\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000410737\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000416184\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000398980\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000403360\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000409233\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000468791\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000289738\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000360108\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000181727\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000355509\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000196528\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000343434\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000157439\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000349989\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000161853\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000405541\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000108882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000301774\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000229201\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000414414\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000154729\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000315757\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000211218\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000433189\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000189588\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000350676\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000102179\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000351949\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000058768\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000331512\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000049305\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000343239\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000035561\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000335372\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000027621\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000328415\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000023617\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000333335\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000019749\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000330122\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000016776\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000331690\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000015718\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000331454\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0000279763\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0000268856\n",
            "Training Results - Epoch: 2  Avg loss: 0.0000471825\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0000284796\n",
            "Training Results - Epoch: 3  Avg loss: 0.0000238410\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0000298661\n",
            "Training Results - Epoch: 4  Avg loss: 0.0000320604\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0000313289\n",
            "Training Results - Epoch: 5  Avg loss: 0.0000264614\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0000258324\n",
            "Training Results - Epoch: 6  Avg loss: 0.0000401730\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0000327309\n",
            "Training Results - Epoch: 7  Avg loss: 0.0000323086\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0000457472\n",
            "Training Results - Epoch: 8  Avg loss: 0.0000231603\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0000229666\n",
            "Training Results - Epoch: 9  Avg loss: 0.0000257713\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0000329875\n",
            "Training Results - Epoch: 10  Avg loss: 0.0000394153\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0000383353\n",
            "Training Results - Epoch: 11  Avg loss: 0.0000340531\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0000378048\n",
            "Training Results - Epoch: 12  Avg loss: 0.0000206898\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0000268201\n",
            "Training Results - Epoch: 13  Avg loss: 0.0000239877\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0000298060\n",
            "Training Results - Epoch: 14  Avg loss: 0.0000367529\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0000441873\n",
            "Training Results - Epoch: 15  Avg loss: 0.0000324022\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0000300098\n",
            "Training Results - Epoch: 16  Avg loss: 0.0000259376\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0000321773\n",
            "Training Results - Epoch: 17  Avg loss: 0.0000254276\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0000356517\n",
            "Training Results - Epoch: 18  Avg loss: 0.0000204900\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0000379951\n",
            "Training Results - Epoch: 19  Avg loss: 0.0000119102\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0000354666\n",
            "Training Results - Epoch: 20  Avg loss: 0.0000155525\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0000325096\n",
            "Training Results - Epoch: 21  Avg loss: 0.0000084552\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0000300887\n",
            "Training Results - Epoch: 22  Avg loss: 0.0000069614\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0000286300\n",
            "Training Results - Epoch: 23  Avg loss: 0.0000094556\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0000329570\n",
            "Training Results - Epoch: 24  Avg loss: 0.0000056235\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0000305085\n",
            "Training Results - Epoch: 25  Avg loss: 0.0000061889\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0000286938\n",
            "Training Results - Epoch: 26  Avg loss: 0.0000048633\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0000301049\n",
            "Training Results - Epoch: 27  Avg loss: 0.0000052221\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0000294702\n",
            "Training Results - Epoch: 28  Avg loss: 0.0000068676\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0000337852\n",
            "Training Results - Epoch: 29  Avg loss: 0.0000093738\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0000322936\n",
            "Training Results - Epoch: 30  Avg loss: 0.0000047887\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0000310422\n",
            "Training Results - Epoch: 31  Avg loss: 0.0000035886\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0000316486\n",
            "Training Results - Epoch: 32  Avg loss: 0.0000026687\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0000312457\n",
            "Training Results - Epoch: 33  Avg loss: 0.0000021442\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0000308643\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000018143\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0000310525\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000015195\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0000306410\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000013462\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0000306911\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000011927\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0000309106\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000011195\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0000309408\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xu5kttTMXhJ7",
        "colab_type": "code",
        "outputId": "290a74bf-40d6-4a7a-c62e-e295e7d11ecc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        }
      },
      "source": [
        "plt.plot(range(len(validation_losses)), validation_losses, range(len(training_losses)), training_losses)\n",
        "plt.legend(['Validation Losses', 'Training Losses'])\n",
        "plt.title('Loss curves')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Loss')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEWCAYAAACwtjr+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nfscoG4PBYDD4nf8HA7R3suEnm9gAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNBvQWM8Xlmv",
        "colab_type": "code",
        "outputId": "05a7ac44-52e8-4124-aebf-d3317b63279a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test Loss\n",
        "criterion_c(census_data.data[test_neighbourhoods], decoder_c(census_data.reviews_embedding[test_neighbourhoods]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0003, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqyiM_kxXnNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(census_data.reviews_embedding).cpu().detach().numpy()\n",
        "actual_data = incomes[2016].values\n",
        "predicted_data = incomes[2011].values+ decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EUx6T9e3Xqvu",
        "colab_type": "code",
        "outputId": "a3f60ec1-82a6-482f-b5f4-38c4ee2a3242",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.014095873276626448"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R27qAN4XvBj",
        "colab_type": "code",
        "outputId": "0bf28fa8-2789-4a32-a15a-21b3d6cf4d19",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((predicted_data[test_neighbourhoods]-actual_data[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11754306305605107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5675IoiJxhit",
        "colab_type": "text"
      },
      "source": [
        "## TF-IDF Autoencoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlYfoxBu3S_u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(Encoder, self).__init__()\n",
        "                \n",
        "        layers_en = OrderedDict()       \n",
        "        for i in range(len(sizes)-1):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_en[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            if i==0:\n",
        "                nn.init.xavier_uniform_(layers_en[layer_name].weight)\n",
        "            layers_en[act_name] = nn.Tanh() #-1 to 1\n",
        "        \n",
        "        self.encoder = nn.Sequential(layers_en)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.encoder(x) \n",
        "    \n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, sizes):\n",
        "        super(Decoder, self).__init__()\n",
        "        \n",
        "        sizes = sizes[::-1]\n",
        "        \n",
        "        layers_de = OrderedDict()\n",
        "        for i in range(len(sizes)-2):\n",
        "            layer_name = 'linear{}'.format(i+1)\n",
        "            act_name = 'activation{}'.format(i+1)\n",
        "            layers_de[layer_name] = nn.Linear(sizes[i], sizes[i+1])\n",
        "            layers_de[act_name] = nn.Tanh()\n",
        "\n",
        "        layers_de['linear{}'.format(len(sizes)-1)] = nn.Linear(sizes[-2], sizes[-1])\n",
        "        layers_de['sigmoid'] = nn.Sigmoid() #0 to 1\n",
        "        self.decoder = nn.Sequential(layers_de)\n",
        "\n",
        "    def forward(self, encoded):\n",
        "        return self.decoder(encoded) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQQbaL7TxjKb",
        "colab_type": "text"
      },
      "source": [
        "### Load trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xdhvmqz3xrNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X44OFzttxs10",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_r = [max_features, 2**10, 2**8, pca_components]\n",
        "encoder = Encoder(sizes_r)\n",
        "decoder = Decoder(sizes_r)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    encoder.cuda()\n",
        "    decoder.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGf84g_Oh34S",
        "colab_type": "code",
        "outputId": "4a32877a-ca25-4975-bc96-fc2d34663c89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        }
      },
      "source": [
        "encoder.load_state_dict(torch.load('/content/drive/My Drive/Thesis2019/models/review_encoder2_500.pth'))\n",
        "encoder.eval()\n",
        "encoder.to(torch.device(\"cuda\"))\n",
        "\n",
        "decoder.load_state_dict(torch.load('/content/drive/My Drive/Thesis2019/models/review_decoder2_500.pth'))\n",
        "decoder.eval()\n",
        "decoder.to(torch.device(\"cuda\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=16, out_features=256, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=256, out_features=1024, bias=True)\n",
              "    (activation2): Tanh()\n",
              "    (linear3): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (sigmoid): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71OoLpU43hWF",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH7z_0fK3vee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = RidgeCV(cv=5)\n",
        "delta_census = incomes[2016].values - incomes[2011].values\n",
        "delta_embedding = (encoder(reviews[2016].data) - encoder(reviews[2011].data)).detach().cpu().numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ApOZxNV3xtX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr.fit(delta_embedding[train_val_neighbourhoods], delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6IgNOM03y5W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted_change = lr.predict(delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fkQjARYPADLX",
        "colab_type": "code",
        "outputId": "8dd9cb4f-c9ee-41b2-a065-227a9da83a6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change[train_val_neighbourhoods]-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10074098288154398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLlU9D3QrcW3",
        "colab_type": "code",
        "outputId": "793c6ea7-fa9a-40de-f882-eef0d93d862f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Validation Error\n",
        "np.abs((predicted_change[test_neighbourhoods]-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10321286493590617"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QG5ldoCryItc",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Non-Linear Regression (Census Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK0G6Xm--bxQ",
        "colab_type": "code",
        "outputId": "ac97d851-f6e6-401c-a361-9cbf5a9ff04f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "delta_embedding = encoder(reviews[2016].data) - encoder(reviews[2011].data)\n",
        "delta_census = incomes[2016].values - incomes[2011].values\n",
        "census_data = CensusVector(delta_census, delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mEsgvF_JH8lW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [incomes[2011].shape[1], pca_components]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziKGQL_GIulg",
        "colab_type": "code",
        "outputId": "86df0b79-c374-411e-a8db-e89a5a4c978f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=16, out_features=9, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvBlIbgHOMbH",
        "colab_type": "code",
        "outputId": "340ac084-6885-4fa2-d28c-31a0f17730b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i])\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0052946128\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0057191926\n",
            "Training Results - Epoch: 2  Avg loss: 0.0029022310\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0034734008\n",
            "Training Results - Epoch: 3  Avg loss: 0.0018275210\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0024848400\n",
            "Training Results - Epoch: 4  Avg loss: 0.0013567157\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0020110682\n",
            "Training Results - Epoch: 5  Avg loss: 0.0010867487\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0017343459\n",
            "Training Results - Epoch: 6  Avg loss: 0.0009131554\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0015350160\n",
            "Training Results - Epoch: 7  Avg loss: 0.0007980848\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0013765588\n",
            "Training Results - Epoch: 8  Avg loss: 0.0007130811\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0012541503\n",
            "Training Results - Epoch: 9  Avg loss: 0.0006453837\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0011438526\n",
            "Training Results - Epoch: 10  Avg loss: 0.0005883763\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0010524633\n",
            "Training Results - Epoch: 11  Avg loss: 0.0005449758\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0009656097\n",
            "Training Results - Epoch: 12  Avg loss: 0.0005092962\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0009198063\n",
            "Training Results - Epoch: 13  Avg loss: 0.0004745126\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0008473097\n",
            "Training Results - Epoch: 14  Avg loss: 0.0004517546\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0007900271\n",
            "Training Results - Epoch: 15  Avg loss: 0.0004261522\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0007460129\n",
            "Training Results - Epoch: 16  Avg loss: 0.0004003898\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0007077248\n",
            "Training Results - Epoch: 17  Avg loss: 0.0003868168\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0006724993\n",
            "Training Results - Epoch: 18  Avg loss: 0.0003650462\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0006289357\n",
            "Training Results - Epoch: 19  Avg loss: 0.0003512773\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0005923822\n",
            "Training Results - Epoch: 20  Avg loss: 0.0003348484\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0005614995\n",
            "Training Results - Epoch: 21  Avg loss: 0.0003231769\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0005378535\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003109611\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0005140118\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003026244\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0004846359\n",
            "Training Results - Epoch: 24  Avg loss: 0.0002917588\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0004611815\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002878218\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0004612219\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002769993\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0004240673\n",
            "Training Results - Epoch: 27  Avg loss: 0.0002709513\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0004277847\n",
            "Training Results - Epoch: 28  Avg loss: 0.0002611674\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0003946932\n",
            "Training Results - Epoch: 29  Avg loss: 0.0002614622\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003975415\n",
            "Training Results - Epoch: 30  Avg loss: 0.0002526504\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0003856321\n",
            "Training Results - Epoch: 31  Avg loss: 0.0002481435\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003674244\n",
            "Training Results - Epoch: 32  Avg loss: 0.0002406629\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003551058\n",
            "Training Results - Epoch: 33  Avg loss: 0.0002397576\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003550322\n",
            "Training Results - Epoch: 34  Avg loss: 0.0002368020\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0003419327\n",
            "Training Results - Epoch: 35  Avg loss: 0.0002328068\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003343916\n",
            "Training Results - Epoch: 36  Avg loss: 0.0002302818\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003330544\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002271234\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003232632\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002230579\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003199926\n",
            "Training Results - Epoch: 39  Avg loss: 0.0002223153\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003221726\n",
            "Training Results - Epoch: 40  Avg loss: 0.0002220322\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003144157\n",
            "Training Results - Epoch: 41  Avg loss: 0.0002200533\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003181084\n",
            "Training Results - Epoch: 42  Avg loss: 0.0002221828\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003055086\n",
            "Training Results - Epoch: 43  Avg loss: 0.0002172981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003102047\n",
            "Training Results - Epoch: 44  Avg loss: 0.0002183703\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002949140\n",
            "Training Results - Epoch: 45  Avg loss: 0.0002155239\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0003013164\n",
            "Training Results - Epoch: 46  Avg loss: 0.0002126334\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0002991084\n",
            "Training Results - Epoch: 47  Avg loss: 0.0002123185\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0002948737\n",
            "Training Results - Epoch: 48  Avg loss: 0.0002096473\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0002933305\n",
            "Training Results - Epoch: 49  Avg loss: 0.0002094828\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0002925655\n",
            "Training Results - Epoch: 50  Avg loss: 0.0002103975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0002870338\n",
            "Training Results - Epoch: 51  Avg loss: 0.0002070773\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0002919133\n",
            "Training Results - Epoch: 52  Avg loss: 0.0002137452\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0002852281\n",
            "Training Results - Epoch: 53  Avg loss: 0.0002092096\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0003012667\n",
            "Training Results - Epoch: 54  Avg loss: 0.0002061922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0002868155\n",
            "Training Results - Epoch: 55  Avg loss: 0.0002071724\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0002950963\n",
            "Training Results - Epoch: 56  Avg loss: 0.0002075077\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0003012244\n",
            "Training Results - Epoch: 57  Avg loss: 0.0002070648\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0002911031\n",
            "Training Results - Epoch: 58  Avg loss: 0.0002064635\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0002896440\n",
            "Training Results - Epoch: 59  Avg loss: 0.0002157964\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0003105320\n",
            "Training Results - Epoch: 60  Avg loss: 0.0002078456\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0002901960\n",
            "Training Results - Epoch: 61  Avg loss: 0.0002020920\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0002830047\n",
            "Training Results - Epoch: 62  Avg loss: 0.0002011457\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0002793719\n",
            "Training Results - Epoch: 63  Avg loss: 0.0002062064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0002869853\n",
            "Training Results - Epoch: 64  Avg loss: 0.0002029981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0002836795\n",
            "Training Results - Epoch: 65  Avg loss: 0.0002016799\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0002829547\n",
            "Training Results - Epoch: 66  Avg loss: 0.0002000001\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0002813765\n",
            "Training Results - Epoch: 67  Avg loss: 0.0002019297\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0002958087\n",
            "Training Results - Epoch: 68  Avg loss: 0.0002062787\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0002895424\n",
            "Training Results - Epoch: 69  Avg loss: 0.0001983208\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0002876343\n",
            "Training Results - Epoch: 70  Avg loss: 0.0002036984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0002861941\n",
            "Training Results - Epoch: 71  Avg loss: 0.0002037772\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0002857400\n",
            "Training Results - Epoch: 72  Avg loss: 0.0001989109\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0002852230\n",
            "Training Results - Epoch: 73  Avg loss: 0.0001995591\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0002867901\n",
            "Training Results - Epoch: 74  Avg loss: 0.0002043902\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0002838971\n",
            "Training Results - Epoch: 75  Avg loss: 0.0001976577\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0002835106\n",
            "Training Results - Epoch: 76  Avg loss: 0.0002032388\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0002957257\n",
            "Training Results - Epoch: 77  Avg loss: 0.0002022715\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0002889288\n",
            "Training Results - Epoch: 78  Avg loss: 0.0002063708\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0002803712\n",
            "Training Results - Epoch: 79  Avg loss: 0.0001987843\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0002825009\n",
            "Training Results - Epoch: 80  Avg loss: 0.0002001975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0002827219\n",
            "Training Results - Epoch: 81  Avg loss: 0.0002051805\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0002824594\n",
            "Training Results - Epoch: 82  Avg loss: 0.0001981958\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0002862979\n",
            "Training Results - Epoch: 83  Avg loss: 0.0002028522\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0003006440\n",
            "Training Results - Epoch: 84  Avg loss: 0.0001962219\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0002898003\n",
            "Training Results - Epoch: 85  Avg loss: 0.0001941722\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0002852076\n",
            "Training Results - Epoch: 86  Avg loss: 0.0001932349\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0002823843\n",
            "Training Results - Epoch: 87  Avg loss: 0.0001930959\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0002820749\n",
            "Training Results - Epoch: 88  Avg loss: 0.0001930044\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0002825436\n",
            "Training Results - Epoch: 89  Avg loss: 0.0001929540\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0002818503\n",
            "Training Results - Epoch: 90  Avg loss: 0.0001928958\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0002819360\n",
            "Training Results - Epoch: 91  Avg loss: 0.0001928407\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0002816277\n",
            "Training Results - Epoch: 92  Avg loss: 0.0001928556\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0002814854\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0002094867\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002551727\n",
            "Training Results - Epoch: 2  Avg loss: 0.0002021986\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002639475\n",
            "Training Results - Epoch: 3  Avg loss: 0.0001964550\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002539543\n",
            "Training Results - Epoch: 4  Avg loss: 0.0001945250\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002728032\n",
            "Training Results - Epoch: 5  Avg loss: 0.0001953942\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002765290\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001913078\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002797280\n",
            "Training Results - Epoch: 7  Avg loss: 0.0001918087\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002974554\n",
            "Training Results - Epoch: 8  Avg loss: 0.0001934965\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0003005832\n",
            "Training Results - Epoch: 9  Avg loss: 0.0001932254\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0003049996\n",
            "Training Results - Epoch: 10  Avg loss: 0.0001869803\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002899385\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001910618\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0003065401\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001885861\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0003033361\n",
            "Training Results - Epoch: 13  Avg loss: 0.0001925126\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0003044867\n",
            "Training Results - Epoch: 14  Avg loss: 0.0001871281\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0003038481\n",
            "Training Results - Epoch: 15  Avg loss: 0.0001916363\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0003021185\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001900204\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0003152398\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001927686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0003221671\n",
            "Training Results - Epoch: 18  Avg loss: 0.0002011893\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0003536549\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001931394\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0003320451\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001875976\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0003193914\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001919358\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0003367557\n",
            "Training Results - Epoch: 22  Avg loss: 0.0002051596\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0003273722\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001876134\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003344365\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001905554\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003406200\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001853639\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0003321398\n",
            "Training Results - Epoch: 26  Avg loss: 0.0001837425\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0003265420\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001834087\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0003249723\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001832137\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0003237965\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001831312\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003241648\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001830406\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0003239707\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001830064\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003230168\n",
            "Training Results - Epoch: 32  Avg loss: 0.0001829865\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003232188\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001829367\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003234679\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0002177585\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0001957925\n",
            "Training Results - Epoch: 2  Avg loss: 0.0002099499\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002012146\n",
            "Training Results - Epoch: 3  Avg loss: 0.0002001329\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002080956\n",
            "Training Results - Epoch: 4  Avg loss: 0.0001966304\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002192460\n",
            "Training Results - Epoch: 5  Avg loss: 0.0001981018\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002313718\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001934688\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002372895\n",
            "Training Results - Epoch: 7  Avg loss: 0.0001939614\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002512174\n",
            "Training Results - Epoch: 8  Avg loss: 0.0001891067\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002394024\n",
            "Training Results - Epoch: 9  Avg loss: 0.0001901147\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0002457901\n",
            "Training Results - Epoch: 10  Avg loss: 0.0001896836\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002565205\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001885567\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002631117\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001977700\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002739350\n",
            "Training Results - Epoch: 13  Avg loss: 0.0001901042\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0002653396\n",
            "Training Results - Epoch: 14  Avg loss: 0.0001861980\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0002744440\n",
            "Training Results - Epoch: 15  Avg loss: 0.0001863550\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002640174\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001855318\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0002611690\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001848151\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0002840006\n",
            "Training Results - Epoch: 18  Avg loss: 0.0001915814\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002974005\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001814292\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002775793\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001855080\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0002822264\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001878789\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002777933\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001821954\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0002772343\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001781877\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002768749\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001770559\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002773026\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001764355\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002774741\n",
            "Training Results - Epoch: 26  Avg loss: 0.0001761593\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002789097\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001759335\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002782116\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001757840\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002792589\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001756913\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002794942\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001755806\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002790506\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001755302\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002795636\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0002054741\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002266416\n",
            "Training Results - Epoch: 2  Avg loss: 0.0001977977\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002359701\n",
            "Training Results - Epoch: 3  Avg loss: 0.0001946735\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002375631\n",
            "Training Results - Epoch: 4  Avg loss: 0.0002037516\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002391410\n",
            "Training Results - Epoch: 5  Avg loss: 0.0001907546\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002361886\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001871035\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002459944\n",
            "Training Results - Epoch: 7  Avg loss: 0.0001870620\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002445699\n",
            "Training Results - Epoch: 8  Avg loss: 0.0002016270\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002594783\n",
            "Training Results - Epoch: 9  Avg loss: 0.0001870506\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0002569341\n",
            "Training Results - Epoch: 10  Avg loss: 0.0001876346\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002625377\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001875882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002637476\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001857454\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002613294\n",
            "Training Results - Epoch: 13  Avg loss: 0.0001881622\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0002680022\n",
            "Training Results - Epoch: 14  Avg loss: 0.0001884460\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0002723787\n",
            "Training Results - Epoch: 15  Avg loss: 0.0001854069\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002685303\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001870428\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0002766689\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001875453\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0002789263\n",
            "Training Results - Epoch: 18  Avg loss: 0.0001890842\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002788697\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001823832\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002745321\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001898124\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0002813505\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001840422\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002912102\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001866466\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0002999109\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001820221\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002909004\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001807414\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002871485\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001803542\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002858267\n",
            "Training Results - Epoch: 26  Avg loss: 0.0001801285\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002851210\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001800018\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002858396\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001799113\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002849362\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001799753\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002852393\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001798349\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002850822\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001798276\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002864075\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0002027016\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002280282\n",
            "Training Results - Epoch: 2  Avg loss: 0.0001985223\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0002427275\n",
            "Training Results - Epoch: 3  Avg loss: 0.0001982514\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0002563611\n",
            "Training Results - Epoch: 4  Avg loss: 0.0002051425\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0002889353\n",
            "Training Results - Epoch: 5  Avg loss: 0.0001966821\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0002684687\n",
            "Training Results - Epoch: 6  Avg loss: 0.0001944337\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0002635456\n",
            "Training Results - Epoch: 7  Avg loss: 0.0002010846\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0002834711\n",
            "Training Results - Epoch: 8  Avg loss: 0.0001965760\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0002662093\n",
            "Training Results - Epoch: 9  Avg loss: 0.0001944053\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0002584180\n",
            "Training Results - Epoch: 10  Avg loss: 0.0001923015\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0002727979\n",
            "Training Results - Epoch: 11  Avg loss: 0.0001982391\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0002801015\n",
            "Training Results - Epoch: 12  Avg loss: 0.0001928223\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002723756\n",
            "Training Results - Epoch: 13  Avg loss: 0.0001928239\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0002746057\n",
            "Training Results - Epoch: 14  Avg loss: 0.0001903222\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0002687639\n",
            "Training Results - Epoch: 15  Avg loss: 0.0001943051\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0002732164\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001960788\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0002778951\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001972052\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0002610001\n",
            "Training Results - Epoch: 18  Avg loss: 0.0001927216\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002711317\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001934876\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002796012\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001976686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0002851768\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001948905\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002890343\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001983711\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0002751579\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001901885\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0002701317\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001872876\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002702244\n",
            "Training Results - Epoch: 25  Avg loss: 0.0001862715\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002714824\n",
            "Training Results - Epoch: 26  Avg loss: 0.0001857836\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002732985\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001855589\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002745820\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001854859\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002753321\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001854364\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002751153\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001854287\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002761666\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001853928\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002752899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVsY58vBYt1u",
        "colab_type": "code",
        "outputId": "654b0eca-eae9-4793-b067-cd22e0f55fc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "plt.plot(range(len(validation_losses)), validation_losses, range(len(training_losses)), training_losses)\n",
        "plt.savefig(DRIVE_PATH.joinpath('loss.png'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nICKSUAoAEZGEUgCIiCTU/wfRvEq8QbnucgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "b74646a7-1c8f-466f-f89a-6ce671fa4c4b",
        "id": "MNGu-CraHZuT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Test Loss\n",
        "criterion_c(census_data.data[test_neighbourhoods], decoder_c(census_data.reviews_embedding[test_neighbourhoods]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.0002, device='cuda:0', grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cMq3lkm-zGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "actual_data = incomes[2016].values\n",
        "predicted_data = incomes[2011].values+ decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hj1wJE4NtbQG",
        "colab_type": "code",
        "outputId": "5c461d95-e8ef-4b8e-bee8-43dc5c5a2c84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09893043176503016"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gh_2u83Gt7yq",
        "colab_type": "code",
        "outputId": "1e344a08-aa05-4ced-9b44-848503f32fd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing error\n",
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.1290547679091938"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc7Acl_o_q8Y",
        "colab_type": "text"
      },
      "source": [
        "## ELMO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBVwfdBT_xAg",
        "colab_type": "text"
      },
      "source": [
        "### Load trained model and built csvs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HYNGkUzM_9Kg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reviews_elmo_all = {}\n",
        "reviews_elmo = {}\n",
        "for year in [2011, 2016]:\n",
        "  reviews_elmo_all[year] = pd.read_csv(DRIVE_PATH.joinpath('elmo_reviews_{}.csv'.format(year)))\n",
        "  reviews_elmo_all[year].set_index('Unnamed: 0', inplace=True)\n",
        "  reviews_elmo[year] = ReviewsVector(reviews_elmo_all[year].T.values)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWkrab33AUoX",
        "colab_type": "code",
        "outputId": "3d69b9f5-4b8b-4452-f082-bf1680c1b23b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "delta_embedding = reviews_elmo[2016].data.T - reviews_elmo[2011].data.T\n",
        "delta_census = incomes[2016].values - incomes[2011].values\n",
        "census_data = CensusVector(delta_census, delta_embedding)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jtFEv6GZAMBR",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK-0krFfAPq2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lingreg = RidgeCV(cv=5)\n",
        "lingreg.fit(delta_embedding[train_val_neighbourhoods].detach().cpu().numpy(), delta_census[train_val_neighbourhoods])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF3Pkd9OIeax",
        "colab_type": "code",
        "outputId": "4bbed45c-e92e-4ea5-b792-820fd7c83806",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs((predicted_change[train_val_neighbourhoods]-delta_census[train_val_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.09775755647610872"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3zzChz7MP-w",
        "colab_type": "code",
        "outputId": "77c17f8e-8f79-4d0c-efff-15d3a269a5d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs((predicted_change[test_neighbourhoods]-delta_census[test_neighbourhoods])).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.10627870847900157"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_-NPDm4AooY",
        "colab_type": "text"
      },
      "source": [
        "### Multi-target Non-Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUFcqugAwjZ",
        "colab_type": "text"
      },
      "source": [
        "#### With one layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wksmfZieA1ze",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lr = 0.001\n",
        "patience = 20\n",
        "min_lr = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3OvtNXeA2mE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [incomes[2011].shape[1], reviews_elmo_all[2016].shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XjzEumZjJxmY",
        "colab_type": "code",
        "outputId": "6ee36b76-c1c2-4600-ef3d-2d13342278eb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=9, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swv9W-tiB4As",
        "colab_type": "code",
        "outputId": "02001ed6-7919-4812-f29c-a1ab5224f593",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='inc_elmo')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0027470749\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0017631822\n",
            "Training Results - Epoch: 2  Avg loss: 0.0033430097\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0010259317\n",
            "Training Results - Epoch: 3  Avg loss: 0.0057298000\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0024721965\n",
            "Training Results - Epoch: 4  Avg loss: 0.0062040288\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0011419559\n",
            "Training Results - Epoch: 5  Avg loss: 0.0042454136\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0009794299\n",
            "Training Results - Epoch: 6  Avg loss: 0.0045418475\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0012158749\n",
            "Training Results - Epoch: 7  Avg loss: 0.0046741401\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0008619955\n",
            "Training Results - Epoch: 8  Avg loss: 0.0050186254\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0019986758\n",
            "Training Results - Epoch: 9  Avg loss: 0.0051857487\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0011316098\n",
            "Training Results - Epoch: 10  Avg loss: 0.0034887220\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0041099784\n",
            "Training Results - Epoch: 11  Avg loss: 0.0035717726\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0006159839\n",
            "Training Results - Epoch: 12  Avg loss: 0.0073471845\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0073531242\n",
            "Training Results - Epoch: 13  Avg loss: 0.0040450547\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0005303867\n",
            "Training Results - Epoch: 14  Avg loss: 0.0043873650\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0013052406\n",
            "Training Results - Epoch: 15  Avg loss: 0.0049995329\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0015400940\n",
            "Training Results - Epoch: 16  Avg loss: 0.0028425197\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0007094704\n",
            "Training Results - Epoch: 17  Avg loss: 0.0030658290\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0007827909\n",
            "Training Results - Epoch: 18  Avg loss: 0.0018407251\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004745740\n",
            "Training Results - Epoch: 19  Avg loss: 0.0030096423\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0007880414\n",
            "Training Results - Epoch: 20  Avg loss: 0.0054593232\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0031611070\n",
            "Training Results - Epoch: 21  Avg loss: 0.0050706162\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0008985772\n",
            "Training Results - Epoch: 22  Avg loss: 0.0038950780\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0009247338\n",
            "Training Results - Epoch: 23  Avg loss: 0.0042846898\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0007187241\n",
            "Training Results - Epoch: 24  Avg loss: 0.0036965364\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0009478626\n",
            "Training Results - Epoch: 25  Avg loss: 0.0030413481\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0004959910\n",
            "Training Results - Epoch: 26  Avg loss: 0.0087859000\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0057513464\n",
            "Training Results - Epoch: 27  Avg loss: 0.0046883210\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0007806867\n",
            "Training Results - Epoch: 28  Avg loss: 0.0021245294\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0007142361\n",
            "Training Results - Epoch: 29  Avg loss: 0.0018260615\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0005851798\n",
            "Training Results - Epoch: 30  Avg loss: 0.0031159244\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0008475468\n",
            "Training Results - Epoch: 31  Avg loss: 0.0069715300\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0068732702\n",
            "Training Results - Epoch: 32  Avg loss: 0.0031779636\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003932421\n",
            "Training Results - Epoch: 33  Avg loss: 0.0037694117\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0013796252\n",
            "Training Results - Epoch: 34  Avg loss: 0.0031547303\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0004526401\n",
            "Training Results - Epoch: 35  Avg loss: 0.0115309734\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0091489470\n",
            "Training Results - Epoch: 36  Avg loss: 0.0036898863\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0007829980\n",
            "Training Results - Epoch: 37  Avg loss: 0.0034295439\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0006698558\n",
            "Training Results - Epoch: 38  Avg loss: 0.0032897945\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0009184214\n",
            "Training Results - Epoch: 39  Avg loss: 0.0031645254\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0005282304\n",
            "Training Results - Epoch: 40  Avg loss: 0.0066140522\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0055136000\n",
            "Training Results - Epoch: 41  Avg loss: 0.0044871956\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0008973163\n",
            "Training Results - Epoch: 42  Avg loss: 0.0054596386\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0024391679\n",
            "Training Results - Epoch: 43  Avg loss: 0.0080726436\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0067576609\n",
            "Training Results - Epoch: 44  Avg loss: 0.0032912948\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0006967921\n",
            "Training Results - Epoch: 45  Avg loss: 0.0050966962\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0017902183\n",
            "Training Results - Epoch: 46  Avg loss: 0.0039716931\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0011306985\n",
            "Training Results - Epoch: 47  Avg loss: 0.0032618937\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0008213299\n",
            "Training Results - Epoch: 48  Avg loss: 0.0075039956\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0053099532\n",
            "Training Results - Epoch: 49  Avg loss: 0.0054821717\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0011759359\n",
            "Training Results - Epoch: 50  Avg loss: 0.0016138917\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0003496226\n",
            "Training Results - Epoch: 51  Avg loss: 0.0043340653\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0016940233\n",
            "Training Results - Epoch: 52  Avg loss: 0.0034625549\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0005873900\n",
            "Training Results - Epoch: 53  Avg loss: 0.0056862663\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0014828282\n",
            "Training Results - Epoch: 54  Avg loss: 0.0061414499\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0026830865\n",
            "Training Results - Epoch: 55  Avg loss: 0.0047109819\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0012133954\n",
            "Training Results - Epoch: 56  Avg loss: 0.0069823566\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0067206004\n",
            "Training Results - Epoch: 57  Avg loss: 0.0041574048\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0007673631\n",
            "Training Results - Epoch: 58  Avg loss: 0.0037351709\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0016029159\n",
            "Training Results - Epoch: 59  Avg loss: 0.0023454963\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0004547865\n",
            "Training Results - Epoch: 60  Avg loss: 0.0032852423\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0009179229\n",
            "Training Results - Epoch: 61  Avg loss: 0.0033694007\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0011810280\n",
            "Training Results - Epoch: 62  Avg loss: 0.0039150032\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0016636107\n",
            "Training Results - Epoch: 63  Avg loss: 0.0051263540\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0015749608\n",
            "Training Results - Epoch: 64  Avg loss: 0.0052822305\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0023122608\n",
            "Training Results - Epoch: 65  Avg loss: 0.0042128402\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0007907598\n",
            "Training Results - Epoch: 66  Avg loss: 0.0049749979\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0013108775\n",
            "Training Results - Epoch: 67  Avg loss: 0.0052697866\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0011271338\n",
            "Training Results - Epoch: 68  Avg loss: 0.0040547204\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0016732011\n",
            "Training Results - Epoch: 69  Avg loss: 0.0024720788\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0008902365\n",
            "Training Results - Epoch: 70  Avg loss: 0.0055855651\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0045196156\n",
            "Training Results - Epoch: 71  Avg loss: 0.0058684539\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0012264613\n",
            "Training Results - Epoch: 72  Avg loss: 0.0040010084\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0009964701\n",
            "Training Results - Epoch: 73  Avg loss: 0.0024851091\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0007904117\n",
            "Training Results - Epoch: 74  Avg loss: 0.0020398898\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0005317178\n",
            "Training Results - Epoch: 75  Avg loss: 0.0008267771\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0006968775\n",
            "Training Results - Epoch: 76  Avg loss: 0.0004747845\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0005756957\n",
            "Training Results - Epoch: 77  Avg loss: 0.0003086412\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0005230666\n",
            "Training Results - Epoch: 78  Avg loss: 0.0002287636\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0004712618\n",
            "Training Results - Epoch: 79  Avg loss: 0.0001909447\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0004382482\n",
            "Training Results - Epoch: 80  Avg loss: 0.0001735003\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0004335531\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0030453553\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0006530937\n",
            "Training Results - Epoch: 2  Avg loss: 0.0062713940\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0018987502\n",
            "Training Results - Epoch: 3  Avg loss: 0.0039372829\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0009195629\n",
            "Training Results - Epoch: 4  Avg loss: 0.0033283342\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0012542669\n",
            "Training Results - Epoch: 5  Avg loss: 0.0047631169\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0012619548\n",
            "Training Results - Epoch: 6  Avg loss: 0.0069245846\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0043002951\n",
            "Training Results - Epoch: 7  Avg loss: 0.0091345524\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0099154198\n",
            "Training Results - Epoch: 8  Avg loss: 0.0067072553\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0020649011\n",
            "Training Results - Epoch: 9  Avg loss: 0.0149057740\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0163444956\n",
            "Training Results - Epoch: 10  Avg loss: 0.0044557114\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0014500548\n",
            "Training Results - Epoch: 11  Avg loss: 0.0034574774\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0012440901\n",
            "Training Results - Epoch: 12  Avg loss: 0.0036413673\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0008788936\n",
            "Training Results - Epoch: 13  Avg loss: 0.0029945670\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0006316047\n",
            "Training Results - Epoch: 14  Avg loss: 0.0026186743\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0005245995\n",
            "Training Results - Epoch: 15  Avg loss: 0.0030420966\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0006280362\n",
            "Training Results - Epoch: 16  Avg loss: 0.0049312444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0018181235\n",
            "Training Results - Epoch: 17  Avg loss: 0.0025607687\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0004944680\n",
            "Training Results - Epoch: 18  Avg loss: 0.0078647066\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0108727903\n",
            "Training Results - Epoch: 19  Avg loss: 0.0033410333\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0008328759\n",
            "Training Results - Epoch: 20  Avg loss: 0.0024941407\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0005497324\n",
            "Training Results - Epoch: 21  Avg loss: 0.0026060863\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0008407458\n",
            "Training Results - Epoch: 22  Avg loss: 0.0028515554\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0005208068\n",
            "Training Results - Epoch: 23  Avg loss: 0.0034269799\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0011329324\n",
            "Training Results - Epoch: 24  Avg loss: 0.0089048331\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0101406094\n",
            "Training Results - Epoch: 25  Avg loss: 0.0040426398\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0016112910\n",
            "Training Results - Epoch: 26  Avg loss: 0.0127008647\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0142824286\n",
            "Training Results - Epoch: 27  Avg loss: 0.0045359369\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0008754329\n",
            "Training Results - Epoch: 28  Avg loss: 0.0036282271\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0008098663\n",
            "Training Results - Epoch: 29  Avg loss: 0.0028843444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0005997395\n",
            "Training Results - Epoch: 30  Avg loss: 0.0028912102\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0004618465\n",
            "Training Results - Epoch: 31  Avg loss: 0.0028645643\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0018429412\n",
            "Training Results - Epoch: 32  Avg loss: 0.0105930503\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0106688036\n",
            "Training Results - Epoch: 33  Avg loss: 0.0094607595\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0069255672\n",
            "Training Results - Epoch: 34  Avg loss: 0.0018380279\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0003821061\n",
            "Training Results - Epoch: 35  Avg loss: 0.0031760258\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0016757367\n",
            "Training Results - Epoch: 36  Avg loss: 0.0041283956\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0014804193\n",
            "Training Results - Epoch: 37  Avg loss: 0.0045440921\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0009679632\n",
            "Training Results - Epoch: 38  Avg loss: 0.0080316794\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0107540631\n",
            "Training Results - Epoch: 39  Avg loss: 0.0086922528\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0066249190\n",
            "Training Results - Epoch: 40  Avg loss: 0.0048058880\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0014174354\n",
            "Training Results - Epoch: 41  Avg loss: 0.0028091964\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0005064272\n",
            "Training Results - Epoch: 42  Avg loss: 0.0041178389\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0026193268\n",
            "Training Results - Epoch: 43  Avg loss: 0.0030977626\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0007955147\n",
            "Training Results - Epoch: 44  Avg loss: 0.0042290105\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0018847768\n",
            "Training Results - Epoch: 45  Avg loss: 0.0079786531\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0054924956\n",
            "Training Results - Epoch: 46  Avg loss: 0.0039844309\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0008422997\n",
            "Training Results - Epoch: 47  Avg loss: 0.0036453027\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0010589937\n",
            "Training Results - Epoch: 48  Avg loss: 0.0074933547\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0090623196\n",
            "Training Results - Epoch: 49  Avg loss: 0.0031859853\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0004407763\n",
            "Training Results - Epoch: 50  Avg loss: 0.0066013654\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0029797302\n",
            "Training Results - Epoch: 51  Avg loss: 0.0046596341\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0036507977\n",
            "Training Results - Epoch: 52  Avg loss: 0.0025704812\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0014035955\n",
            "Training Results - Epoch: 53  Avg loss: 0.0017371408\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0013207158\n",
            "Training Results - Epoch: 54  Avg loss: 0.0028221274\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0010479526\n",
            "Training Results - Epoch: 55  Avg loss: 0.0051496922\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0018244878\n",
            "Training Results - Epoch: 56  Avg loss: 0.0033552658\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0016718112\n",
            "Training Results - Epoch: 57  Avg loss: 0.0017240822\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0011244553\n",
            "Training Results - Epoch: 58  Avg loss: 0.0009039843\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0008560507\n",
            "Training Results - Epoch: 59  Avg loss: 0.0005348600\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0006848862\n",
            "Training Results - Epoch: 60  Avg loss: 0.0002829524\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0006096136\n",
            "Training Results - Epoch: 61  Avg loss: 0.0001777156\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0005300009\n",
            "Training Results - Epoch: 62  Avg loss: 0.0001269284\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0004340192\n",
            "Training Results - Epoch: 63  Avg loss: 0.0001041707\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0003949812\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000941690\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0003721870\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000892599\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0003592875\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000863311\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0003652838\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000839316\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0003573865\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000820506\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0003603760\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000804998\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0003585431\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000791897\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0003570277\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000778166\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0003561957\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000766939\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0003558577\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000756461\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0003500792\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000749468\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0003501082\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000745293\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0003592146\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000735376\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0003532503\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000728801\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0003546357\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000723154\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0003478181\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000715725\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0003488810\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000715211\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0003520584\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000714742\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0003401468\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000706777\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0003545023\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000700648\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0003432002\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000696849\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0003491890\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000699160\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0003411937\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000688280\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0003482100\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000687886\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0003528648\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000680746\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0003420039\n",
            "Training Results - Epoch: 89  Avg loss: 0.0000683345\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 89  Avg loss: 0.0003454772\n",
            "Training Results - Epoch: 90  Avg loss: 0.0000681364\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 90  Avg loss: 0.0003440831\n",
            "Training Results - Epoch: 91  Avg loss: 0.0000691594\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 91  Avg loss: 0.0003559454\n",
            "Training Results - Epoch: 92  Avg loss: 0.0000672465\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 92  Avg loss: 0.0003503584\n",
            "Training Results - Epoch: 93  Avg loss: 0.0000679087\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 93  Avg loss: 0.0003484356\n",
            "Training Results - Epoch: 94  Avg loss: 0.0000663129\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 94  Avg loss: 0.0003473243\n",
            "Training Results - Epoch: 95  Avg loss: 0.0000668654\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 95  Avg loss: 0.0003415387\n",
            "Training Results - Epoch: 96  Avg loss: 0.0000671675\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 96  Avg loss: 0.0003471791\n",
            "Training Results - Epoch: 97  Avg loss: 0.0000673260\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 97  Avg loss: 0.0003563190\n",
            "Training Results - Epoch: 98  Avg loss: 0.0000694054\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 98  Avg loss: 0.0003355240\n",
            "Training Results - Epoch: 99  Avg loss: 0.0000687401\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 99  Avg loss: 0.0003592692\n",
            "Training Results - Epoch: 100  Avg loss: 0.0000667983\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 100  Avg loss: 0.0003450571\n",
            "Training Results - Epoch: 101  Avg loss: 0.0000694598\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 101  Avg loss: 0.0003675118\n",
            "Training Results - Epoch: 102  Avg loss: 0.0000655397\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 102  Avg loss: 0.0003467021\n",
            "Training Results - Epoch: 103  Avg loss: 0.0000677339\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 103  Avg loss: 0.0003589524\n",
            "Training Results - Epoch: 104  Avg loss: 0.0000660676\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 104  Avg loss: 0.0003350228\n",
            "Training Results - Epoch: 105  Avg loss: 0.0000685371\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 105  Avg loss: 0.0003592942\n",
            "Training Results - Epoch: 106  Avg loss: 0.0000725803\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 106  Avg loss: 0.0003255201\n",
            "Training Results - Epoch: 107  Avg loss: 0.0000686444\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 107  Avg loss: 0.0003538083\n",
            "Training Results - Epoch: 108  Avg loss: 0.0000919582\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 108  Avg loss: 0.0003413938\n",
            "Training Results - Epoch: 109  Avg loss: 0.0000728351\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 109  Avg loss: 0.0003483343\n",
            "Training Results - Epoch: 110  Avg loss: 0.0000724575\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 110  Avg loss: 0.0003619439\n",
            "Training Results - Epoch: 111  Avg loss: 0.0000777192\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 111  Avg loss: 0.0003423285\n",
            "Training Results - Epoch: 112  Avg loss: 0.0000949124\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 112  Avg loss: 0.0003590333\n",
            "Training Results - Epoch: 113  Avg loss: 0.0001085266\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 113  Avg loss: 0.0003564966\n",
            "Training Results - Epoch: 114  Avg loss: 0.0001939027\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 114  Avg loss: 0.0003610990\n",
            "Training Results - Epoch: 115  Avg loss: 0.0002770722\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 115  Avg loss: 0.0005662799\n",
            "Training Results - Epoch: 116  Avg loss: 0.0001723658\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 116  Avg loss: 0.0004013801\n",
            "Training Results - Epoch: 117  Avg loss: 0.0002209147\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 117  Avg loss: 0.0006297185\n",
            "Training Results - Epoch: 118  Avg loss: 0.0001418500\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 118  Avg loss: 0.0003705452\n",
            "Training Results - Epoch: 119  Avg loss: 0.0001372487\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 119  Avg loss: 0.0003773692\n",
            "Training Results - Epoch: 120  Avg loss: 0.0001308143\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 120  Avg loss: 0.0004988674\n",
            "Training Results - Epoch: 121  Avg loss: 0.0000719345\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 121  Avg loss: 0.0003633019\n",
            "Training Results - Epoch: 122  Avg loss: 0.0000719418\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 122  Avg loss: 0.0003670610\n",
            "Training Results - Epoch: 123  Avg loss: 0.0000698124\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 123  Avg loss: 0.0003567356\n",
            "Training Results - Epoch: 124  Avg loss: 0.0000700868\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 124  Avg loss: 0.0003721521\n",
            "Training Results - Epoch: 125  Avg loss: 0.0000881591\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 125  Avg loss: 0.0004434033\n",
            "Training Results - Epoch: 126  Avg loss: 0.0000919342\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 126  Avg loss: 0.0003800756\n",
            "Training Results - Epoch: 127  Avg loss: 0.0000702200\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 127  Avg loss: 0.0003713089\n",
            "Training Results - Epoch: 128  Avg loss: 0.0000629941\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 128  Avg loss: 0.0003800176\n",
            "Training Results - Epoch: 129  Avg loss: 0.0000611746\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 129  Avg loss: 0.0003734704\n",
            "Training Results - Epoch: 130  Avg loss: 0.0000601909\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 130  Avg loss: 0.0003706851\n",
            "Training Results - Epoch: 131  Avg loss: 0.0000595390\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 131  Avg loss: 0.0003693766\n",
            "Training Results - Epoch: 132  Avg loss: 0.0000590092\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 132  Avg loss: 0.0003681479\n",
            "Training Results - Epoch: 133  Avg loss: 0.0000586733\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 133  Avg loss: 0.0003672430\n",
            "Training Results - Epoch: 134  Avg loss: 0.0000584570\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 134  Avg loss: 0.0003667800\n",
            "Training Results - Epoch: 135  Avg loss: 0.0000582219\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 135  Avg loss: 0.0003658599\n",
            "Training Results - Epoch: 136  Avg loss: 0.0000580453\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 136  Avg loss: 0.0003660521\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0023674824\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0004276795\n",
            "Training Results - Epoch: 2  Avg loss: 0.0046861512\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0014475648\n",
            "Training Results - Epoch: 3  Avg loss: 0.0083893361\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0040739791\n",
            "Training Results - Epoch: 4  Avg loss: 0.0025650237\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0005910692\n",
            "Training Results - Epoch: 5  Avg loss: 0.0031741817\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0009383723\n",
            "Training Results - Epoch: 6  Avg loss: 0.0049190565\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0014449853\n",
            "Training Results - Epoch: 7  Avg loss: 0.0028074483\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0006218661\n",
            "Training Results - Epoch: 8  Avg loss: 0.0035849180\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0010704267\n",
            "Training Results - Epoch: 9  Avg loss: 0.0035288293\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0010051749\n",
            "Training Results - Epoch: 10  Avg loss: 0.0040425864\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0009506788\n",
            "Training Results - Epoch: 11  Avg loss: 0.0042620696\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0015524308\n",
            "Training Results - Epoch: 12  Avg loss: 0.0031419658\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0005926784\n",
            "Training Results - Epoch: 13  Avg loss: 0.0046635885\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0019638987\n",
            "Training Results - Epoch: 14  Avg loss: 0.0052468909\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0015551915\n",
            "Training Results - Epoch: 15  Avg loss: 0.0088841547\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0058038907\n",
            "Training Results - Epoch: 16  Avg loss: 0.0027983810\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0005662047\n",
            "Training Results - Epoch: 17  Avg loss: 0.0065930627\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0039343363\n",
            "Training Results - Epoch: 18  Avg loss: 0.0030841678\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0005145606\n",
            "Training Results - Epoch: 19  Avg loss: 0.0049502885\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0019417168\n",
            "Training Results - Epoch: 20  Avg loss: 0.0080568239\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0055981708\n",
            "Training Results - Epoch: 21  Avg loss: 0.0116010165\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0084748874\n",
            "Training Results - Epoch: 22  Avg loss: 0.0090951217\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0065144592\n",
            "Training Results - Epoch: 23  Avg loss: 0.0016987950\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0008366724\n",
            "Training Results - Epoch: 24  Avg loss: 0.0004425013\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003153362\n",
            "Training Results - Epoch: 25  Avg loss: 0.0003283558\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002849163\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002553813\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0002641281\n",
            "Training Results - Epoch: 27  Avg loss: 0.0002159709\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0002490566\n",
            "Training Results - Epoch: 28  Avg loss: 0.0001829155\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002373243\n",
            "Training Results - Epoch: 29  Avg loss: 0.0001675373\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002356239\n",
            "Training Results - Epoch: 30  Avg loss: 0.0001408112\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002243005\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001269387\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0002202317\n",
            "Training Results - Epoch: 32  Avg loss: 0.0001135720\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0002191729\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001054846\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0002183335\n",
            "Training Results - Epoch: 34  Avg loss: 0.0000963185\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0002140875\n",
            "Training Results - Epoch: 35  Avg loss: 0.0000909950\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0002116242\n",
            "Training Results - Epoch: 36  Avg loss: 0.0000875660\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0002105467\n",
            "Training Results - Epoch: 37  Avg loss: 0.0000806717\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0002085039\n",
            "Training Results - Epoch: 38  Avg loss: 0.0000780734\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0002080720\n",
            "Training Results - Epoch: 39  Avg loss: 0.0000748342\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0002073949\n",
            "Training Results - Epoch: 40  Avg loss: 0.0000735106\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0002077465\n",
            "Training Results - Epoch: 41  Avg loss: 0.0000713287\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0002057164\n",
            "Training Results - Epoch: 42  Avg loss: 0.0000699865\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0002050145\n",
            "Training Results - Epoch: 43  Avg loss: 0.0000688145\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0002043390\n",
            "Training Results - Epoch: 44  Avg loss: 0.0000678791\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002029576\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000665456\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0002021521\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000659483\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0002031861\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000652694\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0002001265\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000648795\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0002017473\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000654514\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0002013162\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000651599\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0002040762\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000633235\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0001983237\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000637250\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0002013595\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000622009\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0001984867\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000620814\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0001977130\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000618888\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0002018096\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000613491\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0001996097\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000608811\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0001962053\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000617351\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0001958381\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000603546\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0001966389\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000603984\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0001993364\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000601430\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0001973798\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000599190\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0001984175\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000602319\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0001979862\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000600877\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0001997152\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000611575\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0002041072\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000606258\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0001982397\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000588505\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0001992222\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000627747\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0002058658\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000621266\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0002103128\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000596163\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0001968706\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000600929\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0002051456\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000606677\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0002026293\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000763491\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0002214148\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000772537\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0002055137\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000940310\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0002096984\n",
            "Training Results - Epoch: 76  Avg loss: 0.0001052271\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0002065794\n",
            "Training Results - Epoch: 77  Avg loss: 0.0001465505\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0002378338\n",
            "Training Results - Epoch: 78  Avg loss: 0.0001176764\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0001996753\n",
            "Training Results - Epoch: 79  Avg loss: 0.0001251085\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0002227002\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000963842\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0002129942\n",
            "Training Results - Epoch: 81  Avg loss: 0.0000800128\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 81  Avg loss: 0.0002088741\n",
            "Training Results - Epoch: 82  Avg loss: 0.0000699505\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 82  Avg loss: 0.0002054526\n",
            "Training Results - Epoch: 83  Avg loss: 0.0000638678\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 83  Avg loss: 0.0002026683\n",
            "Training Results - Epoch: 84  Avg loss: 0.0000602251\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 84  Avg loss: 0.0002022680\n",
            "Training Results - Epoch: 85  Avg loss: 0.0000579224\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 85  Avg loss: 0.0002009584\n",
            "Training Results - Epoch: 86  Avg loss: 0.0000565909\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 86  Avg loss: 0.0002010511\n",
            "Training Results - Epoch: 87  Avg loss: 0.0000557198\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 87  Avg loss: 0.0002010392\n",
            "Training Results - Epoch: 88  Avg loss: 0.0000552770\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 88  Avg loss: 0.0002004824\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0007606330\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0006288809\n",
            "Training Results - Epoch: 2  Avg loss: 0.0011372123\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0007748641\n",
            "Training Results - Epoch: 3  Avg loss: 0.0007613184\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0005618017\n",
            "Training Results - Epoch: 4  Avg loss: 0.0007621417\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0007962179\n",
            "Training Results - Epoch: 5  Avg loss: 0.0004786735\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0009893086\n",
            "Training Results - Epoch: 6  Avg loss: 0.0007618510\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0004977413\n",
            "Training Results - Epoch: 7  Avg loss: 0.0006948979\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0007406023\n",
            "Training Results - Epoch: 8  Avg loss: 0.0015811024\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0022735553\n",
            "Training Results - Epoch: 9  Avg loss: 0.0009424007\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0003555431\n",
            "Training Results - Epoch: 10  Avg loss: 0.0014218178\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0016796218\n",
            "Training Results - Epoch: 11  Avg loss: 0.0005918409\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0017101219\n",
            "Training Results - Epoch: 12  Avg loss: 0.0010056317\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0007146167\n",
            "Training Results - Epoch: 13  Avg loss: 0.0014764204\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0004663439\n",
            "Training Results - Epoch: 14  Avg loss: 0.0008712088\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0004359993\n",
            "Training Results - Epoch: 15  Avg loss: 0.0008099885\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0011137803\n",
            "Training Results - Epoch: 16  Avg loss: 0.0004517633\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0003635233\n",
            "Training Results - Epoch: 17  Avg loss: 0.0006555189\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0007687426\n",
            "Training Results - Epoch: 18  Avg loss: 0.0008979729\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004961604\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001911577\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0002585073\n",
            "Training Results - Epoch: 20  Avg loss: 0.0002247579\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0003475017\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001858808\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0003726950\n",
            "Training Results - Epoch: 22  Avg loss: 0.0004713695\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0003414408\n",
            "Training Results - Epoch: 23  Avg loss: 0.0016113191\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0011991414\n",
            "Training Results - Epoch: 24  Avg loss: 0.0004438136\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0006392810\n",
            "Training Results - Epoch: 25  Avg loss: 0.0005522676\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0007694451\n",
            "Training Results - Epoch: 26  Avg loss: 0.0014837707\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0017065849\n",
            "Training Results - Epoch: 27  Avg loss: 0.0007634211\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0004344130\n",
            "Training Results - Epoch: 28  Avg loss: 0.0019863225\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0014996240\n",
            "Training Results - Epoch: 29  Avg loss: 0.0007074431\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0010504216\n",
            "Training Results - Epoch: 30  Avg loss: 0.0042162596\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0058345523\n",
            "Training Results - Epoch: 31  Avg loss: 0.0013521089\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0023767901\n",
            "Training Results - Epoch: 32  Avg loss: 0.0021557235\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0030002946\n",
            "Training Results - Epoch: 33  Avg loss: 0.0010774272\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0008362428\n",
            "Training Results - Epoch: 34  Avg loss: 0.0016258728\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0008594430\n",
            "Training Results - Epoch: 35  Avg loss: 0.0006948059\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0008302456\n",
            "Training Results - Epoch: 36  Avg loss: 0.0002993239\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003035930\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002272716\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0002940411\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002671573\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003648820\n",
            "Training Results - Epoch: 39  Avg loss: 0.0005222705\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003105169\n",
            "Training Results - Epoch: 40  Avg loss: 0.0005945639\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0010492097\n",
            "Training Results - Epoch: 41  Avg loss: 0.0002861465\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0006592534\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001792029\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003886902\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001286936\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003163399\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001057799\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0002453436\n",
            "Training Results - Epoch: 45  Avg loss: 0.0000894842\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0002551548\n",
            "Training Results - Epoch: 46  Avg loss: 0.0000797535\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0002444795\n",
            "Training Results - Epoch: 47  Avg loss: 0.0000730178\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0002372269\n",
            "Training Results - Epoch: 48  Avg loss: 0.0000697405\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0002277976\n",
            "Training Results - Epoch: 49  Avg loss: 0.0000650134\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0002472433\n",
            "Training Results - Epoch: 50  Avg loss: 0.0000615370\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0002173167\n",
            "Training Results - Epoch: 51  Avg loss: 0.0000605777\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0002506821\n",
            "Training Results - Epoch: 52  Avg loss: 0.0000583543\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0002180924\n",
            "Training Results - Epoch: 53  Avg loss: 0.0000544759\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0002276552\n",
            "Training Results - Epoch: 54  Avg loss: 0.0000527217\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0002293848\n",
            "Training Results - Epoch: 55  Avg loss: 0.0000523403\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0002359838\n",
            "Training Results - Epoch: 56  Avg loss: 0.0000509278\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0002276423\n",
            "Training Results - Epoch: 57  Avg loss: 0.0000493275\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0002240907\n",
            "Training Results - Epoch: 58  Avg loss: 0.0000492944\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0002280462\n",
            "Training Results - Epoch: 59  Avg loss: 0.0000486667\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0002442177\n",
            "Training Results - Epoch: 60  Avg loss: 0.0000468187\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0002321862\n",
            "Training Results - Epoch: 61  Avg loss: 0.0000457739\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0002347984\n",
            "Training Results - Epoch: 62  Avg loss: 0.0000457638\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0002327616\n",
            "Training Results - Epoch: 63  Avg loss: 0.0000463820\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0002302126\n",
            "Training Results - Epoch: 64  Avg loss: 0.0000441689\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0002392176\n",
            "Training Results - Epoch: 65  Avg loss: 0.0000440338\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0002411398\n",
            "Training Results - Epoch: 66  Avg loss: 0.0000435654\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0002342251\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000434501\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0002344437\n",
            "Training Results - Epoch: 68  Avg loss: 0.0000423515\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0002376332\n",
            "Training Results - Epoch: 69  Avg loss: 0.0000423568\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0002352303\n",
            "Training Results - Epoch: 70  Avg loss: 0.0000436050\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0002538703\n",
            "Training Results - Epoch: 71  Avg loss: 0.0000431489\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0002272335\n",
            "Training Results - Epoch: 72  Avg loss: 0.0000412030\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0002355143\n",
            "Training Results - Epoch: 73  Avg loss: 0.0000409408\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0002384987\n",
            "Training Results - Epoch: 74  Avg loss: 0.0000408805\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 74  Avg loss: 0.0002388946\n",
            "Training Results - Epoch: 75  Avg loss: 0.0000408391\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 75  Avg loss: 0.0002401097\n",
            "Training Results - Epoch: 76  Avg loss: 0.0000407967\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 76  Avg loss: 0.0002405056\n",
            "Training Results - Epoch: 77  Avg loss: 0.0000407558\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 77  Avg loss: 0.0002407128\n",
            "Training Results - Epoch: 78  Avg loss: 0.0000407039\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 78  Avg loss: 0.0002403102\n",
            "Training Results - Epoch: 79  Avg loss: 0.0000406677\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 79  Avg loss: 0.0002403522\n",
            "Training Results - Epoch: 80  Avg loss: 0.0000406339\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 80  Avg loss: 0.0002384055\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0016700841\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0002607042\n",
            "Training Results - Epoch: 2  Avg loss: 0.0144599765\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0124969325\n",
            "Training Results - Epoch: 3  Avg loss: 0.0025237436\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0009000683\n",
            "Training Results - Epoch: 4  Avg loss: 0.0024622729\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0006757853\n",
            "Training Results - Epoch: 5  Avg loss: 0.0106688163\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0092193069\n",
            "Training Results - Epoch: 6  Avg loss: 0.0113228350\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0087623402\n",
            "Training Results - Epoch: 7  Avg loss: 0.0022358182\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0004764960\n",
            "Training Results - Epoch: 8  Avg loss: 0.0033973584\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0008698007\n",
            "Training Results - Epoch: 9  Avg loss: 0.0041964308\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0024703868\n",
            "Training Results - Epoch: 10  Avg loss: 0.0046535211\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0051427059\n",
            "Training Results - Epoch: 11  Avg loss: 0.0104188850\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0089052495\n",
            "Training Results - Epoch: 12  Avg loss: 0.0015881226\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0002542211\n",
            "Training Results - Epoch: 13  Avg loss: 0.0020844326\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0005318118\n",
            "Training Results - Epoch: 14  Avg loss: 0.0041454186\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0009202456\n",
            "Training Results - Epoch: 15  Avg loss: 0.0117511057\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0109669500\n",
            "Training Results - Epoch: 16  Avg loss: 0.0043722085\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0010429269\n",
            "Training Results - Epoch: 17  Avg loss: 0.0030521708\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0006979546\n",
            "Training Results - Epoch: 18  Avg loss: 0.0019344246\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0026679491\n",
            "Training Results - Epoch: 19  Avg loss: 0.0100671595\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0081840213\n",
            "Training Results - Epoch: 20  Avg loss: 0.0034497683\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0011118040\n",
            "Training Results - Epoch: 21  Avg loss: 0.0040642382\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0014207614\n",
            "Training Results - Epoch: 22  Avg loss: 0.0038821575\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0008458647\n",
            "Training Results - Epoch: 23  Avg loss: 0.0034604794\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0007277716\n",
            "Training Results - Epoch: 24  Avg loss: 0.0046553936\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0017879185\n",
            "Training Results - Epoch: 25  Avg loss: 0.0060496181\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0055247844\n",
            "Training Results - Epoch: 26  Avg loss: 0.0025036816\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0009446073\n",
            "Training Results - Epoch: 27  Avg loss: 0.0020432476\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0006197440\n",
            "Training Results - Epoch: 28  Avg loss: 0.0038057786\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0007807688\n",
            "Training Results - Epoch: 29  Avg loss: 0.0014677494\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0021203412\n",
            "Training Results - Epoch: 30  Avg loss: 0.0066972952\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0038567828\n",
            "Training Results - Epoch: 31  Avg loss: 0.0038451895\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0019233827\n",
            "Training Results - Epoch: 32  Avg loss: 0.0034414987\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0007124398\n",
            "Training Results - Epoch: 33  Avg loss: 0.0035495526\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0010584038\n",
            "Training Results - Epoch: 34  Avg loss: 0.0019962928\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0005333380\n",
            "Training Results - Epoch: 35  Avg loss: 0.0012007418\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0004162955\n",
            "Training Results - Epoch: 36  Avg loss: 0.0006752120\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003628592\n",
            "Training Results - Epoch: 37  Avg loss: 0.0003622188\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003011694\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002480815\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0002797642\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001855455\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0002672707\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001566326\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0002634496\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001433408\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0002638859\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001298050\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0002575339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XbmY8XxTBIuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "predicted_data = incomes[2011].values + decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwljwJczL5S9",
        "colab_type": "code",
        "outputId": "8e0cdf22-862a-4423-e07a-b1d887de8a28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Training Error\n",
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.07935540107798843"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iim2CWKFL_gV",
        "colab_type": "code",
        "outputId": "63522e02-6e18-4554-afd6-bd46cbafd468",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Testing Error\n",
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.18424671551108865"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVJXQIgaBZav",
        "colab_type": "text"
      },
      "source": [
        "#### With Additional Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPwMX2LsBuoP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sizes_c = [incomes[2011].shape[1], reviews_elmo_all[2016].shape[1]//2, reviews_elmo_all[2016].shape[1]]\n",
        "decoder_c = Decoder_C(sizes_c)\n",
        "\n",
        "if use_cuda and torch.cuda.is_available():\n",
        "    decoder_c.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjUIN1I5P8Mv",
        "colab_type": "code",
        "outputId": "76c9ddac-33c8-47d1-9db8-064da53c387d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        }
      },
      "source": [
        "decoder_c"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Decoder_C(\n",
              "  (decoder): Sequential(\n",
              "    (linear1): Linear(in_features=1024, out_features=512, bias=True)\n",
              "    (activation1): Tanh()\n",
              "    (linear2): Linear(in_features=512, out_features=9, bias=True)\n",
              "    (tanh): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cC-JK4z1P5ZD",
        "colab_type": "code",
        "outputId": "ae30315e-807f-4bd3-8b62-1f2b3254f781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "validation_losses, training_losses = [], []\n",
        "for i in range(folds):\n",
        "  print('FOLD', i)\n",
        "  t, v = train_decoder(decoder_c, census_data, all_trains[i], all_vals[i], name='inc_elmo')\n",
        "  training_losses.extend(t)\n",
        "  validation_losses.extend(v)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "FOLD 0\n",
            "Training Results - Epoch: 1  Avg loss: 0.0209234761\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0137059486\n",
            "Training Results - Epoch: 2  Avg loss: 0.0118081027\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0050664927\n",
            "Training Results - Epoch: 3  Avg loss: 0.0222936612\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0090419727\n",
            "Training Results - Epoch: 4  Avg loss: 0.0055555309\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0012912722\n",
            "Training Results - Epoch: 5  Avg loss: 0.0079673715\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0044263674\n",
            "Training Results - Epoch: 6  Avg loss: 0.0035997835\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0009635887\n",
            "Training Results - Epoch: 7  Avg loss: 0.0023973171\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0008790885\n",
            "Training Results - Epoch: 8  Avg loss: 0.0016483821\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0007659951\n",
            "Training Results - Epoch: 9  Avg loss: 0.0016910993\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0009042362\n",
            "Training Results - Epoch: 10  Avg loss: 0.0013742405\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0009612332\n",
            "Training Results - Epoch: 11  Avg loss: 0.0010536091\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0005961926\n",
            "Training Results - Epoch: 12  Avg loss: 0.0078729668\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0060396618\n",
            "Training Results - Epoch: 13  Avg loss: 0.0076263702\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0070709143\n",
            "Training Results - Epoch: 14  Avg loss: 0.0014005624\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0003204112\n",
            "Training Results - Epoch: 15  Avg loss: 0.0097919933\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0090133241\n",
            "Training Results - Epoch: 16  Avg loss: 0.0008448686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0006125423\n",
            "Training Results - Epoch: 17  Avg loss: 0.0012644361\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0012978293\n",
            "Training Results - Epoch: 18  Avg loss: 0.0010809621\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0002375523\n",
            "Training Results - Epoch: 19  Avg loss: 0.0003762410\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0003087233\n",
            "Training Results - Epoch: 20  Avg loss: 0.0009573086\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0009872397\n",
            "Training Results - Epoch: 21  Avg loss: 0.0007886764\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0002996379\n",
            "Training Results - Epoch: 22  Avg loss: 0.0014687753\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0011608961\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003544565\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003755605\n",
            "Training Results - Epoch: 24  Avg loss: 0.0004847013\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0002735709\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002287414\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0002357951\n",
            "Training Results - Epoch: 26  Avg loss: 0.0016076189\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0005357857\n",
            "Training Results - Epoch: 27  Avg loss: 0.0003572941\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0003493144\n",
            "Training Results - Epoch: 28  Avg loss: 0.0002622703\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0002438932\n",
            "Training Results - Epoch: 29  Avg loss: 0.0003392376\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0002327735\n",
            "Training Results - Epoch: 30  Avg loss: 0.0003698750\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0002637104\n",
            "Training Results - Epoch: 31  Avg loss: 0.0005904781\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0005082350\n",
            "Training Results - Epoch: 32  Avg loss: 0.0003921606\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003088469\n",
            "Training Results - Epoch: 33  Avg loss: 0.0004605707\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0002675257\n",
            "Training Results - Epoch: 34  Avg loss: 0.0002348902\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0002384403\n",
            "Training Results - Epoch: 35  Avg loss: 0.0003914771\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003461765\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001979115\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0002294719\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002555748\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0002144987\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002022896\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0002300637\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001978560\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0002186293\n",
            "Training Results - Epoch: 40  Avg loss: 0.0005718805\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003076494\n",
            "Training Results - Epoch: 41  Avg loss: 0.0003806804\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003395297\n",
            "Training Results - Epoch: 42  Avg loss: 0.0090022961\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0039885530\n",
            "Training Results - Epoch: 43  Avg loss: 0.0024902905\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0010844753\n",
            "Training Results - Epoch: 44  Avg loss: 0.0060266697\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0023597043\n",
            "Training Results - Epoch: 45  Avg loss: 0.0116152811\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0051816167\n",
            "Training Results - Epoch: 46  Avg loss: 0.0525481962\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0384329599\n",
            "Training Results - Epoch: 47  Avg loss: 0.0149949582\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0104279076\n",
            "Training Results - Epoch: 48  Avg loss: 0.0190662501\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0058356013\n",
            "Training Results - Epoch: 49  Avg loss: 0.0202042015\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0040225004\n",
            "Training Results - Epoch: 50  Avg loss: 0.0047049238\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0012803573\n",
            "Training Results - Epoch: 51  Avg loss: 0.0093596344\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0020376538\n",
            "Training Results - Epoch: 52  Avg loss: 0.0046426410\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0010604186\n",
            "Training Results - Epoch: 53  Avg loss: 0.0028923961\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0007205526\n",
            "Training Results - Epoch: 54  Avg loss: 0.0059846633\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0014750071\n",
            "Training Results - Epoch: 55  Avg loss: 0.0071159444\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0012653235\n",
            "Training Results - Epoch: 56  Avg loss: 0.0055018083\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0020795912\n",
            "Training Results - Epoch: 57  Avg loss: 0.0033840921\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0016299571\n",
            "Training Results - Epoch: 58  Avg loss: 0.0011914511\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0005449373\n",
            "Training Results - Epoch: 59  Avg loss: 0.0007724444\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0004047563\n",
            "Training Results - Epoch: 60  Avg loss: 0.0005796363\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0003403918\n",
            "Training Results - Epoch: 61  Avg loss: 0.0004444288\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0003023699\n",
            "Training Results - Epoch: 62  Avg loss: 0.0003661908\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0002883672\n",
            "Training Results - Epoch: 63  Avg loss: 0.0003010644\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0002532526\n",
            "Training Results - Epoch: 64  Avg loss: 0.0002618664\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0002426676\n",
            "Training Results - Epoch: 65  Avg loss: 0.0002398358\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0002408403\n",
            "Training Results - Epoch: 66  Avg loss: 0.0002148095\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0002216096\n",
            "Training Results - Epoch: 67  Avg loss: 0.0002040939\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0002224537\n",
            "FOLD 1\n",
            "Training Results - Epoch: 1  Avg loss: 0.0038729064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0012777148\n",
            "Training Results - Epoch: 2  Avg loss: 0.0229069225\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0346267614\n",
            "Training Results - Epoch: 3  Avg loss: 0.0058178904\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0056231690\n",
            "Training Results - Epoch: 4  Avg loss: 0.0030466491\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0010909253\n",
            "Training Results - Epoch: 5  Avg loss: 0.0082427738\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0023305908\n",
            "Training Results - Epoch: 6  Avg loss: 0.0178509255\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0075035422\n",
            "Training Results - Epoch: 7  Avg loss: 0.0158472032\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0045470217\n",
            "Training Results - Epoch: 8  Avg loss: 0.0105445064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0022246659\n",
            "Training Results - Epoch: 9  Avg loss: 0.0039132485\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0031905916\n",
            "Training Results - Epoch: 10  Avg loss: 0.0023206843\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0010838263\n",
            "Training Results - Epoch: 11  Avg loss: 0.0023979248\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0010177247\n",
            "Training Results - Epoch: 12  Avg loss: 0.0083066949\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0014815956\n",
            "Training Results - Epoch: 13  Avg loss: 0.0026281319\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0037818936\n",
            "Training Results - Epoch: 14  Avg loss: 0.0026621882\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0007930952\n",
            "Training Results - Epoch: 15  Avg loss: 0.0394074076\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0516973707\n",
            "Training Results - Epoch: 16  Avg loss: 0.0027989776\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0027665930\n",
            "Training Results - Epoch: 17  Avg loss: 0.0007540434\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0003533840\n",
            "Training Results - Epoch: 18  Avg loss: 0.0052969720\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0035539985\n",
            "Training Results - Epoch: 19  Avg loss: 0.0027602705\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0027453763\n",
            "Training Results - Epoch: 20  Avg loss: 0.0066798491\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0053626495\n",
            "Training Results - Epoch: 21  Avg loss: 0.0031332454\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0009457357\n",
            "Training Results - Epoch: 22  Avg loss: 0.0063796129\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0010165460\n",
            "Training Results - Epoch: 23  Avg loss: 0.0025797371\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0015682562\n",
            "Training Results - Epoch: 24  Avg loss: 0.0098357127\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0060416971\n",
            "Training Results - Epoch: 25  Avg loss: 0.0010043795\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0004552516\n",
            "Training Results - Epoch: 26  Avg loss: 0.0015178256\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0008334461\n",
            "Training Results - Epoch: 27  Avg loss: 0.0016487649\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0010943685\n",
            "Training Results - Epoch: 28  Avg loss: 0.0006669607\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0008082921\n",
            "Training Results - Epoch: 29  Avg loss: 0.0002661461\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003262794\n",
            "Training Results - Epoch: 30  Avg loss: 0.0003279514\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0004524963\n",
            "Training Results - Epoch: 31  Avg loss: 0.0002108049\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003487392\n",
            "Training Results - Epoch: 32  Avg loss: 0.0016971686\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0019066113\n",
            "Training Results - Epoch: 33  Avg loss: 0.0007612751\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0009475802\n",
            "Training Results - Epoch: 34  Avg loss: 0.0006453073\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0006277307\n",
            "Training Results - Epoch: 35  Avg loss: 0.0003280915\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003416151\n",
            "Training Results - Epoch: 36  Avg loss: 0.0003148673\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003728634\n",
            "Training Results - Epoch: 37  Avg loss: 0.0003565701\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003745997\n",
            "Training Results - Epoch: 38  Avg loss: 0.0014369330\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0016561772\n",
            "Training Results - Epoch: 39  Avg loss: 0.0007395268\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0006609031\n",
            "Training Results - Epoch: 40  Avg loss: 0.0002121320\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003315237\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001745075\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003214807\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001716635\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003411186\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001870711\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0002892160\n",
            "Training Results - Epoch: 44  Avg loss: 0.0002543216\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0003330368\n",
            "Training Results - Epoch: 45  Avg loss: 0.0002691156\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0003989186\n",
            "Training Results - Epoch: 46  Avg loss: 0.0001495828\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0003068169\n",
            "Training Results - Epoch: 47  Avg loss: 0.0001504166\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0003017406\n",
            "Training Results - Epoch: 48  Avg loss: 0.0001423550\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0003193731\n",
            "Training Results - Epoch: 49  Avg loss: 0.0001608907\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0002985776\n",
            "Training Results - Epoch: 50  Avg loss: 0.0003652330\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0003690072\n",
            "Training Results - Epoch: 51  Avg loss: 0.0005442092\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0006195181\n",
            "Training Results - Epoch: 52  Avg loss: 0.0003272453\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0003820556\n",
            "Training Results - Epoch: 53  Avg loss: 0.0004523339\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0004208234\n",
            "Training Results - Epoch: 54  Avg loss: 0.0007650150\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0004854107\n",
            "Training Results - Epoch: 55  Avg loss: 0.0011536117\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0004103375\n",
            "Training Results - Epoch: 56  Avg loss: 0.0013891462\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0005507194\n",
            "Training Results - Epoch: 57  Avg loss: 0.0103998809\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0070389733\n",
            "Training Results - Epoch: 58  Avg loss: 0.0026323652\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0020075709\n",
            "Training Results - Epoch: 59  Avg loss: 0.0278745411\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0159392082\n",
            "Training Results - Epoch: 60  Avg loss: 0.0372452493\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0480063804\n",
            "Training Results - Epoch: 61  Avg loss: 0.0284605774\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0195518723\n",
            "Training Results - Epoch: 62  Avg loss: 0.0158445250\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0031058119\n",
            "Training Results - Epoch: 63  Avg loss: 0.0124074569\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0025235065\n",
            "Training Results - Epoch: 64  Avg loss: 0.0076041246\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0085056117\n",
            "Training Results - Epoch: 65  Avg loss: 0.0017666549\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0012855284\n",
            "Training Results - Epoch: 66  Avg loss: 0.0009898073\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0007255966\n",
            "Training Results - Epoch: 67  Avg loss: 0.0007333428\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0006269001\n",
            "Training Results - Epoch: 68  Avg loss: 0.0005426778\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 68  Avg loss: 0.0006130984\n",
            "Training Results - Epoch: 69  Avg loss: 0.0004220344\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 69  Avg loss: 0.0004934772\n",
            "Training Results - Epoch: 70  Avg loss: 0.0003368316\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 70  Avg loss: 0.0004778039\n",
            "Training Results - Epoch: 71  Avg loss: 0.0002766353\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 71  Avg loss: 0.0004296267\n",
            "Training Results - Epoch: 72  Avg loss: 0.0002409093\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 72  Avg loss: 0.0004097399\n",
            "Training Results - Epoch: 73  Avg loss: 0.0002139220\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 73  Avg loss: 0.0004064392\n",
            "FOLD 2\n",
            "Training Results - Epoch: 1  Avg loss: 0.0089497009\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0036850642\n",
            "Training Results - Epoch: 2  Avg loss: 0.0060317655\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0045329693\n",
            "Training Results - Epoch: 3  Avg loss: 0.0062740064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0022490372\n",
            "Training Results - Epoch: 4  Avg loss: 0.0051331523\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0014068924\n",
            "Training Results - Epoch: 5  Avg loss: 0.0046134076\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0023371136\n",
            "Training Results - Epoch: 6  Avg loss: 0.0166671898\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0093681441\n",
            "Training Results - Epoch: 7  Avg loss: 0.0026040381\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0007105530\n",
            "Training Results - Epoch: 8  Avg loss: 0.0038552185\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0012596538\n",
            "Training Results - Epoch: 9  Avg loss: 0.0186531285\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0147233224\n",
            "Training Results - Epoch: 10  Avg loss: 0.0059663665\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0013919593\n",
            "Training Results - Epoch: 11  Avg loss: 0.0075280385\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0067116433\n",
            "Training Results - Epoch: 12  Avg loss: 0.0028430127\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0012243952\n",
            "Training Results - Epoch: 13  Avg loss: 0.0013943097\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0007055369\n",
            "Training Results - Epoch: 14  Avg loss: 0.0035520103\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0016306185\n",
            "Training Results - Epoch: 15  Avg loss: 0.0027901019\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0016005172\n",
            "Training Results - Epoch: 16  Avg loss: 0.0004190780\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0005760932\n",
            "Training Results - Epoch: 17  Avg loss: 0.0008295520\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0007143410\n",
            "Training Results - Epoch: 18  Avg loss: 0.0011961972\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0005647859\n",
            "Training Results - Epoch: 19  Avg loss: 0.0025606984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0026361033\n",
            "Training Results - Epoch: 20  Avg loss: 0.0053685981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0034145841\n",
            "Training Results - Epoch: 21  Avg loss: 0.0010999160\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0009710301\n",
            "Training Results - Epoch: 22  Avg loss: 0.0003699659\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0005027756\n",
            "Training Results - Epoch: 23  Avg loss: 0.0003182306\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0004663294\n",
            "Training Results - Epoch: 24  Avg loss: 0.0003965659\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0003608500\n",
            "Training Results - Epoch: 25  Avg loss: 0.0004454648\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0006853756\n",
            "Training Results - Epoch: 26  Avg loss: 0.0002404861\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0003595489\n",
            "Training Results - Epoch: 27  Avg loss: 0.0001474510\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0003075489\n",
            "Training Results - Epoch: 28  Avg loss: 0.0005153328\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0004927386\n",
            "Training Results - Epoch: 29  Avg loss: 0.0002273229\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0003210493\n",
            "Training Results - Epoch: 30  Avg loss: 0.0002173277\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0003250775\n",
            "Training Results - Epoch: 31  Avg loss: 0.0003515880\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0004325345\n",
            "Training Results - Epoch: 32  Avg loss: 0.0003081302\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003620909\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001847137\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003217619\n",
            "Training Results - Epoch: 34  Avg loss: 0.0004093982\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0005036369\n",
            "Training Results - Epoch: 35  Avg loss: 0.0001773288\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003676005\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001651176\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003445578\n",
            "Training Results - Epoch: 37  Avg loss: 0.0001927363\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003698697\n",
            "Training Results - Epoch: 38  Avg loss: 0.0002053760\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003676610\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001620566\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003336309\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001584265\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003592438\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001561553\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003638292\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001987560\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003812848\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001949492\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0004011761\n",
            "Training Results - Epoch: 44  Avg loss: 0.0002050714\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0003619384\n",
            "Training Results - Epoch: 45  Avg loss: 0.0003337731\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0004601343\n",
            "Training Results - Epoch: 46  Avg loss: 0.0007984981\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0007497369\n",
            "Training Results - Epoch: 47  Avg loss: 0.0017413884\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0007266005\n",
            "Training Results - Epoch: 48  Avg loss: 0.0473043700\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0312446314\n",
            "Training Results - Epoch: 49  Avg loss: 0.0016566562\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0015785092\n",
            "Training Results - Epoch: 50  Avg loss: 0.0007061341\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0009872793\n",
            "Training Results - Epoch: 51  Avg loss: 0.0004989156\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0008016587\n",
            "Training Results - Epoch: 52  Avg loss: 0.0003448172\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0007211621\n",
            "Training Results - Epoch: 53  Avg loss: 0.0002962018\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0006298768\n",
            "Training Results - Epoch: 54  Avg loss: 0.0002793758\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0006854306\n",
            "Training Results - Epoch: 55  Avg loss: 0.0002645570\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0006417313\n",
            "Training Results - Epoch: 56  Avg loss: 0.0002405682\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0005742923\n",
            "Training Results - Epoch: 57  Avg loss: 0.0003102604\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0004974108\n",
            "FOLD 3\n",
            "Training Results - Epoch: 1  Avg loss: 0.0041864310\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0032350246\n",
            "Training Results - Epoch: 2  Avg loss: 0.0014447955\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0012297388\n",
            "Training Results - Epoch: 3  Avg loss: 0.0005883858\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0007902917\n",
            "Training Results - Epoch: 4  Avg loss: 0.0011654814\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0005776650\n",
            "Training Results - Epoch: 5  Avg loss: 0.0007793346\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0005785700\n",
            "Training Results - Epoch: 6  Avg loss: 0.0006311820\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0006279364\n",
            "Training Results - Epoch: 7  Avg loss: 0.0005563295\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0005317549\n",
            "Training Results - Epoch: 8  Avg loss: 0.0006030010\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0004200197\n",
            "Training Results - Epoch: 9  Avg loss: 0.0006547189\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0006951400\n",
            "Training Results - Epoch: 10  Avg loss: 0.0003365365\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0004112100\n",
            "Training Results - Epoch: 11  Avg loss: 0.0003088294\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0005230345\n",
            "Training Results - Epoch: 12  Avg loss: 0.0002783316\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0004403394\n",
            "Training Results - Epoch: 13  Avg loss: 0.0002271171\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0004392457\n",
            "Training Results - Epoch: 14  Avg loss: 0.0003266450\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0006880335\n",
            "Training Results - Epoch: 15  Avg loss: 0.0002032064\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0003389726\n",
            "Training Results - Epoch: 16  Avg loss: 0.0001694008\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0005682227\n",
            "Training Results - Epoch: 17  Avg loss: 0.0001714486\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0003320728\n",
            "Training Results - Epoch: 18  Avg loss: 0.0001636556\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0004404894\n",
            "Training Results - Epoch: 19  Avg loss: 0.0001603541\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0004158700\n",
            "Training Results - Epoch: 20  Avg loss: 0.0001725559\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0005415348\n",
            "Training Results - Epoch: 21  Avg loss: 0.0001547136\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0004658510\n",
            "Training Results - Epoch: 22  Avg loss: 0.0001933522\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0004038450\n",
            "Training Results - Epoch: 23  Avg loss: 0.0001671308\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0003585448\n",
            "Training Results - Epoch: 24  Avg loss: 0.0001843023\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0004699639\n",
            "Training Results - Epoch: 25  Avg loss: 0.0002276218\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0005319022\n",
            "Training Results - Epoch: 26  Avg loss: 0.0004216349\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0009657347\n",
            "Training Results - Epoch: 27  Avg loss: 0.0004870354\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0011018034\n",
            "Training Results - Epoch: 28  Avg loss: 0.0065315928\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0034013633\n",
            "Training Results - Epoch: 29  Avg loss: 0.0251763065\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0182716622\n",
            "Training Results - Epoch: 30  Avg loss: 0.0153312046\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0059843473\n",
            "Training Results - Epoch: 31  Avg loss: 0.0198425814\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0135966580\n",
            "Training Results - Epoch: 32  Avg loss: 0.0076906974\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0041457324\n",
            "Training Results - Epoch: 33  Avg loss: 0.0063398786\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0025473875\n",
            "Training Results - Epoch: 34  Avg loss: 0.0019710988\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0022784181\n",
            "Training Results - Epoch: 35  Avg loss: 0.0007683499\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0006990262\n",
            "Training Results - Epoch: 36  Avg loss: 0.0003332808\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0005041160\n",
            "Training Results - Epoch: 37  Avg loss: 0.0002024002\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003203892\n",
            "Training Results - Epoch: 38  Avg loss: 0.0001370140\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003875835\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001253532\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003536274\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001159871\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003984520\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001110735\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003635817\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001115984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0004826488\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001140851\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003663225\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001334193\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0004335232\n",
            "Training Results - Epoch: 45  Avg loss: 0.0001539395\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0004248577\n",
            "Training Results - Epoch: 46  Avg loss: 0.0001607268\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0004723642\n",
            "Training Results - Epoch: 47  Avg loss: 0.0002794304\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0006329498\n",
            "Training Results - Epoch: 48  Avg loss: 0.0003918004\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0005479814\n",
            "Training Results - Epoch: 49  Avg loss: 0.0007709601\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0013378550\n",
            "Training Results - Epoch: 50  Avg loss: 0.0007687096\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0006135324\n",
            "Training Results - Epoch: 51  Avg loss: 0.0009374586\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0026619852\n",
            "Training Results - Epoch: 52  Avg loss: 0.0006375069\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0005800714\n",
            "Training Results - Epoch: 53  Avg loss: 0.0004670157\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0004723161\n",
            "Training Results - Epoch: 54  Avg loss: 0.0005899084\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0010296337\n",
            "Training Results - Epoch: 55  Avg loss: 0.0009687873\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0007938248\n",
            "Training Results - Epoch: 56  Avg loss: 0.0007169211\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0005943374\n",
            "Training Results - Epoch: 57  Avg loss: 0.0005770627\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0006592409\n",
            "Training Results - Epoch: 58  Avg loss: 0.0003898047\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0011863857\n",
            "Training Results - Epoch: 59  Avg loss: 0.0002037393\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0007257291\n",
            "Training Results - Epoch: 60  Avg loss: 0.0001520207\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0005672982\n",
            "Training Results - Epoch: 61  Avg loss: 0.0001307462\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0004881981\n",
            "Training Results - Epoch: 62  Avg loss: 0.0001190812\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0004659850\n",
            "Training Results - Epoch: 63  Avg loss: 0.0001125389\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0004632181\n",
            "Training Results - Epoch: 64  Avg loss: 0.0001072343\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0004577145\n",
            "Training Results - Epoch: 65  Avg loss: 0.0001031916\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 65  Avg loss: 0.0004551430\n",
            "Training Results - Epoch: 66  Avg loss: 0.0001003077\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 66  Avg loss: 0.0004484669\n",
            "Training Results - Epoch: 67  Avg loss: 0.0000977119\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 67  Avg loss: 0.0004450960\n",
            "FOLD 4\n",
            "Training Results - Epoch: 1  Avg loss: 0.0026753869\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 1  Avg loss: 0.0011764320\n",
            "Training Results - Epoch: 2  Avg loss: 0.0061566276\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 2  Avg loss: 0.0024067458\n",
            "Training Results - Epoch: 3  Avg loss: 0.0048249134\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 3  Avg loss: 0.0049548519\n",
            "Training Results - Epoch: 4  Avg loss: 0.0029682975\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 4  Avg loss: 0.0023439784\n",
            "Training Results - Epoch: 5  Avg loss: 0.0054778639\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 5  Avg loss: 0.0024264946\n",
            "Training Results - Epoch: 6  Avg loss: 0.0053288657\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 6  Avg loss: 0.0043145419\n",
            "Training Results - Epoch: 7  Avg loss: 0.0005755186\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 7  Avg loss: 0.0008912145\n",
            "Training Results - Epoch: 8  Avg loss: 0.0011198694\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 8  Avg loss: 0.0004875630\n",
            "Training Results - Epoch: 9  Avg loss: 0.0013917609\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 9  Avg loss: 0.0007025985\n",
            "Training Results - Epoch: 10  Avg loss: 0.0016523514\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 10  Avg loss: 0.0005520036\n",
            "Training Results - Epoch: 11  Avg loss: 0.0017206057\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 11  Avg loss: 0.0005755205\n",
            "Training Results - Epoch: 12  Avg loss: 0.0012113883\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 12  Avg loss: 0.0008178788\n",
            "Training Results - Epoch: 13  Avg loss: 0.0010724876\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 13  Avg loss: 0.0006108455\n",
            "Training Results - Epoch: 14  Avg loss: 0.0012115590\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 14  Avg loss: 0.0016225483\n",
            "Training Results - Epoch: 15  Avg loss: 0.0022642234\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 15  Avg loss: 0.0016307125\n",
            "Training Results - Epoch: 16  Avg loss: 0.0018101984\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 16  Avg loss: 0.0006910805\n",
            "Training Results - Epoch: 17  Avg loss: 0.0055867898\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 17  Avg loss: 0.0057834338\n",
            "Training Results - Epoch: 18  Avg loss: 0.0021253960\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 18  Avg loss: 0.0015945504\n",
            "Training Results - Epoch: 19  Avg loss: 0.0033427416\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 19  Avg loss: 0.0015382619\n",
            "Training Results - Epoch: 20  Avg loss: 0.0019276809\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 20  Avg loss: 0.0022985425\n",
            "Training Results - Epoch: 21  Avg loss: 0.0021504595\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 21  Avg loss: 0.0011855787\n",
            "Training Results - Epoch: 22  Avg loss: 0.0045612264\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 22  Avg loss: 0.0048575639\n",
            "Training Results - Epoch: 23  Avg loss: 0.0012709431\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 23  Avg loss: 0.0013663962\n",
            "Training Results - Epoch: 24  Avg loss: 0.0019475034\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 24  Avg loss: 0.0007923730\n",
            "Training Results - Epoch: 25  Avg loss: 0.0022636074\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 25  Avg loss: 0.0011877449\n",
            "Training Results - Epoch: 26  Avg loss: 0.0028844945\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 26  Avg loss: 0.0025868662\n",
            "Training Results - Epoch: 27  Avg loss: 0.0013930578\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 27  Avg loss: 0.0009175495\n",
            "Training Results - Epoch: 28  Avg loss: 0.0049076453\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 28  Avg loss: 0.0016283409\n",
            "Training Results - Epoch: 29  Avg loss: 0.0034580017\n",
            "Current lr: 0.001\n",
            "Validation Results - Epoch: 29  Avg loss: 0.0026146425\n",
            "Training Results - Epoch: 30  Avg loss: 0.0007873175\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 30  Avg loss: 0.0005012187\n",
            "Training Results - Epoch: 31  Avg loss: 0.0001768561\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 31  Avg loss: 0.0003756880\n",
            "Training Results - Epoch: 32  Avg loss: 0.0001479641\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 32  Avg loss: 0.0003648741\n",
            "Training Results - Epoch: 33  Avg loss: 0.0001412960\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 33  Avg loss: 0.0003613803\n",
            "Training Results - Epoch: 34  Avg loss: 0.0001377016\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 34  Avg loss: 0.0003591035\n",
            "Training Results - Epoch: 35  Avg loss: 0.0001339549\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 35  Avg loss: 0.0003598687\n",
            "Training Results - Epoch: 36  Avg loss: 0.0001315893\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 36  Avg loss: 0.0003592067\n",
            "Training Results - Epoch: 37  Avg loss: 0.0001293293\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 37  Avg loss: 0.0003596986\n",
            "Training Results - Epoch: 38  Avg loss: 0.0001273779\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 38  Avg loss: 0.0003606105\n",
            "Training Results - Epoch: 39  Avg loss: 0.0001250185\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 39  Avg loss: 0.0003601069\n",
            "Training Results - Epoch: 40  Avg loss: 0.0001229741\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 40  Avg loss: 0.0003602430\n",
            "Training Results - Epoch: 41  Avg loss: 0.0001214451\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 41  Avg loss: 0.0003607128\n",
            "Training Results - Epoch: 42  Avg loss: 0.0001195315\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 42  Avg loss: 0.0003619566\n",
            "Training Results - Epoch: 43  Avg loss: 0.0001184004\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 43  Avg loss: 0.0003614749\n",
            "Training Results - Epoch: 44  Avg loss: 0.0001166890\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 44  Avg loss: 0.0003640281\n",
            "Training Results - Epoch: 45  Avg loss: 0.0001152209\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 45  Avg loss: 0.0003636170\n",
            "Training Results - Epoch: 46  Avg loss: 0.0001142142\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 46  Avg loss: 0.0003660572\n",
            "Training Results - Epoch: 47  Avg loss: 0.0001126484\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 47  Avg loss: 0.0003684535\n",
            "Training Results - Epoch: 48  Avg loss: 0.0001116583\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 48  Avg loss: 0.0003672473\n",
            "Training Results - Epoch: 49  Avg loss: 0.0001107850\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 49  Avg loss: 0.0003706315\n",
            "Training Results - Epoch: 50  Avg loss: 0.0001095273\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 50  Avg loss: 0.0003737232\n",
            "Training Results - Epoch: 51  Avg loss: 0.0001089698\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 51  Avg loss: 0.0003757131\n",
            "Training Results - Epoch: 52  Avg loss: 0.0001076364\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 52  Avg loss: 0.0003743281\n",
            "Training Results - Epoch: 53  Avg loss: 0.0001067656\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 53  Avg loss: 0.0003780510\n",
            "Training Results - Epoch: 54  Avg loss: 0.0001053103\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 54  Avg loss: 0.0003810436\n",
            "Training Results - Epoch: 55  Avg loss: 0.0001060888\n",
            "Current lr: 0.0001\n",
            "Validation Results - Epoch: 55  Avg loss: 0.0003845089\n",
            "Training Results - Epoch: 56  Avg loss: 0.0001052879\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 56  Avg loss: 0.0003845151\n",
            "Training Results - Epoch: 57  Avg loss: 0.0001046002\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 57  Avg loss: 0.0003846731\n",
            "Training Results - Epoch: 58  Avg loss: 0.0001042835\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 58  Avg loss: 0.0003849316\n",
            "Training Results - Epoch: 59  Avg loss: 0.0001040707\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 59  Avg loss: 0.0003852878\n",
            "Training Results - Epoch: 60  Avg loss: 0.0001038968\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 60  Avg loss: 0.0003854908\n",
            "Training Results - Epoch: 61  Avg loss: 0.0001037616\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 61  Avg loss: 0.0003858282\n",
            "Training Results - Epoch: 62  Avg loss: 0.0001036275\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 62  Avg loss: 0.0003862140\n",
            "Training Results - Epoch: 63  Avg loss: 0.0001035038\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 63  Avg loss: 0.0003868515\n",
            "Training Results - Epoch: 64  Avg loss: 0.0001034029\n",
            "Current lr: 1e-05\n",
            "Validation Results - Epoch: 64  Avg loss: 0.0003871988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5RM5xCQBeJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "decoded_delta = decoder_c(delta_embedding).cpu().detach().numpy()\n",
        "predicted_data = incomes[2011].values + decoded_delta"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "6b7d74c6-c411-441d-cf7c-b648c127125e",
        "id": "PJO-Vs__BhP0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.abs(predicted_data[train_val_neighbourhoods] - actual_data[train_val_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0858662570687752"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKVsosH0Ok-U",
        "colab_type": "code",
        "outputId": "fdad3275-e1e5-4e13-979c-366030a745db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "np.abs(predicted_data[test_neighbourhoods] - actual_data[test_neighbourhoods]).sum(axis=1).mean()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.131040148882963"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "45Jwz6sHN4jD",
        "colab_type": "text"
      },
      "source": [
        "## Results Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NzmVh3vN6Ji",
        "colab_type": "text"
      },
      "source": [
        "<table>\n",
        "    <thead>\n",
        "        <tr>\n",
        "            <th rowspan=2>Model</th>\n",
        "            <th rowspan=2> \"No change\" </th>\n",
        "            <th colspan=2>TF-IDF Autoencoder</th>\n",
        "            <th colspan=2>ELMO</th>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <th>LR</th>\n",
        "            <th>NLR1</th>\n",
        "            <th>LR</th>\n",
        "            <th>NLR1</th>\n",
        "            <th>NLR2</th>\n",
        "        </tr>\n",
        "    </thead>\n",
        "    <tbody>\n",
        "        <tr>\n",
        "            <td>Training MAE</td>\n",
        "            <td>14.76%</td>\n",
        "            <td>10.07%</td>\n",
        "            <td>9.89%</td>\n",
        "            <td>9.78%</td>\n",
        "            <td>7.94%</td>\n",
        "            <td>8.59%</td>\n",
        "        </tr>\n",
        "        <tr>\n",
        "            <td>Testing MAE</td>\n",
        "            <td>14.10%</td>\n",
        "            <td>10.32%</td>\n",
        "            <td>12.90%</td>\n",
        "            <td>10.63%</td>\n",
        "            <td>18.42%</td>\n",
        "            <td>13.10%</td>\n",
        "        </tr>\n",
        "</tbody>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S4pqGXLIdmgA",
        "colab_type": "text"
      },
      "source": [
        "## Sensitivity Analysis\n",
        "\n",
        "Senesitivity Analysis is done on the best performing model, TF-IDF Autoencoder with Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n93Vnj4zdwrH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_dim_reviews(reviews, orig_delta_embedding, dim):\n",
        "    orig_decoded = lr.predict(orig_delta_embedding)\n",
        "    diff = np.zeros(orig_decoded.shape)\n",
        "  \n",
        "    r_2011 = reviews[2011].data.clone()\n",
        "    r_2016 = reviews[2016].data.clone()\n",
        "\n",
        "    r_2011[:, dim] = torch.zeros(r_2011.shape[0])\n",
        "    r_2016[:, dim] = torch.zeros(r_2016.shape[0])\n",
        "\n",
        "    delta_embedding = encoder(r_2016) - encoder(r_2011)\n",
        "\n",
        "    decoded = lr.predict(delta_embedding.detach().cpu().numpy())\n",
        "    diff = orig_decoded - decoded\n",
        "\n",
        "    diff = diff.mean(axis=0)\n",
        "    \n",
        "    return diff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DcfvLHM3eHBr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feature_importance_reviews = np.zeros((reviews[2011].data.shape[1], delta_census.shape[1]))\n",
        "for i in range(reviews[2011].data.shape[1]):\n",
        "  feature_importance_reviews[i] = evaluate_dim_reviews(reviews, delta_embedding, i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8q4rs7NFh9j3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def heatmap(data, name, absolute=False, width=30, height=30):\n",
        "  fig, ax = plt.subplots()\n",
        "\n",
        "  # We want to show all ticks...\n",
        "  ax.set_xticks(np.arange(data.shape[1]))\n",
        "  ax.set_yticks(np.arange(data.shape[0]))\n",
        "  # ... and label them with the respective list entries\n",
        "\n",
        "  try:\n",
        "    ax.set_xticklabels(data.columns)\n",
        "    ax.set_yticklabels(data.index)\n",
        "  except Exception:\n",
        "    pass\n",
        "\n",
        "  if type(data) != np.ndarray:\n",
        "    data = data.to_numpy(dtype=float)\n",
        "\n",
        "  if absolute:\n",
        "    im = ax.imshow(np.abs(data))\n",
        "  else:\n",
        "    im = ax.imshow(data)\n",
        "\n",
        "  plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
        "          rotation_mode=\"anchor\")\n",
        "\n",
        "  cbar = ax.figure.colorbar(im, ax=ax)\n",
        "\n",
        "  fig.set_size_inches(width, height)\n",
        "\n",
        "  ax.set_title(name)\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DmNLcXN7ebDF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort by absolute mean\n",
        "sort_importance = np.argsort(np.abs(feature_importance_reviews).mean(axis=1))[::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzWh_8AVeeBL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(DRIVE_PATH.joinpath('models/words.out'), 'r') as f:\n",
        "  words = f.read().split('\\n')\n",
        "\n",
        "words = words[:-1]\n",
        "\n",
        "df_importance = pd.DataFrame(feature_importance_reviews, columns=incomes[2011].columns, index=words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrBDkO32eivt",
        "colab_type": "code",
        "outputId": "e4b050fa-eac6-4f19-fc57-cd8f697d812e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "df_importance.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['  Under $10,000', '  $10,000 to $19,999', '  $20,000 to $29,999',\n",
              "       '  $30,000 to $39,999', '  $40,000 to $49,999', '  $50,000 to $59,999',\n",
              "       '  $60,000 to $79,999', '  $80,000 to $99,999', '  $100,000 and over'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "096X1oKciNK9",
        "colab_type": "code",
        "outputId": "03fd6b35-4828-4fb2-a368-1904e98f59a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 566
        }
      },
      "source": [
        "heatmap(df_importance.iloc[sort_importance[:40]], 'feature importance sorted by absolute mean', width=5, height=8)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASsAAAImCAYAAAAL0qz3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/w/mA+9iaD65tQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x576 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFaCM2DSiQSv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# sort by std\n",
        "sort_importance_std = np.argsort(feature_importance_reviews.std(axis=1))[::-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR6tfTx1iTBv",
        "colab_type": "code",
        "outputId": "90792565-c5c8-4c22-f99f-12fe6a5acf06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 675
        }
      },
      "source": [
        "heatmap(df_importance.iloc[sort_importance_std[:40]], 'feature importance sorted by std', width=5, height=10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASwAAAKTCAYAAACjCTVIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEAR1QyisIAjqhlBYQRDUDf8fMdHYnTr8hz0AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 360x720 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njcZhNzde1ld",
        "colab_type": "code",
        "outputId": "7e7cbd85-352d-4cbd-af4f-16b21b3bbf8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# Low income\n",
        "df_importance['  Under $10,000'][df_importance['  Under $10,000'].abs().sort_values(ascending=False)[3:13].index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "really good         0.000025\n",
              "violating           0.000024\n",
              "customer service   -0.000020\n",
              "patio              -0.000014\n",
              "amazing            -0.000014\n",
              "sales               0.000014\n",
              "customer           -0.000012\n",
              "deep fried         -0.000012\n",
              "fish               -0.000012\n",
              "sashimi             0.000011\n",
              "Name:   Under $10,000, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWHIKm1ve4k7",
        "colab_type": "code",
        "outputId": "e8d275c2-f4b5-4771-f314-84c561a69ddb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# Mid-level\n",
        "df_importance['  $50,000 to $59,999'][df_importance['  $50,000 to $59,999'].abs().sort_values(ascending=False)[3:13].index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "violating terms     0.000036\n",
              "burger             -0.000022\n",
              "chips              -0.000016\n",
              "customer service   -0.000015\n",
              "jerk                0.000014\n",
              "service            -0.000013\n",
              "coffee              0.000013\n",
              "ice cream          -0.000013\n",
              "pizza              -0.000012\n",
              "school              0.000011\n",
              "Name:   $50,000 to $59,999, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMFNW6pJe-Ou",
        "colab_type": "code",
        "outputId": "b3c903b0-0c1c-45e8-bbe3-8de5256dddf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        }
      },
      "source": [
        "# High-income\n",
        "df_importance['  $100,000 and over'][df_importance['  $100,000 and over'].abs().sort_values(ascending=False)[:10].index]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "chicken             0.000032\n",
              "review removed      0.000020\n",
              "highly recommend    0.000014\n",
              "store              -0.000011\n",
              "sashimi            -0.000011\n",
              "salmon             -0.000010\n",
              "school              0.000010\n",
              "really good        -0.000009\n",
              "burger             -0.000009\n",
              "class               0.000008\n",
              "Name:   $100,000 and over, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    }
  ]
}